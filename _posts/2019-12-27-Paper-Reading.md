---
layout: post
comments: tr=e
title: Paper Reading
published: false
---

- learn
    - scst
    - https://arxiv.org/abs/2111.07832
    - https://arxiv.org/pdf/2111.09886v1.pdf
- book to read
    - Rites of Passage at $100,000 to $1,000,000+
    - design your life from a standardford writer
- doue read
    - Differentiable Patch Selection for Image Recognition
        - how the backward is derived?
    - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
- To read
    - Open-Vocabulary Image Segmentation
    - Improving language models by retrieving from trillions of tokens
    - FQ-ViT: Fully Quantized Vision Transformer without Retraining
    - Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
    - Self-EMD: Self-Supervised Object Detection without ImageNet
    - Unsupervised Object-Level Representation Learning from Scene Images
    - Resnest: Split-attention networks
    - Generating long sequences with sparse transformers
    - TNT: Text-Conditioned Network with Transductive Inference for Few-Shot Video Classification
    - Temporal context network for activity localization in videos
    - Fast Convergence of DETR with Spatially Modulated Co-Attention
    - TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?
    - Self-supervised Video Representation Learning with Cross-Stream Prototypical Contrasting
    - Scaling Vision with Sparse Mixture of Experts
    - Open-Vocabulary Object Detection Using Captions
    - MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding
    - FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding
    - Deep Networks with Stochastic Depth
    - Are Sixteen Heads Really Better than One?
    - Rethinking attention with performers
    - REFORMER: THE EFFICIENT TRANSFORMER
    - Deep modular co-attention networks for visual question answering
    - Pyramid scene parsing network
    - Spatially Aware Multimodal Transformers for TextVQA
    - DeepViT: Towards Deeper Vision Transformer
    - Multigrain: a unified image embedding for classes and instances
    - Augment your batch: better training with larger batches
    - random erasing data augmentation
    - Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval (arxiv.org)
    - Xiaobin's CvT
    - How to represent part-whole hierarchies in a neural network
    - Unbiased Teacher for Semi-Supervised Object Detection 
    - read Adversarial Feature Augmentation and Normalization for Vision Systems
    - Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer
    - Zero-Shot Text-to-Image Generation: ()[https://arxiv.org/pdf/2102.12092.pdf]
    - Incorporating copying mechanism in sequence-to-sequence learning
    - ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
    - SCAN: Learning to Classify Images without Labels
    - Dimensionality reduction by learning an invariant mapping
    - knwoledge distillation
    - Transductive Learning for Zero-Shot Object Detection
    - Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering
    - Learning to Cluster under Domain Shift
    - Unsupervised Feature Learning by Cross-Level Discrimination between Instances and Groups
    - Spatiotemporal Contrastive Video Representation Learning
    - Exploring Relations in Untrimmed Videos for Self-Supervised Learning
    - Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework
    - Functional Regularization for Representation Learning: A Unified Theoretical Perspective
    - Self-supervised Temporal Discriminative Learning for Video Representation Learning
    - Self-supervised learning using consistency regularization of spatio-temporal data augmentation for action recognition
    - Memory-augmented Dense Predictive Coding for Video Representation Learning
    - Predicting What You Already Know Helps: Provable Self-Supervised Learning
    - Self-supervised learning through the eyes of a child
    - Learning from the Past: Continual Meta-Learning with Bayesian Graph Neural Networks
    - High-order structure preserving graph neural network for few-shot learning
    - Meta-learning with memory-augmented neural networks
    - Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation
    - Predicting What You Already Know Helps: Provable Self-Supervised Learning
    - Contrastive Visual-Linguistic Pretraining
    - ReLaB: Reliable Label Bootstrapping for Semi-Supervised Learning
    - CrossTransformers: spatially-aware few-shot transfer
    - Video Representation Learning by Recognizing Temporal Transformations
    - A comprehensive overhaul of feature distillation
    - Meta-Learning for Semi-Supervised Few-Shot Classification
    - Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection
    - Prototypical Networks for Few-shot Learning
    - Improving Few-Shot Learning using Composite Rotation based Auxiliary Task
    - Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining Paradigms
    - Enabling On-Device CNN Training by Self-Supervised Instance Filtering and Error Map Pruning
    - Self-supervised Neural Architecture Search
    - Knowledge Distillation by On-the-Fly Native Ensemble
    - Online Knowledge Distillation via Collaborative Learning
    - MOD: A Deep Mixture Model with Online Knowledge Distillation for Large Scale Video Temporal Concept Localization
    - Dynamic Refinement Network for Oriented and Densely Packed Object Detection
    - Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models
    - Learning Invariant Representation for Unsupervised Image Restoration
    - Transformation GAN for Unsupervised Image Synthesis and Representation Learning
    - Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective
    - Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds
    - HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation
    - Probabilistic Structural Latent Representation for Unsupervised Embedding
    - Decoupled Representation Learning for Skeleton-Based Gesture Recognition
    - Towards Backward-Compatible Representation Learning
    - Sketch-BERT: Learning Sketch Bidirectional Encoder Representation From Transformers by Self-Supervised Learning of Sketch Gestalt
    - Towards Universal Representation Learning for Deep Face Recognition
    - S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation
    - Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning
    - Unsupervised Representation Learning for Gaze Estimation
    - Hierarchically Robust Representation Learning
    - Learning Representations by Predicting Bags of Visual Words

    - Large Scale Video Representation Learning via Relational Graph Clustering
    - Distribution-Induced Bidirectional Generative Adversarial Network for Graph Representation Learning
    - Are we done with ImageNet?
    - Evolving Losses for Unsupervised Video Representation Learning
    - Learning Representations by Predicting Bags of Visual Words
    - Knowledge Distillation Meets Self-Supervision
    - Training low bitwidth convolutional neural networks with low bitwidth gradients
    - fb-net
    - Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection
    - https://blog.evjang.com/2016/11/tutorial-categorical-variational.html
    - htps://openreview.net/pdf?id=rkE3y85ee 
    - https://towardsdatascience.com/what-is-gumbel-softmax-7f6d9cdcb90e?gi=26de57769dc4 
    - Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing
    - Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models
    - Stand-Alone Self-Attention in Vision Models
    - On the Relationship between Self-Attention and Convolutional Layers
    - Multi-similarity loss with general pair weighting for deep metric learning
    - Cut, paste and learn: Surprisingly easy synthesis for instance detection
    - Circle Loss: A Unified Perspective of Pair Similarity Optimization
    - MaxUp: A Simple Way to Improve Generalization of Neural Network Training
    - DetNAS: Backbone Search for Object Detection
    - https://lilianweng.github.io/lil-log/tag/generative-model
    - https://github.com/jason718/awesome-self-supervised-learning
    - Extracting and Composing Robust Features with Denoising Autoencoders
    - Context Encoders: Feature Learning by Inpainting
    - Multimedia Search with Pseudo-Relevance Feedback 
    - M2det: A single-shot object detector based on multi-level feature pyramid network
    - Dynamic anchor feature selection for single-shot object detection
    - Low-shot learning with large-scale diffusion
    - FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence
    - ∆-encoder: an effective sample synthesis method for few-shot object recognition
    - Conditional image generation with pixelcnn decoders
    - Population Based Training of Neural Networks
    - Algorithms for Hyper-Parameter Optimization
    - random search for hyper-parameter optimization
    - Snapshot Distillation: Teacher-Student Optimization in One Generation
    - Learning to compose domain-specifici transformations for data
      augmentation.
    - Object detection via a multiregion & semantic segmentation-aware cnn model
    - Web-Scale Responsive Visual Search at Bing
    - Wasserstein GAN
    - Image-to-image translation with conditional adversarial networks
    - https://venturebeat.com/2020/02/11/researchers-develop-technique-to-increase-sample-efficiency-in-reinforcement-learning/
    - Generative adversarial nets
    - Conditional generative adversarial nets
    - U-net: Convolutional networks for biomedical image segmentation
    - Progressive pose attention transfer for person image generation
    - Differentiable learningto-normalize via switchable normalization
    - Disentangled Person Image Generation
    - Web-Scale Responsive Visual Search at Bing
    - When Unsupervised Domain Adaptation Meets Tensor Representations
    - Learning Transferable Features with Deep Adaptation Networks
    - Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks
    - unsupervised domain adaptation by backpropagation
    - Domain-Adversarial Training of Neural Networks
    - Few-shot Object Detection via Feature Reweighting
    - Meta-RCNN: Meta Learning for Few-Shot Object Detection
    - https://towardsdatascience.com/few-shot-learning-in-cvpr19-6c6892fc8c5
    - Robust scene text recognition with automatic rectification
    - Convolutional Sequence to Sequence Learning
    - DARTS: Differentiable Architecture Search
        - https://github.com/quark0/darts
    - symmetry-constrained rectification network for scene text recognition
    - XNAS: Neural Architecture Search with Expert Advice
    - Progressive Neural Architecture Search
    - Non-Local Neural Network
    - CornerNet: Detecting Objects as Paired Keypoints
    - GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond
    - Bottom-up Object Detection by Grouping Extreme and Center Points
    - Side-Aware Boundary Localization for More Precise Object Detection
    - Feature Selective Anchor-Free Module for Single-Shot Object Detection
    - Efficient Object Detection in Large Images using Deep Reinforcement Learning
    - Dually Supervised Feature Pyramid for Object Detection and Segmentation
    - SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization
    - IoU-uniform R-CNN: Breaking Through the Limitations of RPN
    - AugFPN: Improving Multi-scale Feature Learning for Object Detection
    - RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation
    - Learning from Noisy Anchors for One-stage Object Detection
    - RefineNet: multi-path refinement networks for dense prediction
    - FCOS: Fully Convolutional One-Stage Object Detection
        - github: https://github.com/tianzhi0549/FCOS
    - NAS-FCOS: Fast Neural Architecture Search for Object Detection
        - https://github.com/Lausannen/NAS-FCOS
    - Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing


# machine learning
- inductive bias for deep learning of higher level cognition
    - not interesting
- towards causal representation learning
    - not useful
- A Fast Proximal Point Method for Computing Exact Wasserstein Distance
    - a nice paper to solve the transportation problem
# Co-localization
- MOPRO: WEBLY SUPERVISED LEARNING WITH MOMENTUM PROTOTYPES
- AutoAssign
- Learning to find common objects across few image collections
    - arxiv 8/2019
    - the problem is to find a common proposal within each image given multiple
      images which containing common proposals and one image which does not
      contain the proposal of the target.

# dataset
- Open Images V5 Text Annotation and Yet Another Mask Text Spotter
    - arxiv 2021
    - key
        - add text annotation in some of the data.
# variational auto encoder (VAE)
- a very nice tutorial:
    - https://www.jeremyjordan.me/variational-autoencoders/

# Product Recognition
- Fine-Grained Product Class Recognition for Assisted Shopping
    - ICCV 2015 workshop
    - still based on a lot of feature designs. not fully by neural network
- Product recognition in store shelves as a sub-graph isomorphism problem
    - arxiv 2017, ICIAP 2017
    - still based on sift matching. solving the problem to match the observed
      production relationship with a pre-defined layout graph.
- Recognizing Products: A Per-exemplar Multi-label Image Classification Approach
    - ECCV14
    - recognize based on SIFt matching.

# Network pruning
- Visual Transformer Pruning
    - arxiv 4/20/2021
    - key
        - for the linear layer, check which dimension output is important.
          Learn a coefficient ratio to multiple it and then truncate before
          further fine-tuning the network
        - no comparision baseline. For example, how the result is by just
          enabling part of the outputs and train it from scratch.
        - no speed time result. not sure how useful this is for vision
          transformer.

# Data
- Connecting Vision and Language with Localized Narratives
    - provide annotations on coco (all, 100k+) and part (504k) of open image dataset
    - an annotator is required to 1) describe the image content by voice 2)
      hover the mouse on the image region, synchronized with the voice, 3) write
      the transcription immediately after annotating one image.
      - to align the transcript to the voice, (then to the mouse trace/image
        region)
        - automatically generate a transcription of the spoken caption.
        - align the manually written transcription with the auto transcription
          which has the timestamp information
          - the alignment keeps the order.
          - the alignment is based on the edit distance between words.
    - 40.6 seconds for speaking; 110.2 for writing the transcript.
    - manually tried the annotation process, and it is not that natural. maybe
      combine with the click event or draw event (click-drag-clik)? Otherwise,
      there might be some redundant trace between objects, which might not be
      useful and not be easy to remove.

# Network Component
- ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases
    - 3/2021
    - key
        - add relative positional encoding to the transformer block
        - relative positional will be 0 if the two pixels are too far away. In
          this way, it can mimic the conv operations
        - the attention matrix is the sum of two softmax matrices. one is based
          on teh innter product of query and key; the other is based on the
          relative position
        - the first few attention blocks uses this attention. similar like to
          have conv layers in the stem
- Fastformer: Additive Attention Can Be All You Need
    - 8/2021
    - key
        - summarize global tokens from query matrix and then this global query
          token is fused with the key matrix. Then, summarize these fused value
          tokens into a global token, which is fused with teh value.
          Eventually, the query is added with the value matrix.
        - linear complexity
- Linformer: Self-Attention with Linear Complexity
    - arxiv 6/2020
    - key
        - rather than to apply n^2 attention matrix, it maps key and value to k
          so that the attention matrix is nk.
- Efficient Attention: Attention with Linear Complexities
    - wacv 2021
    - key
        - update non-local module with such efficient implementation of
          dot-product attention, so that the cost can be reduced to O(N) from
          O(N^2)
        - it has no relu non-linear operator on the inner product. it proposed
          another non-linear function, i.e. softmax on query and value
          individually. however, the softmax version here cannot give Rnn
          property for auto-regressive case.
- Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
    - very good paper
    - key
        - use dot product to replace the softmax in transformer so that the
          computational cost is linear.
- Implicit Kernel Attention
    - AAAI-21
    - key
        - reformulate the attention as a multiplication of similarity score and
          magnitude score. Similarity score is replaced with an inner product.
          Teh claimed complexity is still O(T^2)
        - the decomposed magnitude is based on L2, but this paper generalize it
          to L_p. The gaol of this paper is to improve the accuracy, not to
          reduce the cost.
- Gaussian error linear units (gelus)
    - arixv 6/2016
    - x*cumulative(x)
- Non-local Neural Networks
    - cvpr 2018
    - similar with self-attention module to increase the dependency of the
      output on the global information through the whole spatial input.
- AdderNet: Do We Really Need Multiplications in Deep Learning
    - 12/31/2019, arxiv
    - replace the conv layer with addition only operations. That is, the
      similarity between the input region and the kernel is replaced by the
      (negative) l1 distance, which has no multiplications.
    - BN is still used here.
    - The gradient of the filter is calculated based on squared l2 distance loss.
    - The gradient of the input from the output is also based on the full
      precision, but truncated within -1 to 1.
    - exp
        - resnet18, imagenet; CNN: 69.8; the proposed: 67.0
        - resnet50, imagenet; CNN: 76.2; the proposed: 74.9
- Weight Standardization
    -  3/25/2019 arxiv
    - idea
        - normalize the weight for the conv layers.
    - experiment
        - r50 on imagenet
        - bn: 24.3; ws + bn: 23.76
        - gn: 24.81: ws + gn: 23.72
- Evolving Normalization-Activation Layers
    - arxiv 4/2020
    - search the nromalization + activation layer and replace it with BN-Relu
    - +1 improvement on instance segmentation, +0.2 around in imagenet
- Dynamic ReLU
    - Key idea
        - Generalize Relu and LeackyRelu as a learnable relu, which is the
          maximum of multiple linear layer. The relu and the leackyrelu can be
          treated as a special case.
        - the parameters are learned from a small network composed of 2 linear
          layers after the global spatial pooling.
    - Experiments
        - the comparision is based on the relu or leakyrelu. The gain is even
          more than 4 points on mobilenetv2 x 0.35, with a sacrifice of more
          parameters (even doubled). Not sure if the gain comes from more
          parameters or the relu. In large network, the gain becomes small,
          where the ratio of extra parameters is smaller. (10%).
- Making Convolutional Networks Shift-Invariant Again
    - ICML 19
    - The motivation is that the downsampling layer (pool and conv with
      stride) could make output variant with input shift. The solution is
      to apply a smoothing filter before sampling.
        - smoothing filter or anti-aliasing filter is equivalent with a
          conv layer with the following parameters
            - the group size is the same as the input size. That is, each
              input feature map is processed independently
            - the output feature number is the same as the input feature
              number. That is, each input feature map generates one output
              feature.
            - the kernel is a pre-defined parameter, which behaves like a
              low-pass filter, e.g. outer product of [1, 2, 1] and [1, 2, 1]^T
                - note, since it is a pre-defined paraameter, one natural
                  extension is to make it learnable! But i did not see the
                  experiment with this.
            - normalization is used.
        - max pooling with stride 2
            - this layer is converted to
                - 1) max pooling with stride 1,
                - 2) apply a smoothing filter or anti-aliasing filter
                - 3) sub-sampling.
        - conv with stride 2 and relu
            - this layer is converted to
                - conv with stride 1
                - relu
                - apply a smoothing filter
                - sub-sampling
            - it combines with relu, and it is not like the following
                - conv with stride 1
                - apply a smoothing filter
                - sub-sampling
                - relu
            - the reason should be that conv is a linear operation;
              smoothing is also a linear operation. Thus it would be
              equivalent to have one learnable conv layer as long as the
              kernel size of smoothing filter is smaller than/equal to the
              conv kernel size.
            - the computational time cost of such converted conv operation
              will be doubled at least since conv with stride 2 becomes
              with conv with stride 1 first. It would be hard to optimize
              since the non-linear layer is applied afterwards.
        - average pooling with stride 2
            - this layer is converted to
                - smoothing filter
                - sub-sampling with stride 2
    - Experiment
        - in imagenet, it has 0.5-1 point gain.
        - the inference time should be increased, but there is no report in
          the paper. it only applies on the down-sampling layer, and thus
          maybe not too much, 10% maybe?
        - the smoothing kernel can all become learnable since it is one of
          special kinds of conv layer with group and fixed kernel. no
          report of such experiment in the paper. Learning time could be
          longer, but inference time should be the same.
        - not sure how it performs in detection and other tasks.

# Analysis
- The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers
    - iclr 2021
    - some definition to study the deep learning. the gap between the model
      trained with a finite number of training samples (multiple epochs) and
      the model trained with an infinite number of images. No conclusion, but
      more about that we can think this kind of gap.
- Rethinking Pre-training and Self-training
    - 6/11/2020 arxiv
    - conclusion
        - stronger data augmentation for coco detection task does not need
          pre-training, which could even hurt the accuracy.
    - similar with data distillation

# Network architecture
- Deep Transformers with Latent Depth
    - nips 2020
    - key
        - gumbel softmax to learn which block should be enabled.
- DeepViT: Towards Deeper Vision Transformer
    - arxiv 3/2021
    - key
        - study the problem of that deeper transformer fails to scale well
        - the solution is to add a fusion matrix to fuse the attention map from
          different heads to make the attention matrix different.
        - the issue it observes is that the attention matrix becomes similar
          when the network goes deeper.
        - the improvement is not that large. 80.9 vs 79.3; 80.1 vs 79.4
- Training data-efficient image transformers & distillation through attention
    - arxiv 12/2020
    - key
        - the idea is to propose a distillation token
        - cutmix improves from 80.0 to 81.8
        - mix-up improves from 78.7 to 81.8
        - AA hurts the accuracy
        - random aug improves from 79.6 to 81.8
        - random erasing improves significantly
        - stochastic depth improves significantly
        - repeated aug improves also
        - drop-out not useful
        - expo moving average not useful
- An Image is Worth 16x16 Words: Transformers for Image
    - arxiv 2020
    - key
        - pre-train with a smaller network input
        - fine-tune with a larger input
- Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
    - 3/25/2021
    - key
        - the spatial resolution is down-sampled gradually like a CNN
        - the attention is only applied on a sub region rather than the whole
          image. The sub region is also shiffted to connect inter-region
          relationship.
        - exp is on classification, detection, segmentation. results on
          detection are pretty good.
- One Weight Bitwidth to Rule Them All
    - arxiv 8/2020
    - key
        - for the normal conv network, 1 bit quantization is good enough.
        - for depth-wise conv network, it could be 4 bits
- Attention Augmented Convolutional Networks
    - arxiv 9/9/2020
    - key
        - add attention module into the cnn
- Fast and Accurate Model Scaling
    - cvpr 2021
    - key
        - scale the width is more useful
        - width scaled by 0.8; depth and resolution by 0.1
- Attention Augmented Convolutional Networks
    - iccv 2019
    - key
        - add attention module besides the conv layers
- High-Performance Large-Scale Image Recognition Without Normalization
    - arxiv 2/2021
    - key
        - remove the batch norm layer
        - instead, replace it with the adaptive gradient cliping
- Are Labels Necessary for Neural Architecture Search?
    - arxiv 3/2020
    - highlights
        - the paper does not provide any new algorithm for NAS
        - the paper replaces the supervised loss based on labels by rotation,
          jigsaw, colorization losses, which are unsupervised.
        - one experiment is to rank 500 architectures based on supervised loss
          and self-supervised loss. Then, calculate the correlation. The
          conclusion is that the rank is highly correlated so that replacing
          supervised labels with self-supervised labels is promising for NAS
        - another experiment is to replace the loss in off-the-shelf DARTS (a
          NAS algorithm) to search the architecture and evaluate on imagenet
          classification and scemantic segmentation problems. The accuracy is
          competitive compared with supervised counterpart.
    - pros
        - raised a good question and found labels are not essential for NAS
    - cons
        - no new algorithm
- FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function
    - 6/3/2020 arxiv, FB
    - architecture search, novelty
        - combine the architecture search and training algorithm
    - misc
        - RMSProp is better than SGD
        - EMA also improves
        - pre-train the predictor (input is archtecture embedding; output is
          the accuracy) by the task to predict the flops, which performs better
          than non-pretraining
    - experiment
        - fbnetv3-a vs efficient-net-0
            - 343M flops vs 390M flops
            - 78.0 top1 vs 77.3
        - fbnetv3-c vs efficient-net-1
            - 544M vs 734M
            - 79.6 vs 79.2
        - fbnetv3-e vs efficient-net-2
            - 752 vs 1G
            - 80.4 vs 80.3
        - fbnetv3-g vs efficient-net-3
            - 2G vs 1.8G
            - 82.3 vs 81.7

# Optimization
- SWALP : Stochastic Weight Averaging in Low Precision Training
    - arxiv 5/2019
    - key
        - swa with 8 bit
- ZeRO: Memory Optimizations Toward Training Trillion Parameter Models
    - arxiv 5/13/2020
    - key
        - split the optimizer state in different rankers for large-scale model
          training
- Fixing the train-test resolution discrepancy
    - 3/30/2020, arxiv
    - the contribution
        - during test, use a larger input
        - during training, fine-tune the network with the larger input size.
    - exp
        - 85.4 -> 86.4 for x101-32-48d
- Understanding the Role of Momentum in Stochastic Gradient Methods
    - NIPs 2019, Igor/Pengchuan
- some docs about multi-armed bandit optimization
    - https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html
    - https://arxiv.org/pdf/1904.07272.pdf
- Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization
    - arxiv 6/2018, journal of machine learning research
- Bayesian Optimization
    - https://github.com/fmfn/BayesianOptimization
        - github star is 3.8k
        - good to use; checkout the advanced tutorial, with
          suggest-evaluate-register
    - https://ax.dev/
        - github star is 1k
        - from facebook
        - good to use. checkout the service api example
    - https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf

## optimization/transport problem
- Sinkhorn Distances: Lightspeed Computation of Optimal Transport
    - nips 2013
    - a very nice solution
- https://michielstock.github.io/OptimalTransport/
    - nice blog about some basis

# network component
- Ghost Loss to Question the Reliability of Training Data
    - arxiv 9/2021
    - key
        - ignore the most significant non-gt labels in prediction
        - intuition is that the gt might be wrong. similar like to drop the
          worst
- In-Place Activated BatchNorm for Memory-Optimized Training of DNNs
    - an efficient BN implementation.
    - worth giving it a try
    - 2018, CVPR

- network architecture search
    - MnasNet: Platform-aware neural architecture search for mobile
    - Neural Architecture Search: A Survey
        - high-level introduction of the approaches from 3 aspects: the search space,
          search strategy, and performance evaluation
        - pretty-good survey

# image classification
- LiT : Zero-Shot Transfer with Locked-image Text Tuning
    - arxiv 10/2021
    - key
        - google brain
        - in contrastive learning, image encoder is pretrained and fixed. Then,
          tune the text encoder, which is randomly initialized
- Combined Scaling for Zero-shot Transfer Learning
    - arxiv 10/2021
    - key
        - increase the image-text pairs to 6B+
        - increase the batch size for the contrastive learning by gradient
          caching
            - does not cite an earlier paper on gradient cache
        - zero-shot imagenet classification, 85 around
- Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup
    - 2021
    - key
        - gradient cache for contrastive learning. actually it is some special
          checkpointing, but split the batch into chunks
- Swin Transformer V2: Scaling Up Capacity and Resolution
    - arxiv 18/11/2021
    - key
        - strong performance with some changes on the structure and scale it up
- CoAtNet: Marrying Convolution and Attention for All Data Sizes
    - arxiv 9/2021
    - key
        - use conv as stem layers
        - new sota on imagenet
- Perceiver IO: A General Architecture for Structured Inputs & Outputs
    - arxiv 8/2021, deepmind
    - key
        - multiple queries to generate the output
        - introduce the query to generate any number of outputs.
        - nice paper, also
- perceiver general perception with iterative attention
    - arxiv 6/2021
    - key
        - a nice paper to use a set of queries to reduce the attention cost
- BEiT: BERT Pre-Training of Image Transformers
    - arxiv 6/2021
    - key
        - recover the masked image patch. Tokenize the image patch first also
          by the variational autoencoder. Then, recover that token.
        - results is 83% on imagenet with vit-base model, which looks quite
          strong
- Multi-Exit Vision Transformer for Dynamic Inference
    - arxiv 6/2021
    - key
        - not a good paper.
- Early Convolutions Help Transformers See Better
    - arxiv 6/2021
    - key
        - add some 5-7 conv layers  before patchifying the input could give
          better stability
- Co-advise: Cross Inductive Bias Distillation
    - arxiv 6/2016
    - key
        - far from ready paper
        - use two distillation tokens. one is for teh teacher of conv-based
          teacher. the other is for involutional network
- VISION PERMUTATOR: A PERMUTABLE MLP-LIKE ARCHITECTURE FOR VISUAL RECOGNITION
    - arxiv 6/2021
    - key
        - encode width/height/channel seperately.
        - the best accuracy is 83 only
        - the number of parameters are not that large, and may not benefit well
          from large pre-training
- A Circular-Structured Representation for Visual Emotion Distribution Learning
    - cvpr 2021
    - key
        - classify the emotion information of the image
        - the classification is similar like a 2-layer prediction.the first
          layer is to predict some high-level emotions, which are 3 attributes.
          The next is to predict the targeted emotion.
- How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers
    - arxiv 6/18/2021
    - key
        - larger backbone is better than smaller patch size
        - stronger data augmentation & drop-out/connect does not work for
          larger dataset with the same epochs
- CoAtNet: Marrying Convolution and Attention for All Data Sizes
    - arxiv 6/2021
    - key
        - two stages of conv layers, depth-wise conv layer is used
        - two layers of transformer layers
        - 89% on imagenet
- Pay Attention to MLPs
    - arxiv 6/2021
    - key
        - use a learnable linear layer as the attention matrix
        - also similar with mlp-mixer
- Not All Images are Worth 16x16 Words: Dynamic Vision Transformers with Adaptive Sequence Length
    - arxiv 5/2021
    - key
        - multiple transformer networks are stacked. The first networks
          processes 2x2 tokens. If the inference score is not high enough, use
          the second netowrk, which processes 4x4 tokens.
- When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations
    - arxiv 6/2021
    - key
        - apply sharpness-aware optimizer to vit and mlp-mixer and achieves
          quite strong accuracy. it is worth trying that optimizer
- multiscale vision transformers
    - arxiv 4/2021
    - key
        - mainly for video task. initialized from scratch
- Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding
    - arxiv 3/29/2021
    - key
        - multi-scale pyramid like Conv in ViT
        - longformer -> ViT
- Differentiable Patch Selection for Image Recognition
    - arxiv 4/7/2021
    - key
        - there is one 4-layer CNN to score each input image patch. The higher
          the score is, the better the region is for the task
        - during training, the top-k selection is actually a weighted sum of
          all input image patch. The weighting scheme is also vanished at the
          end of the training
            - the benefit of such weighting is to make it learnable
            - no comparision with the results without such weighting scheme
        - during inference, only the top-k is selected.
- Visual Transformers: Token-based Image Representation and Processing for Computer Vision
    - 2020
    - key
        - in the last stage of the convolutional network, replace it by the
          proposed transformer module
        - the transformer module
            - the input is a feature map and the output is also a feature map
            - first it compress HxW features into K features, and treat K
              features as the abstract scemantic tokens. After self-attention
              module, it convert it back to HxW with skip connection
- CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification
    - key
        - input has two scales for transformer.
        - the larger scale has more tokens, with fewer hidden dimensions
        - the smaller scale has fewer tokens, with more hidden dimensions
- Adversarial Examples Improve Image Recognition
    - arxiv 11/2019
    - the idea is to use seperate BN for clean image and adversirial images
      during training, which increases the accuarcy on efficient-net by 0.3~0.7
      points.
- Designing Network Design Spaces
    - The key idea is to filter the network architecture search by applying
      some constraints, e.g. the network width should be non-decreasing. Then,
      randomly select, e.g. 10, networks to do training and use the best one.
      - some filters include network bandwidth are non-decreasing; the group
        number among different blocks should be the same; sharing the
        bottleneck ratio among different blocks.
      - the weight how to measure if the constraint changes the powerfullness
        of the network space is by 1) randomly sampling 500 networks in the
        space, 2) train and testing each network, 3) plot the cumulative error
        rate, 4) visually decide if the constraint or the smaller search space
        is as good as the original one.
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
    - ICML 2019
    - contribution
        - scale the network width (channels), depth, and input image size at the same time
    - others
        - apply an existing work (same author) to search a network with fewer target flops and 
          use flops as the cost measurement rather than hardware cost
            - MnasNet: Platform-aware neural architecture search for mobile
- ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
    - ECCV 18



# RL
- Policy Gradients in a Nutshell
    - [blog](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)

# nlp
- Recursively Summarizing Books with Human Feedback
    - arxiv 10/2021
    - key
        - tree-like redcursive summarization for a long sentence
- Scatterbrain: Unifying Sparse and Low-rank Attention Approximation
    - arxiv 10/2021
    - key
        - combine sparse-like attention and low-rank attention. it seems like
          promising
- CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models
    - arxiv 9/2021
    - key
        - incteresting work
        - the region, extracted from VinVL, is marked with a color. Then, the
          task is to predict the color
- Robust Open­Vocabulary Translation from Visual Text Representations
    - arxiv 9/14
    - key
        - make text as image to extract features
- infinity-former: Infinite Memory Transformer
    - key
        - sample a limitted inputs and then model a continous function
- Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing
    - arxi 7/2021
    - nice survey paper on prompting
- FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS
    - arxiv 9/2021
    - key
        - decoder only structure
        - only the pretraining, do the fine-tuning on many tasks. Each task is
          also re-phrased as generation task with some templates.
        - with this fine-tuned model as the pretrained model, it can be better
          than GPT-3
- Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation
    - arxiv 9/2021
    - key
        - style transfer. The problem is that given sentence X and Z, generate
          sentence Y. Y should have the same content with X, but with the style
          of Z.
        - one content encoder to encode X; one style encoder to encode Z. The
          decoder's input is the encoded X and the encoded Z, to generate Y.
        - contrastive loss is applied on content encoder and style encoder.
        - fully supervised.
- UNIFIEDQA: Crossing Format Boundaries with a Single QA System
    - EMNLP 2020
    - key
        - text-to-text to cover most of QA tasks. Use T5 or BART model
- QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering
    - arxiv 6/2021
    - key
        - combine language model and knowledge graph for question answering
        - based on the query and posible answers, get the langauge model
          information, and query the knowledge graph to get a sub-graph
          relevant to the query. The question context is then modeled as a node
          together with the sub-graph. Then, apply the grapha network to model
          the results.
- On the Effect of Dropping Layers of Pre-trained Transformer Models
    - key
        - discuss different strategies to drop the pre-trained layers
          simply. The conclusion is that dropping the top layers is a good
          strategy.
- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
    - T5
    - key
        - encoder-decoder with masked language modeling performs better than
          prefix language model
- DeBERTa: Decoding-enhanced BERT with Disentangled Attention
    - iclr 2021
    - key
        - 1.5 billion parameters
        - surpass human accuracy on super-glue
        - positional encoding with context encoding.
- Big Bird: Transformers for Longer Sequences
    - key
        - local attention + some tokens are global token which can attend all
          others
- ByT5: Towards a token-free future with pre-trained byte-to-byte models
    - arxiv 5/2021
    - key
        - tokenizer is byte by byte, that is 256 token size
- Language Models are Unsupervised Multitask Learners
    - key
        - gpt 2
        - more dataset, larger backbone
- Improving Language Understanding by Generative Pre-Training
    - key
        - gpt 1
        - only one decoder transformer network.
        - pre-training and then fine-tuning
- plug and play language models: a simple approach to controlled text generation
    - key
        - langauge model is fixed.
        - the idea is to update the hidden states including the keys and values
          in the transformer network, so that the output could tend to generate
          desired outputs. the update is based on the gradient, so the key is
          how to define the loss
        - control code can be used in two ways
            - a bag of word
                - the loss is based on the sum of the probability over all
                  those expected words
            - a discriminator or a classifier
                - the loss is based on this classifier loss where the feature
                  is from the language model last hidden state
- CTRL: A Conditional Transformer Language Model for Controllable Generation
    - key
        - code is released
        - generate sentence based on control code and prefix as well.
- Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training
    - arxiv 6/2021
    - key
        - visual encoder is swin
        - pre-training objectives
            - itm and mlm
            - mask the region and use regression, carefully sampled which
              token to mask
- LoRA: Low-Rank Adaptation of Large Language Models
    - arxiv 6/2021
    - key
        - the update matrix for a linear layer is AB, where A is Dxr and B is
          rxD. r is much smaller than D. Thus, the hardware requirement can be
          reduced. comment: the training time should be increased.
- GeDi: Generative Discriminator Guided Sequence Generation
    - key
        - related to controllable text generation
- multiple-attribute text rewriting
    - iclr 2019
    - key
        - back translation to learn the style transfer.
        - problem: input is text and requested style, output is the stylized text
        - train: input + requested style -> stylized text. then stylized text +
          original style -> original text
        - code is released
- Controllable Generation from Pre-trained Language Models via Inverse Prompting
    - 3/19/2021
    - key
        - when doing the beam search, it changes how to evaluate the candidate.
          The goal is that if the geenrated answer as the prompt and the prompt
          as the generated answer, the probability should be high. This
          probability is used as the indicator of how good/bad the candidate
          sentence is.
- Going deeper with Image Transformers
    - arxiv 4/2020
    - key
        - per-channel weight so that at the very beginning, the residual branch
          contributes less to the result.

- Longformer: The Long-Document Transformer
    - arxiv 4/2020
    - key
        - use window to do attention. similar like conv layer
        - use dilated window to extend the receptive field
        - lower layer, shorter window size
        - earlier in training, shorter sentence lenght/window size
        - closer to the end of the training, longer sentence/longer window size
            - 2048 -> 23040
- hierarchical attention networks for document classification
    - arxiv 2016
    - key
        - attention is applied to each word to calculate the sentence-level
          representation
        - each sentence has one representation. For one document, then apply
          attention to formulate document-level representation
- Uxploring the limits of transfer learning with a unified text-to-text transformer
    - 2015 
    - provide a dataset, named Colossal Clean roweled Corpus (C4) and did a lot
      of experiments.
- Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension
    - arxiv 10/29/2019
    - key
        - the input is sent to encoder with bidirectional modeling
            - this input can be corrupted with any scheme
        - a decoder is used with seq2seq model
- Distilling Knowledge Learned in BERT for Text Generation
    - arxiv 7/2020
    - key
        - the teacher network is based on a different task from the teacher's
          task. The teacher network is based on bi-directional mask language
          modeling, while the student is based on seq2seq modeling task.
        - if teacher is also based on seq2seq modeling, teh accuracy even drops
          compared with the case without teacher network. This is kind of
          strange.
- Pointer Networks
    - arxiv 2015
    - key
        - there is no vocabulary, and the output is always the input. Thus,
          only one softmax is used there.
- Get to the point: Summarization with pointer-generator networks
    - arxiv 2017
    - key
        - the task is text summarization
        - the solution is based on seq2seq model.
        - pointer-generator netowrk is used. That is, the network predicts the
          probability of using the word from the input text by 1) one
          proability about the contribution from the input text and vocabulary,
          and 2) one distribution on the probability of input text tokens,
          which is the attention weight rather than from another network's
          prediction.
          Then, it combines the two distribution to merge the intersected part.
        - added another loss about coverage loss. That is, it tracks the tokens
          the network predicts and try not to predict the same words, as one
          issue in previous work is to generate duplicate words in the output.
- Taking Notes on the Fly Helps BERT Pre-training
    - arxiv 8/2020
    - add a memory bank to contain the surrounding contextural information of
      rare words. The bank size is pre-defined based on how rare the word is.
      The entry is updated by the average of surrounding vectors of this word.
      During pre-training, the token vector is also averaged with the entry in
      the bank.
    - improves the accuracy from 82.5 to 83.8 in GLUE tasks.
- Contrastive Distillation on Intermediate Representations for Language Model Compression
    - arxiv 9/2020
    - contribution
        - each sentence is represented as one vector
            - the vector is an average pooling results from all layers
            - it compares with to use CLS as the representation vector, which
              has 0.5 ponits' drop
        - use a memory bank here to keep the vector for other instances
            - not use the queue-based memory bank
        - negative samples are sampled based on the sentence in current
          min-batch
- MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers
    - arxiv 2/2020
    - contribution
        - two losses. one is kl losss for attention probability. The other is
          the kl loss for the relationship between values. That is, for the
          value matrix
        - only apply to the last transformer layer
- DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
    - arxiv 3/2020
    - contribution
        - apply distillation on pre-training stage only.
        - reduce the number of layers by half. The initialization is also done
          by sampling one every two layers from original BERT model.
- Small and Practical BERT Models for Sequence Labeling
    - arxiv 8/2019
    - contribution
        - 3 layers' BERT callced miniBERT
        - disllation on the cross entropy part
- Distilling Task-Specific Knowledge from BERT into Simple Neural Networks
    - arxiv 3/28/2019
    - contribution
        - the student model is not a simple bert model but BiLSTM.
        - only apply on cross entropy loss
        - the distillation is only based on mimicking the logits (before
          softmax). No distillation loss on the feature maps as the network
          architecture is quite dissimilar.
- TinyBERT: Distilling BERT for Natural Language Understanding
    - arxiv 9/23/2019
    - contribution
        - use a linear mapping function to bridge two features with different
          dimensions
        - alignment loss between embedding feature, attention maps, transformer
          hidden state, cross entropy loss
        - data augmentation is used during downstream task fine-tuning to
          enrich the dataset. The augmentation is based on word replacement
          with BERT and GloVe.
        - distillation is performed on both pre-training and fine-tuning.
- MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices
    - 12/4/2019
    - contribution
        - small network
        - distillation study, including distillation in the cross entropy loss,
          feature map, and attention map. all works to improve the accuracy,
          but from Table 9, feature map transfer works the best.
        - distillation strategies. compared 3. The first one is to add
          distillation loss as auxiliary loss during pre-training, but it works
          the worst. The second is that first distill the feature map and
          attention map and then re-fine the network with the traditional loss
          with cross entorpy distillation
        - removing layer norm and gelu and replacing it can increase the speed
          a lot, but drops the accuracy.

# video
unsupervised discovery of actions in insturctional videos
- arxiv 6/2021
- key
	- the problem is action segmentation
	- during training, tehre is no ground-truth. it only uses the video without the subtitle
	- the model is stochastic based on gumble softmax. That is, each forward could generate different action predictions for all frames. The basic idea is to generate multiple action sequences, and then score them. The one with the highest score is the pseudo ground-truth.
	- it is autoregressive to generate the action prediction.
- CLIP2Video: Mastering Video-Text Retrieval via Image CLIP
    - arxiv 7/2021
    - key
        - apply CLIP model to video retrieval
- Video Swin Transformer
    - arxiv 6/2021
    - key
        - apply the swin transformer to the video classification problem.
        - achieves the new sota accuracy
- Towards Long-Form Video Understanding
    - cvpr 2021
    - key
        - defined 9 video classification and regression tasks. The video length
          is kind of longer than the existing datasets, although not super long
- VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning
    - arxiv 6/2021
    - key
        - input is video only. no language, no audio, no transcript
        - pre-training task is MLM and sim-clr version of contrastive loss
            - each frame is tokenized by a pre-trained VQ-VAE module
        - pre-training, the first 90 epochs are with smaller input size (128). the
          last 10 epochs are with larger input size (256)
        - temporal position encoding + x positional encoding + y positional encoding
- End-to-end Temporal Action Detection with Transformer
    - arxiv 6/18/2021
    - key
        - apply DETR to video segment problem.
        - the frame is extracted with a pre-trained feature extractor
        - then use the encoder-decoder framework to predict the segment
- ViViT: A Video Vision Transformer
    - arxiv 3/29/2021
    - key
        - multiple design choices for video transformer
            - for token sampling, one is to tokenize each frame indepdently;
              the other is to sample the cubic region.
            - for the model, it desgins multiple ways of teh self-attention.
              factorized or not
- Is Space-Time Attention All You Need for Video Understanding?
    - from Facebook
    - key
        - temperal attention + spatial attention. make it seperable which
          improves the accuracy as well comopared with teh full attention
          mechanism.
- Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling
    - cvpr 2021
    - key
        - based on pixel-bert, but the input is the video. sampled video frames

# vl - visual language
- TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models
    - arxiv 9/21/2021
    - key
        - image transformer as image encoder
        - add decoder network
        - input is the textline images
        - BeiT as the image encoder gives better performance than DeiT
- VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding
    - emnlp2021
    - key
        - video: cnn + transformer
        - text: transformer
        - careful design of positive pairs and negative pairs
- TxT: Crossmodal End-to-End Learning with Transformers
    - arxiv 9/2021
    - key
        - DETR as the image encoder.
        - useless paper. vqa is less than 70
- Towards General Purpose Vision Systems
    - arxiv 9/2021
    - key
        - DETR as the image encoder, can do localization
        - BERT as the text encoder.
        - performance is quite worse
- Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training
    - arxiv 8/21-2021
    - key
        - pixel bert but with the image encoder from In-defense grid feature
- Attntion Bottlenecks for Multimodal Fusion
    - arxiv 6/30/2020
    - key
        - interesting paper
        - use some 4 extra tokens as the bridge to exchange information among
          modalities.
        - this could save the computational cost a lot
- Multi-Modality Cross Attention Network for Image and Sentence Matching
    - iccv 2020
    - key
        - the paper is specifically focused on image-text retrieval part
        - for each image, there is a representation based on fster-rcnn and
          transformer network
        - for each text, there is another representaiton based on the
          transformer network
        - the two modalities are merged together and fused by another
          transformer network
        - the loss is applied on the inner product of the unimodal features and
          the ininer product of the crossed-fused representation.
- Large-Scale Adversarial Training for Vision-and-Language Representation Learning
    - nips 2020
    - key
        - apply the adversarial training on the features
        - during the pre-training, only the last few iterations adopts the
          adversarial training
        - improves on standard tasks.
- LAMP: Label Augmented Multimodal Pretraining
    - arxiv 12/2020
    - key
        - also use teh label for vl pretraining.
        - teh pretraining is first done on all pretraining dataset and then
          done on the fine-tuning dataset as well, which is called
          second-setage pretraing.
        - the best accuracy on vqa v2 is 72
- UniT: Multimodal Multitask Learning with a Unified Transformer
    - key
        - differdent encoders are used for each modality
        - a decoder is designed for each task
        - the encoder output is sent to the decoder directly or they
          concatenate the two modalities to form the input of the decoder
- VirTex: Learning Visual Representations from Textual Annotations
    - key
        - use captioning task to do encoder pretraining
        - the downstream is imagenet classification
- VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning
    - arxiv 2/2021
    - key
        - image encoder + pretrained gpt. the signal from image is used as gate
- Align before Fuse: Vision and Language Representation Learning with Momentum Distillation
    - arxiv 7/2021
    - key
        - from saleforce, good paper
        - moving-averaged teacher network is used also
        - image encoder, text encoder, joint encoder
        - after unimodel encoder, a contrastive loss is applied
        - text encoder is the first half of the bert-base, fusion transformer
          is the second half of the bert-base.
        - image encoder is Vit-B/16
        - for vqa
            - a decoder is used
        - for retrieval
            - first to use the representation from the unimodel encoder and
              then use the conf from itm
- DocFormer: End-to-End Transformer for Document Understanding
    - arxiv 6/2021
    - key
        - for document understanding
        - input of transformer includes the OCR text, teh location infromation,
          and the raw image, which is fed into r50 to extract the features
- Neural Fashion Image Captioning : Accounting for Data Diversity
    - key
        - released a dataset about fashion image. teh description is also
          auto-generated based on some template.
- multimodal few-shot learning with frozen language models
    - arxiv 6/2021
    - key
	    - T5. image encoder is a resnet50 without normalization. only image encoder is learned on cc dataset.
	    - during inference, the image is as prefix and can be combined with other image-text hints so that it can be treated as few-shot learner.
- CPTR: Full Transformer Network for Image Captioning
    - arxiv 1/2021
    - key
        - pass the image to vit, then add a decoder for image captioning
        - no joint encoder there
- Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs
    - arxiv 5/30/2021
    - key
        - renamed as Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs
        - more like to analize the performance of multiple vl work
        - use the attention to generalize the one-stream and two-stream, but it
          seems useless
- Unifying Vision-and-Language Tasks via Text Generation
    - iclr 2021
    - key
        - in prompt, it adds some prefix, e.g. vqa to describe different tasks
        - 36 faster-rcnn region features
- M6: A Chinese Multimodal Pretrainer
    - arxiv 3/2021
    - key
        - the image is plit into patches. For each patch, the feature is the
          teh feature from r-50.
- Contrastive Visual-Linguistic Pretraining
    - arxiv 6/2020
    - key
        - a key network or a copy of the current network, but optimized by the
          momentum update rather than the gradient. similar like moco
        - the image input to the query netework is randomly masked.
        - the image input to the key netework is not randomly masked
        - the output of the key network is inserted to a memory queue for
          contrastive loss calculation
        - the contrastive is only performed on the image domain to build the
          contrastive loss.
        - the key message is that such contrastive loss is better than feature
          regression and feature label classification
        - random drop path is applied on teh key encoder after some epochs
- Automatic Curation of Large-Scale Datasets for Audio-Visual Representation Learning
    - arxiv 1/2021
    - key
        - filter the pre-training dataset, which consists of video-audio pairs.
          The target is to find out a subset with good correspondence
        - two method proposed. one is based on contrastive learning, which is
          effectively CLIP. the other one is based on clustering results. That
          is, do the clustering on each modality with different features and
          the calculate the correlation of the clusters
        - finaly filter out 10 Million video data
- CUPID: Adaptive Curation of Pre-training Data for Video-and-Language Representation Learning
    - arxiv 2021
    - key
        - proposed two methods to do filtering on pre-training dataset so that
          the pre-training dataset is more consistant with downstream task
          - one is to maek sure the title contains at least one word from the
            downstream dataset
          - the second one is to compare teh similarity of the video based on
            visual features + pooling.
- Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning
    - arxiv 4/2021
    - key
        - based on pixel-bert. same author
        - for eaach visual token, find the nearest neighbor as the encoded
          feature
        - if the cluster center is c, to calculate the gradient c = stop_grad(c - x) + x, so that the gradient on c can be 
          propagated back to x.
- VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text
    - arxiv 4/2021
    - key
        - similar with CLIP
        - it has audio-video, video-text loss
    - arxiv 4/2021
    - key
        - use CLIP as initialization.
        - for video, CLIP to extract the features for each frame. Then, use a
          mean pooling to calculate the similarity
        - or add some similarity network to encode the tempory information
        - achieve sota result
- Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval
    - arxiv 4/1/2021
    - key
        - similar with OpenAI's CLIP
        - the visual input could be single frame (image) or multiple frames. no
          matter what it is, [cls] token is used to represent the visual
          signal
        - first to train with fewer time span and then on longer time span,
          which is helpful. cirriculum learning
- Neural Baby Talk
    - cvpr 2018
    - key
        - pointer network to determine the template. Template is also a caption
          sentence, but each token can be either a textural word from the
          vocabulary or a visual word. A visual word is associated with a
          region and the word is determined by the sub captegory and the
          plural. The network output is a probability distribution over N + 1,
          where N is the number of region and +1 means the textual word.
- LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval
    - NACCL 2021
    - key
        - region feature, bert
        - text, bert.
        - inner product to calculate the similarity
- HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training
    - arxiv 5/2020
    - key
        - multiple losses for pre-training
- VisualCOMET: Reasoning about the Dynamic Context of a Still Image
    - eccv 2020
    - key
        - share a dataset
        - proposed a task
        - the task is to predict what happened before, what is happening, what
          will happen after, given a still image.
        - in the dataset, the image is extracted from video, and the gt is
          based on the manual describing based on two frames before current
          frams and other two frames afterwards.
- ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision
    - arxiv 2/5/2021
    - key
        - a good paper to use one transformer for VL tasks
- Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision
    - Google-version of CLIP
    - key
        - the temperature in the similarity is learnable by gradient.
            - ablation experiments show that the temperature can be manually
              tuned to have a little bit better accuracy. But as the gain is
              not significant, learnable setting is better.
        - result shows better improvement than CLIP.
        - in coco 5k retrieval tasks
            - clip i2t@1: 58.4
            - this paper: 58.6
                - efficient-net-L2 + bert large
        - 1.8 billion image-text pairs
        - image input size is 289
        - 1.2 million steps (12 epochs)
        - batch size = 16384; 1024 TPUv3
- RoBERTa: A Robustly Optimized BERT Pretraining Approach
    - arxiv 7/2019
    - key
        - SQuAD v2.0 contains questions which are not answered.
        - removing the alignment loss or next sentence prediction is ok
- UniVL: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation
    - arxiv 6/2020
    - key
        - video -> extract feature by S3D -> transformer
        - transcript -> transformer
        - use one joint transformer. That is, based on two-stream
        - after that, append one decoder for generation tasks.
        - in pre-training, first pre-train the sepearte encoder, and then for
          all.
        - pre-training takes 14 days for 8 V100
        - pre-training task
            - one joint loss on the seperate streams
            - one mask language modeling loss
            - one mask feature modeling loss to applying the contrastive loss,
              i.e. masked feature and the correponding output is positive
              pairs. masked feature and all other outputs are negative pairs.
            - one alignment loss.
        - result is quite impressive
- Incorporating copying mechanism in image captioning for learning novel objects
    - arxiv 2017
    - copy mechanism for object labels. image feature is one global feature
- Video Understanding as Machine Translation
    - arxiv 12/2020
    - facebook
    - key
        - video pre-processing
            - uniformly select 32 frames for each video clip
        - network input
            - visual feature extraction
                - R(2+1)D-34 network
                - pre-trained on IG65M
                - the output is T * C_v * H * W. -> spatially average-pool to get
                  T*C_v -> linear embedding to get T * C
            - cls token is mapped by looking-up table from text embedding matrix
            - altered text as input
                - masked or shuffled
                - mask: randomly choose 0% to 100% tokens
            - task description as text input also
                - either classification or captioning
        - backbone
            - multimodal encoder
                - T5 model, a transformer model, pre-trained on C4 dataset
        - output
            - the output is N_t * |V|. V is the vocabulary. N_t is the number
              of output tokens. It is not auto-regressive process, but just a
              one pass to get the final matrix.

- Learning Transferable Visual Models From Natural Language Supervision
    - CLIP from openai, arxi 2/26/2021
    - key
        - constructed a dataset with 400 million image-text pairs
            - construct a query list, whose item includes
                - the terms which occurs more than 100 times in English version
                  of wiki
                - augmented with bi-grams with high mutual information as well
                  as the wiki titles (above a certain search volume)
                - merge with WordNet
                - query list: 500k
        - directly predicting the caption based on the image is less efficient.
          it is about 3 times less efficient compared to a simple approach
          which pridicts a bag of word embedding.
        - the proposed approach is: in one batch, N images; N text; construct a
          similarity matrix NxN. Row-wise: i-throw: the gt label is (i, i) ->
          CE loss. Column-wise: j-th column, gt label is (j, j). CE loss.
          - implicitly assumes the text are different within the same batch
        - image encoder can be r50 or ViT
        - text encoder is based on byte pair encoding. vocab size is 49k. max
          length is 76.
        - in zero-shot evaluation, the system should be given a set of the
          target label names. Each label is embeded with the text encoder, and
          then then image can be encoded and compared with all text description
          with cosine distance. The label name is also augmented with a photo
          of a dog, or a photo of a large dog. Each label can be augmented
          multiple times, and then the emebedding is averaged.
- Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge
    - arxiv 12/2020
    - key
        - grid feature
            - the feature is grid feature from resnext-101, not r101. The gain is
              10 CIDEr
            - the feature is from 99th layer and forming 14x14x2048.
        - ocr feature
            - the image is rotated with 0/90/180/270 degress and each image is
              processed by ocr engine. The one with the largest number of fastText
              token is selected.
            - fastText representation is also used
            - max 20 words
        - object detection
            - use efficient-det to only extract the bounding boxes. the conf
              threshold is 0.25
            - the class name is encoded by fastText. no object feature is used
              here.
            - maximum is 10 objects
            - the detector is the released detector on coco
        - above 3 modality is mapped to 512 dimensions by a linear projection
        - it also uses teh copy mechanism and dynamic vocabulary, but it is not
          clear about the details. May need to check reference work on copy
          mechanism.
        - bert tokenizer
        - ensemble. at each step, the probability is averaged over all models,
          and choose the highest confidence.
        - ensemble from 90 models
- UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning
    - arxiv 12/31/2020
    - key
        - data augmentation
            - back-translation to generate positive captions
            - use scene-graph to replace the entity/attribute/relations to
              generate negative captions
        - use contrastive loss to distinguish those positive and negative
          pairs.
        - use text-only data also
- In Defense of Grid Features for Visual Question Answering
    - a nice paper, arxiv 1/2020
    - contribution
        - grid feature fine-tuned on detection dataset or converting a
          detection model so that we can extract the grid feature.
        - as good as the region feature. Note, the number of grid features
          depends on the input size, and all features are used.
        - end2end training together with the detector works a little bit
          better.
        - the loss weight for the attribute on vg dataset is important also
- Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions
    - arxiv 20/2020
    - key
        - in pre-training, the input is either text or image. Each adopts the
          masked reconstruction loss
        - tag is used for image domain. Teh tag is appended with the spatial
          coordinates.
        - also use the object tags as an additional input to the BERT model.
        - pre-training data is CC only. In each iteration, either a batch of
          images are selected or a batch of text descriptions. BookCorpus is
          used as external text domain. 1.7M openimages are used here for
          external images.
        - combining the aligned and non-aligned gives better accuracy
- Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks
    - 2020 ECCV
    - design
        - data
            - image
                - region feature
- Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training
    - 2019/12
    - design
        - data
            - image
                - region feature
                - provided in Pythia. not clear if it is peter anderson's
- VisualBERT: A Simple and Performant Baseline for Vision and Language
    - design
        - data
            - image
                - region features
                - should be also peter anderson's detector, but the wording is
                  standard detetor.
            - image - text
                - segment embedding
            - text
                - bert-like
        - model
            - BERT
        - task
            - masked language modeling
                - mask the text and predict it
            - matching loss
                - each image is paired with 2 sentences. One is the correct
                  one, and the other one has 50% chance to be a random caption.
                  The network is to distinguish them.
- ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph
    - arxiv 2020/6
    - design
        - data
            - text
                - wordpiece tokenizer + position embeddding
            - image - text
                - segment embedding
            - image
                - frcnn region feature
                - a full region covers all the image content is also used.
                    - same as ViLBERT
                - the location is encoded as 5-dimensions: normalized top-left,
                  bottom-right and area ratio
                - boxes with confidence higher than 0.2 is kept; each image has
                  10 to 36 boxes
        - model
            - same as ViLBERT
        - task
            - masked laungauge modeling
                - for each sentence, it first parses it to scene graph. Each node
                  in teh graph is a word in the sentence with the type being
                  object, attribute or relationship between objects
                - object prediction
                    - randomly select 30% of the objects to mask. Each object
                      corresponds to a sub-stentence in the text. Each selected 
                      object will be replaced with [mask] with a prob of 90%;
                      random token with 10%; or kept the same with the rest 10%.
                - attribute prediction
                    - almost the same as what object prediction does. But for each
                      masked attribute, it makes sure the object associated with
                      not be masked.
                - relationship prediction
                    - almost also the same as what attribute prediction does, but
                      the paper does not mention the probability. also 30%.
            - image-text matching task
            - masked region modeling
                - similar like  ViLBERT. mask the region feature as 0 and then
                  align the probability distribution with k-l distance. 15%
                  chance
    - experiment
        - pre-training data
            - CC and SBU
        - visual commonsense reasoning
            - Q->A
                - Each A is paired with teh image; prediction is based on inner
                  product of image and text representation, folllowed by a
                  linear layer.
            - QA->R
                - not mentioned how. it only says same as Q->A
            - pre-training on this vcr dataset is also performed before
              fine-tuning
        - visual question answering
            - question-answer from vg is also used
            - collected 3129 answers. Given an image, each answer is mapped to
              a score.
            - 2-layer mlp
- InterBERT: An Effective Multi-Modal Pretraining Approach via Vision-and-Language Interaction
    - arxiv 2020/3
    - design
        - data
            - text
                - WordPiece tokenizer + position embedding
            - image - text
                - segment embedding
                    - distinguish image and text
            - image
                - faster-rcnn trained on vg
        - model
            - first full attention and then seperate attention for image and text
              individually.
        - task
            - Masked group modeling (proposed in this paper)
                - masked segment modeling
                    - 10% prob to mask current word as anchor
                        - if masked
                            - 1/3 prob to mask 0 or 1 or 2 following consecutive
                              words
                            - predict loss
                - masked region modeling
                    - 10% prob to mask current object region as anchor
                        - if masked
                            - mask all other regions which have IoU larger than
                              0.4.
                            - predict the category by cross entropy loss
            - image-text matching loss
                - 50% prob for positive/negative pairs.
    - experiment
        - grouped version vs non-groupped version modeling
            - VCR task
                - Q->A: 72.3 -> 73.1
                - QA -> R: 74.3 -> 74.8
                - Q -> AR: 54.0 -> 54.9
            - there is improvement but minor
        - extraction model vs bert-base vs single stream pre-trained on
          multi-model data
            - tested it on 8 NLP taskes
            - on avarage, BERT-based achives 82.0; single-stream achieves 80.0.
              The proposed achieves 81.8
- UNITER: UNiversal Image-TExt Representation Learning
    - arxiv 2019/9
    - data
        - visual feature
            - region feature is extracted from frcnn, trained on vg with object and
              attribute data
            - region location feature is 7-dim: 4 normalized top-left/bottom-right
              coordinates + width + height + area.
            - the above two fewaturs are projected to same dim by one
              fully-connected layer and then sum them up. 
        - text feture
            - tokenize based on what BERT does
            - position embedding also
        - task design
            - mask laugugage modeling by replacing a tet token with <mask>
                - 15%
            - visual feature is randomly masked by zeroing out the features.
                - each time, only one modality is masked
                - each iteration, only one tsk is enabled
            - image-text matching
- Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers
    - cvpr 2020
    - only use spatial feature rather than region-level feature.
- VL-BERT: Pre-training of Generic Visual-Linguistic Representations
    - iclr 2020
    - design
        - data
            - text feature as in BERT
            - sequence position embedding
            - image region feature from frcnn + 4-D location (top-left,
              bottom-right) projected by cosine/sine functions of different
              wavelengths
                - peter anderson's
            - segment embedding is added. sentence a, sentence b, image. Three
              types
            - 50% of the images are these visual-linguistic data; the rest is pure
              text dataset. (BooksCorpus)
        - model
            - faster-rcnn is also fine-tuned
        - loss
            - masked language modeling loss. This is the only loss if the input is
              pure text without images.
            - masked roi classification loss. the gt is the frcnn classification
              output. masking is performed by zero out the regions before sending
              it to frcnn instead of to make the feature values as 0
    - experiment
        - image-sentence pair matching loss hurts the accuracy, and thus this
          paper does not use this one.
        - frcnn parameter is also tuned.
- LXMERT: Learning Cross-Modality Encoder Representations from Transformers
    - EMNLP 2019
    - model:
        - first with the transformer for each modality
        - then add a cross-attention transformer. named as co-attention in
          ViLBERT
    - data:
        - image region feature + position embeding (project it to the same
          dimension)
            - apply LayerNorm for region feature and position embedding
            - peter anderson's detector
        - text feature. Embedding + position embedding
            - apply LayerNorm after the sum
        - append [CLS] before the language sentence
    - loss
        - masked language modeling, 15% prob to mask the words. for text
        - 15% prob to mask the region feature to zero. Then predict
            - feature by regression with l2 loss
            - detected label, which is from frcnn model
        - image-caption matching loss. 50% prob to make it a mismatch
        - predict the answer if the input is a question and there is an answer
          only when the input is matched
- VL-BERT: PRE-TRAINING OF GENERIC VISUALLINGUISTIC REPRESENTATIONS
    - arxiv 2020/2
    - design
        - data
            - text
            - image region
                - peter anderson's detector
- ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
    - nips 2019
    - design
        - data:
            - image region features, 10 to 36 regions per images. feature +
              projection of (top-left, bottom-right, area ratio)
                - peter anderson model
            - append [img] token where the whole image as the region
        - model:
            - seperate stream first and then a co-attention module
            - proposed a co-attention module which exchange the visual and laungage
              representation in the transformer modules.
        - task
            - 15% chance to mask region and text.
                - For the masked image region, 90% of chance to zero the feature
                  out and 10% of the chance there is no change on the text.
                - No mention of the chance to mask the text with [mask],
                  or a random text or unaltered.
            - for the image region task, the task is to align the probability score from the bert model
              to the detection classification score in the faster-rcnn model with
              k-l distance
            - masked laungage model to predict the masked word

# Document Understanding
- LayoutLM: Pre-training of Text and Layout for Document Image Understanding
    - arxiv 6/16/2020 from MSRA
    - 2d encoding based on two lookup tables for each region.

# network structure
- P2T: Pyramid Pooling Transformer for Scene Understanding
    - arxiv 7/10-2021
    - key
        - nice paper
        - in attention module, the key and value are not the input signals, but
          the pooled results of the input signals. The pooling here is multiple
          pooling operaitons. In this case, the number of tokens in key and
          values is much smaller than the input. Pyramid design is also
          incorporated.
        - results look quite good
# network structure/Graph Network
- Graph Attention Networks
    - arxiv 2018/4, iclr 2018
    - a very nice paper which improves the representation of each node by the
      adjacent nodes with the attnetion weight


# meta learning
- Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
    - icml - 2017
    - the goal is to optimize the loss where the parameters should be adapted
      with a training sample. Each training sample here means a task which
      consists of n-way k-shot samples.
# Few-short learning
- Learning to compare: Relation network for few-shot learning
    - arxiv 3/2018
    - each image in teh support set and query set is encoded by a learnable
      network. The query's feature is concatenated with each support feature.
      Another network is to map the concatenated feature to a scalar to indicte
      the relationship begtween the query and the feagture in the support set.
      This is for one-shot learning case. For multi shots, all features within
      the same classes are avereged to have 1 feature vector.
- Matching networks for one shot learning
    - arxiv 2016/6
    - similar like a knn classifier
- Edge-Labeling Graph Neural Network for Few-shot Learning
    - arxv 2019/5, 51 citation
    - good writing, nice idea
    - update the edge vectors as well
- Laplacian Regularized Few-Shot Learning
    - icml 2020
    - very elegent optimizaation method to solve teh class prediction problem.
- DPGN: Distribution Propagation Graph Network for Few-shot Learning
    - cvpr 2020
    - based on graph cnn
    - use two graphs. one is the point graph; the other is called distribution
      graph.
- Generalizing from a Few Examples: A Survey on Few-Shot Learning
    - archiv, 5/13/19. worth reading next time

- Infinite Mixture Prototypes for Few-Shot Learning
    - For each class, it designs multiple prototypes to represent. Different
      classes might have different numbers of prototypes. The way how to
      generate is based on the sample's distance to the prototype. If it is
      larger than some pre-defined value, then create a new one.

# Metric Learning
- Multi-similarity loss with general pair weighting for deep metric learning
    - arxiv 3/2020
    - a loss is proposed which is quite similar with the contastive loss.
- Cross-Batch Memory for Embedding Learning
    - arxiv 4/2020
    - idea
        - use a memory bank, implemented as queue to increase the negative
          samples.
        - the application is the supervised learning for few-shot learning
          case.
# Graph Neural Network
- A Comprehensive Survey on Graph Neural Networks
    - A good survey.

# Reinforcement Learning (RL)
- a very nice [introduction](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)


# Vision Language
- DeViSE: A Deep Visual-Semantic Embedding Model
    - nips 2013
    - the idea
        - each label is represented by a vector learned from laungage model
        - in the training, the image is targed on this soft target
    - experiment
        - if the labels are treated as a flat structure, the cross entropy loss
          is still the best. For example, top-1's acc is 55.6; while the
          proposed is 54.9.
        - if we take into account the hirachical structure of the labels, and
          have a newer metric to account for the strucutre, the accuracy of the
          proposed is better. For example, the acc at top10, the traditional
          cross entropy is 31.3; while the proposed is 33.1

# Text Detection
- ReLaText: Exploiting Visual Relationships for Arbitrary-Shaped Scene Text Detection with Graph Convolutional Networks
    - arxiv 3/16/2020
    - the approach is
        - detect the text primitive
            - partition the text (multiple words) to input N partitions
            - based on FPN
        - enhance the feature for each primitive
            - considering the spatial relationship between pairs
            - input is the roi-align feature
            - implemented by multi-linear perception
        - classify if there should be an edge between primitives
            - the feature of the edge is the concatenation of the primitive
              features and the location informatoin
            - binary classifier problem.

# Instance segmentation
- SOLOv2: Dynamic and Fast Instance Segmentation
    - 10/3/2020
    - the mask branch is based on a dynamic convolutional operation. That is,
      the kernel is the output of another layer. In this case, the space can be
      reduced and the computational cost can also be reduced.

# object detection
- DEFORMABLE DETR: DEFORMABLE TRANSFORMERS FOR END-TO-END OBJECT DETECTION
    - key
        - lernable offset around each point, to geenrate the keys, instead of
          using all keys. The goal is still to explore the locality
- Towards Open Vocabulary Object Detection without Human-provided Bounding Boxes
    - arxiv 11/2021
    - key
        - from Salesforce
        - bad writing
        - the key idea is that to use the attention map as the indictor to tell
          which box correpsonds to teh word in the caption. The box is
          generated from proposal generator, which is mask-rcnn here.
        - the paper is claimed not to use box annotation during training, but
          the proposal generator it uses is based on mask-rcnn which was
          trained on the coco by removing the novel category. In experiments,
          it also studied the performance if the proposal generator is replaced
          with selective search, in which case the performance is almost
          halved.
- Benchmarking Detection Transfer Learning with Vision Transformers
    - arxiv 10/2021
    - key
        - nice work of expeirment results
- PIX2SEQ: A LANGUAGE MODELING FRAMEWORK FOR OBJECT DETECTION
    - arxiv 9/2021
    - nice paper to geenrate sequence as detected output
- Tracking Instances as Queries
    - based on instances as queries.
- Instances as Queries
    - key
        - use object queires. lots of details. not well caught the details
- Probabilistic two-stage detection
    - arxiv 3/2021
    - use one-stage detector to replace the rpn and multiply the probability
      from rpn to the final classification score. 
- End-to-End Object Detection with Fully Convolutional Network
    - arxiv 12/2020
    - key
        - remove the NMS
        - one-to-one mapping based on the multiplication of the confidence and
          the IoU. confidence has an exponental factor of 0.2. Iou has a factor
          of 0.8. If it only relies on confidence or regresion, the accuracy is
          much worse (more than 10 points' gap).
        - added one module based on 3-d max pooling, but not sure the
          underlying reason behind it.
        - on coco, the accuracy is comparable with fcos + nms. On CrowdHuman,
          it is much better, and the reason could be the NMS as NMS could
          easily make mistake in crowded scene.
- Sparse R-CNN: End-to-End Object Detection with Learnable Proposals
    - arxiv 11/2020
    - key
        - the proposal box is learnable. it is not from RPN.
        - the proposal feature is also learnable.
        - iteratitve prediction is used for predicting the box coordinate and
          category.
- OneNet: Towards End-to-End One-Stage Object Detection
    - arxiv 12/2020
    - key
        - one point as positive. The matching cost is based on location loss and
          classification loss as well, and use addition to combine.
        - only one feature map is used with stride = 4.
        - cascading the features.
- YOLOv4: Optimal Speed and Accuracy of Object Detection
    - 4/2020
    - designed several components to improve the od
- Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression
    - 11/2019
    - some variant of GIOU loss. DIOU and CIOU.
- AutoAssign: Differentiable Label Assignment for Dense Object Detection 
    - arxiv 7/2020
    - very interesting paper to make the assignment learnable.
- Improving Object Detection with Selective Self-supervised Self-training
    - eccv
    - label propagation with an extra network to judge if the psuedo gt is good
      or bad, or should be ignored.
- REPGN:OBJECT DETECTION WITH RELATIONAL PROPOSAL GRAPH NETWORK
    - 4/18/2019, arxiv only
    - apply the graph netowrk on the proposals. the gain is less than 1 point
      here on coco.
- DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution
- Bridging the gap between372anchor-based and anchor-free detection via adaptive training sample selection
    - cvpr 2020
    - contribution
        - an adaptive method for positive/negative assignment, which is based
          on teh center distance between teh anchor and the gt and the adaptive
          statistic information
        - overall, the imporovement is 1-2 points
- End-to-end object detection with Transformers
    - arxiv 5/2020, facebook
    - idea
        - similar with yolo, but the feature map is re-fused by transformer
          layers.
        - in the end, no aanchor is associated with the spatial locations. Each
          location predicts independent box coordinates and classification
          result
        - in the training, the assignment is based on the loss, which targets
          the matching (bipartite matching from the prediction to the ground
          truth) to have smallest loss.
    - roughly, it achieves 1 percent gain, compared with faster-rcnn
- RepPoints: Point Set Representation for Object Detection
    - idea
        - for the bounding box regression, predict 9 points, and use the
          min/max to get the bounding box. The loss is also based on this box.
- A deep learning pipeline for product recognition on store shelves
    - arxiv 12/2018, IPAS 2018 (IEEE International Conference on Image Processing, Applications and Systems)
    - approach
        - 1-class detector (yolov2)
        - feature extraction (triplet ranking loss)
            - only 1 image exists in the database; or 1-shot learning
            - the image is augmented to create the positive pair.
            - one network is to process both the reference image (clean) and
              teh raw image (not that high quality)
            - vgg16
            - l2 normalized
            - distance is based on the cosine similarity
        - k-NN
            - if the distance to the 1-NN and 2-NN is similar, the recoginition
              is discarded. The paper does not explain what it is by 'discard
              the recognition'.
        - re-ranking
            - by local feature matching
                - score is teh location similarity divided by the local feature distance.
            - by the macro category from other crops in the same shelf/image
                - check the macro category of other products in the same image
                  and use that to filter the ranking result.
    - experiment
        - without fine-tuning the feature extractor -> 21.49
        - fine-tuning -> 27.84;
        - fine-tuning + local feature reranking -> 32.34
        - fine-tuning + 1-NN/2-NN discarding -> 30.46
        - fine-tuning + filter by macro category -> 30.15
        - fine-tuning + local feature reranking + 1-NN/2NN discarding + filter
          by macro -> 36.02
- Dataset
    - Scale Match for Tiny Person Detection
        - release a dataset of TinyPerson
        - the new method is to scale extra dataset for pre-training so that
          teh size could be similar
- Teacher student
    - Mimicking Very Efficient Network for Object Detection
        - student network extracts the region proposal, which is used to
          extract the features from the student network and the teacher
          netowork. The idea is to align the extracted features.
            - The student network can also receive half-sized image as input. The paper discussed this optition, but there is no experiment about this, which is strange.
        - cvpr 17
- Domain adaptation
    - Few-shot Adaptive Faster R-CNN
        - CVPR 19
        - github was set by the author, but no code is shared (1/30/2020)
    - SCL: Towards Accurate Domain Adaptive Object Detection via Gradient Detach Based Stacked Complementary Losses
        - on arxiv 11/29/2019
        - cmu
        - setting
            - the source domain has full annotations (label and bounding
              boxes)
            - the target domain has no labels at all. No image-level labels
        - novelty
            - on the backbone, 3 branches are inserted with the domain
              classifier, so that the features could be in the same domain
              with reverse gradient policy
                - the loss can be cross entropy, weighted loss, focal loss.
                  The author studied the accuracy with different losses
            - context feature is extracted from the branches to combine
              with the region-level features. Thus, each region-level
              feature contains an image-level feature, which are finally
              used for classification and regression
- application
    - Model Adaption Object Detection System for Robot
    - EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional Neural Network Object Detectors
        - on arxiv 11/4/2019
        - looks like a hardware related paper. Not interested
    - RoIMix: Proposal-Fusion among Multiple Images for Underwater Object Detection
        - arxiv 11/8/2019
        - Peking University
        - Contribution
            - The region proposal is mixed up by another region proposal
                - the mix-up is performed on the features, not on coordinates.
                - the mix-up is performed by a linear combination
                - the label is not changed by the second region proposal. Thus
                  the weight for the current poposal is always larger than that
                  for the second proposal by performing max operation
                  (max(lambda, 1 - lambda))
                - the benefit is to mimic overlapping for underwater dataset
                  because it could be transparent in this case
- Small network
    - Localization-aware Channel Pruning for Object Detection
        - arxiv 11/21/2019
        - Huazhong Univerisity
        - reduce network parameters.
- Network architecture
    - Rethinking Classification and Localization for Object Detection
        - arxiv 12/2019
        - key idea
            - use two networks in box head to process the roi feature. One
              is based on fully connected layers; the other is based on
              conv layers. One approach is to add classification and
              localization loss on each head; the other is to add
              classification loss on the fully-connected head and the other
              is to add localization loss on the conv layer
        - experiment
            - one loss on each head
                - 37.3 -> 38.8
    - IoU-aware Single-stage Object Detector for Accurate Localization
        - arxiv 2019/12
        - claimed novelty
            - predict the IoU of the predicted box and the ground truth.
            - the final score is the IoU multiplied by the classification
              prediciton
            - this is the same as what Yolo V2 does.
        - best acc is 40.6 on coco -> X101-FPN-RetinaNet
    - Learning Rich Features at High-Speed for Single-Shot Object Detection
        - iccv/2019
        - novelty
            - downsample the raw image to multiple sub scales and fuse it
              with the feature maps from different levels
        - highest acc is 37.3 on coco with 32 ms for each image on Titan X
    - Guided Attention Network for Object Detection and Counting on Drones
        - 2019/9
        - novelty
            - 4 feature maps. 1/2, 1/4, 1/8, 1/16 for small scale
            - the loss is applied on the feature map of 1/2, not on all
              these 4 maps
            - a component of background attention module, which is used to
              fuse the feature map from higher level (smaller size) with
              the current feature map.
        - the best approach on CARPK is 90.2.
    - Attentional Network for Visual Object Detection
        - not that interesting.
        - 2017
        - no experiment on coco
        - Some recurrent network with reinforcement learning
    - Objects as Points
        - 2019/4 in arxiv.
        - Novelty
            - In Yolov2, the single feature map is with stride of 32. In
              this paper, it is 4.
            - The objectness is learned with focal loss. The objectness is
              called centerness here. 
            - the objectness is class-specific. That is, if we have 80
              classes, we have 80 feature maps. In YoloV2, it is
              class-agnositic
            - cneter offset is predicted as a class-agnositic way. In
              YoloV2, it is class specific
            - box size is predicted as class-agnositc way, which is the
              same as YoloV2.
        - experiment
            - the highest accuracy it can achieve is 42.1 with hourglass-104 as the backbone
    - Improving Object Detection with Inverted Attention
        - on arxiv 3/28/2019 
        - contribution
            - the feature map is re-weighted by its inversed gradient
                - the intuition is that the gradient is high on the most
                  discriminative regions. If we reverse it, we can focus on
                  less discriminative regions. Thus, the idea is try to
                  focus on the whole region part rather than the most
                  discriminative regions. However, it is not straigtforward
                  to conclude the accuracy would be better. Meanwhile, in
                  the experiment, only 20% features are re-weighted.
    - Enriched Feature Guided Refinement Network for Object Detection
        - ICCV 19
        - https://github.com/Ranchentx/EFGRNet
        - Tianjin University
        - Contribution
            - a framework to enchance the features used for prediction at each
              prediction layer
              - a feature enchancemenet module
                - the input image is first downsampled to 1/8. Then, it is fed
                  to a convolutional network as the enchanced feature.
                  - the netework here contains dilated=1, 2, 4, conv layers to
                    contain more contextual features.
                - acc is improved from 77.2 to 79.4 on voc
                - if dilation is 1, the acc is 78.7. Change one as dilation =
                  2, the acc is 79.0, with another as dilation=4, the acc is
                  79.4.
              - Feature guided refinement module
                  - from the enchanced feature, it predicts an objectness for
                    each anchor. Then, sum up all the objectness for all
                    different anchor shapes at the same spatial location, as
                    the attention. The final feature is the original enchanced
                    feature with original feature multiplied by the attention.
                  - Before doing the final prediction, it uses a deconv layer
                    to filter the features. The deconv offset comes from the
                    offset prediction (bounding box regression).
                  - acc is improved from 77.2 to 81.0
               - with the two modules, the acc is improved from 77.2 to 81.4
        - Experiment
            - on voc, the baseline is 77.2, and the approach improves by 4.1
              point.
              - But the improved solution has lots of more parameters and
                computations. The comparision might come from more parameters
                and computations. It is unclear if the accuracy is higher with
                similar computations
            - on coco, the baseline is 20ms with acc = 25.1. The improved one
              is 21ms with accc = 33.2, which looks pretty promising.
              - However, it is not clear if the training logic is the same,
                e.g. the number of iterations.
    - EfficientDet: Scalable and Efficient Object Detection
    - Learning Spatial Fusion for Single-Shot Object Detection
    - Beihang University
    - Strong baseline
        - improve yolov3 with existing approaches
            - bag of tricks (33.0 to 37.2)
                - mixup algorithm
                - cosine learning rate scheduling
                - sync bn
            - add one anchor-free branch together with anchor-based
              branched
            - add anchor guiding mechanism (37.2 to 38.2)
                - Region proposal by guided anchoring
                    - CVPR19
            - add IoU loss (37.2 to 37.6)
            - final 37.2 -> 38.8
    - contribution
        - adaptive fusion, i.e. fuse the three feature maps with
          adaptive weights. Each spatial position has a different
          weight. The weights on the same location but from different
          feature maps are summed as 1.
    - other details
        - data augmentation: 320 to 608
        - NMS: 0.6
        - 300 epochs
        - cosine learning rate from 0.001 to 1e-5
        - weight decay 5e-4
        - turn off mixup augmentation for the last 30 epochs
        - ignore the adjacent negative samples at the same location
          with the positives. epsilon: ignore region ratio
            - epsilon = 0.2 -> 38.8 -> 39.1
            - epsilon = 0.5 -> 38.8 -> 37.5
        - 38.8 -> 40.6 by the proposed adaptive fusion method
        - other fusion method
            - if we use sum as fusion method: 38.8 -> 39.3
            - if we use concat as fusion method: 38.8 -> 39.5. The number
              of parameters are not disclosed
        - improve retinaNet with the fusion method from 35.9 to 37.4
          with R50 and from 39.1 to 40.1 with R101. Note, the
          comparision with sum and concatenation is not disclosed
    - official code release: https://github.com/ruinmessi/ASFF

## object detection/data augmentation
- Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation
    - arxiv 12/2020
    - contribution
        - copy one object based on annotation from another image into the
          current image, and then blend it

# object detection/pre-training
- Aligning Pretraining for Detection via Object-Level Contrastive Learning
    - key
        - selective search is used to extract the proposals
        - mask-rcnn + fpn network is used
        - each proposal: extract 3 region features; one is based on the
          proposal locations (jitter is applied for data augmentation), one is
          a random crop within the reason, the other one is the down-scaled
          region.
        - online and target network is used as BYOL
        - feature between the different views of the proposals are aligned by
          inner product similarity, similar with BYOL
        - bounding box regression is not learned at all.
- DETReg: Unsupervised Pretraining with Region Priors for Object Detection
    - key
        - pre-training with raw image only for detection task
        - network is DETR
        - Selective search to extract the proposals. Then randomly select the
          proposals based on teh score from selective search. Teh score is
          interpreted as a probability. Each time, 30 boxes are selected
        - SwAV is used to extract the feature for each region
        - DETR outputs 3 signals for each box. Location, object or not object,
          feature embedding to align SwAV output.
        - results also only good at low-label regime.
# object detection/zero-shot
- zero-shot detection via vision and language knowledge distillation
    - arxiv 4/28/2021
    - key
        - zero-shot object detection problem
            - we have base categories and the full annotated training data
            - we need to detect novel categories
        - one naive solution discussed is that the class-agnostic detector
          generates the boxes and then crop the regions with 1x and 1.5x. Feed
          these cropped region into clip image classifier and then do the
          clip-style label assignment.
        - two-stage detector to generte class-agnostic boxes as proposals
        - VILD-text
            - take the text classifier as weight embedding function in segment
              everything.
            - for base class, generte the embedding and then do the learning
            - to inference novel classes, generte the embedding for novel
              categories.
        - VILD-image
            - add a regularizer to the region feature so that the region
              feature should be aligned with the image feature from the clip
              image encoder. then use this feautre to predict novel or base
              categories
        - VILD: combine VILD-text and VILD-image. Two heads are used to handle
          both loss. ensemble is used to finalize the result
        - use clip to help identify the novel class
        - clip model parameters are not updated
        - last classifier is set by the text embedding
    - exp
        - LVIS. common categories are as base categories. Rare categories are
          novel categories.
## Object detection/knowledge distillation
- Learning Efficient Object Detection Models with Knowledge Distillation
    - nips 2017
    - contribution:
        - add the class-aware weights on the combined classification loss. 
          The weight for all categories are the same, but different from the background
          - note, this weighting only applies to the soft loss. The
            original loss is not altered
          - compared with the non-weighted soft loss, the gain is from
            57.4 to 57.7 in VOC; from 50.8 to 51.3 in KITTI.
        - present a teacher bounded regression loss, that is, if the
          teacher's regression loss is larger, ignore the teacher's
          output and student's loss. If it is smaller, let the student
          move towards teacher's performance. This is regarded as teh
          soft regression loss, and the paper always added the normal
          regression loss.
          - compared with the non-bounded counterpart, the accuracy is
            improved from 54.6 to 55.9 in voc; from 48.5 to 50.1 in
            KITTI.
    - non-contribution, but adopt existing approaches
        - for the classification loss, it comebines the soft loss (from the
          teacher network) and the hard loss (from the ground-truth). Note,
          this is not the contribution.
        - use the feature map to align from the teacher to the student
            - one observation is that even the feature dimension is the
              same, adding a 1x1 conv layer is helpful.
                - with the adaptive layer, the accuarcy goes from 56.9
                  to 58 in voc; and 50.3 to 52.1 in KITTI.
        - with hint, the accuracy on training can be higher, which is
          kinds of counter-intuitive since it imposes more constraint
          on the features, which are not related with the ground-truth
          alignment. This might be the problem of optimization
          capability issue, where hint can help the optimization
    - experiments
        - small network is student; large network is the teacher
            - with different datasets, the accuary improvement can be
              3-4 points in terms of mAP@0.5.
            - For coco metric, the improvement is always around 1 point.
        - same network, but the network with small input image is
          student; the network with large input image is the teacher
            - the distilled student network can achieve comparable with
              the high resolution netework and is much better than the
              low resolution network.
- Mimicking Very Efficient Network for Object Detection
    - cvpr 2017
    - the knowledge is passed through the roi cropped feature for
      faster-rcnn network.
- Quantization Mimic: Towards Very Tiny CNN for Object Detection
    - ECCV 18
    - the paper is similar with mimiking features for object detection,
      where the roi feature is aligned for student's from teachers. The
      difference is that the feature is quantized before aligning, but
      there is no clue on how the network is learned, since the
      quantization operation will give 0 gradient.
- Distilling Object Detectors with Fine-grained Feature Imitation
    - CVPR 19
    - [github](https://github.com/twangnh/Distilling-Object-Detectors)
    - almost the same with Mimicking Very Efficient Network for
      Object Detection(cvpr17) except that, in Mimicking, it uses
      the proposal to crop the region; while 
      in this paper, it expands the region more, i.e. align the
      features within the neighbor of the target position.
      Specifically, for each ground-truth box, it first finds the
      position with highest objectness; then the objectness is
      multiplied by 0.5 as the threshold to filter all non-target region; finally the features
      within the target regions are aligned.
    - Experiment
        - it also compares the case of aligning the full feature, whose
          accuracy loses 8.9 point; while aligning the features within a
          sub region increases the accuracy by 5.2 point. The baseline
          accuracy is 62.63.
- GAN-Knowledge Distillation for one-stage Object Detection
    - arxiv only, 7/2019
    - the paper is not ready at all. some experiments are missing, some
      table are not fully filled.
    - the idea is interesting. It has a discriminator to predict
      whether the feature is from teacher or from student, which guides
      teh student network to mimic teacher's behavior.
- Mask Guided Knowledge Distillation for Single Shot Detector
    - icme 2019
    - previous methods focus on the region features to be aligned
      between the teacher's and the student's. This paper combines the
      global feature alignment and this local feature assignments.
    - Based on SSD
    - with global feature alignment, it is 54.6%. The baseline is not
      reported by not aligning the feature. with the global + local
      feature alignment, it is 56.88%.
- Learning Efficient Detector with Semi-supervised Adaptive Distillation
    - arxiv 1/2019. not find if it is in peer-reviewed conf/journal

# semi-supervised learning
- unsupervised data augmentation for consistentency training
    - 11/5/2020
    - key:
        - the data augmentation should be strong in semi-supervised learning
          for consistency matching.
- Meta Pseudo Labels
    - arxiv 3/2020
    - google
    - key
        - teacher model is also updated when learning student network on
          unlableed data
        - studnet model is only learned on unlabeled data together with the
          teacher network. Then, it is fine-tuned with the labeled data.
        - taking imagenet as labeled data, JET as unlabeled data, the accuracy
          on imagenet is 90.2%.
- A Simple Semi-Supervised Learning Framework for Object Detection
    - arxiv 12/2020
    - method
        - a teacher network is trained on the labled data
        - label propagation on the unlabled data
        - apply data augmentation on image as well as the pseudo box if needed
        - re-train the model
- Dual Student: Breaking the Limits of the Teacher in Semi-supervised Learning
    - iccv 2019
    - method
        - each batch contains labeled and unlabeled data. classification task
        - each sample is augmented twice.
        - each labeled data are involed in the traditional cross-entropy loss
        - for each data, the two augmented view's prediction should be close
          (not this paper's contribution).
        - if the data point is stable, the prediction between the two networks
          should be close also. Being stable here means that the predicted
          label of the two augmented view is identical & the highest confidence
          score is larger than a pre-defined threshold.
    - contribution
        - two identical networks with different random initialization
        - proposed a loss to expec


# Visual and Lauguage
- 12-in-1: Multi-Task Vision and Language Representation Learning
    - arxiv 4/2020
    - key
        - in pre-training, it is multi-task learning. in this paper, it is 4
          tasks, including VQA, NLVR. Different image-text pairs only
          contribute respective tasks. This can be pre-training task, and
          fine-tuned for each downstream task.
- End-to-End Learning of Visual Representations From Uncurated Instructional Videos 
    - cvpr 2020
    - video and narative, pre-training. based on contrastive learning. the
      positive pairs come from the same video;
- Large-Scale Adversarial Training for Vision-and-Language Representation Learning
    - arxiv 6/2020
    - apply the adversarial signal to the image/text to increase the robustness
      of the model

# Domain adaptation
- Unsupervised Domain Adaptation by Backpropagation
    - icml 2015
    - gradient reversal layer
# Supervised Learning with Extra Image + Image-level labels
- Revisiting Unreasonable Effectiveness of Data in Deep Learning Era
    - arxiv 2017
    - contribution is to use large-scale dataset as the pre-trained dataset and
      then fine-tune the model on the target domain.
        - target imagenet classification problem, random initialization gives 77.5
          top-1 accuracy; initialization from JFT-300M gives 79.2
        - target coco object detection, imagenet-pretrained initialization gives
          34.3 mAP; 300M-pretrained gives 36.7; imagenet-pretrined +
          300M-pretrained gives 37.4.
        - taerget on voc, imagenet-pretrained gives 76.3; 300M gives 81.4; imagenet
          + 300M gives 81.3

# network attack
- Universal Physical Camouflage Attacks on Object Detectors
    - 4/2020
    - for detection. optimize w.r.t. the ditortion pattern. The network
      parameter is fixed.

# Self-supervised Learning

## Self-supervised learning/Theory

- A CRITICAL ANALYSIS OF SELF-SUPERVISION, OR WHAT WE CAN LEARN FROM A SINGLE IMAGE
    - iclr 2020
    - contribution is a conclusion: fewer images are enough to learn teh
      lower-level feature parameters. The experiment is based on N images,
      which can be 1 or 1.2 million. Then, apply data augmentation to extend it
      to a fixed number of training images. Finally, apply pre-training and
      then fine-tuning on different layers to see how the performance is. When
      N is small, the accuracy based on the lower-level features are similar
      with larger N, but it is much worse on the higher-level features.
- UNDERSTANDING THE LIMITATIONS OF VARIATIONAL MUTUAL INFORMATION ESTIMATORS
    - iclr 2020
    - some analysis
- ON MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING
    - iclr 2020
    - some insignt and history of the maxInfo
    - insight: the accuracy does not depend on MI (mutual information) only,
      but also empiracal estimators
- On variational bounds of mutual information
    - ICML 19
- Noise-contrastive estimation: A new estimation principle for unnormalized statistical models
    - AISTAT 2010
    - the problem is to estimate the probability based on some samples, which
      follows some unknown probability.
    - the idea is to have some noisy data, which is based on another
      distribution. The objective is to distinguish the data from the noise
      data.
- Formal limitations on the measurement of mutual information
    - some theory on how to measure the mutual information.

#### self-supervised pretext task/data
- Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals
    - arxiv 2/11/2021
    - key
        - also use the saliency map as supervised signal.
        - the goal is to make the pixel representations from the same mask
          proposals be similar, and dissimilar otherwise. 
- CASTing Your Model: Learning to Localize Improves Self-Supervised Representations
    - arxiv 12/8/2020
    - two contributions
        - for data augmentation, crop the positive pairs so that the crop area
          should be overlapped with the salient regions by at least 30%.
        - use the saliency map as the supervised signal. loss is measured by
          the cosine similarity. The predicted saliency is measured by the
          gradient of the inner product between the query image and a masked key image.
    - the model is pre-trained on coco. The data scale is small and might not
      be enough.
- Distilling Localization for Self-Supervised Representation Learning
    - arxiv 4/2020
    - idea
        - the idea is to add a data augmentation, which extracts the salient
          region by unsupervised method and paste it to other images (random
          grayscale image is good enough, compared with texture images or
          images which has no saliency response in imagenet). This augmentation
          is performed with a probability of 50%.
    - experiment
        - 60.6 on imagenet linear probe, which is similar with moco v1.
- Automatic Shortcut Removal for Self-Supervised Representation Learning
    - arxiv 2/2020
    - contribution
        - add a u-net before feeding the image to the network.
        - the u-net reconstructs the original images, which might remove some
          shutcut information.
        - the u-net is called lens here.
        - then the pretext is performed on the reconstructed image
    - experiments
        - rotation as teh pretext task, 46.6 -> 48.6
        - Exemplar, 43.7 -> 46.1
        - Jigsaw, 37.2 -> 40.9

## self-supervised learning/separable task

### self-supervised learning/separable task/single view
- Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics
    - cvpr 2020. 4/5/2020 arxiv
    - use 5 transformation to process the image; the task is to predict which
      transform it is. The 5 transformations are inpainting, which distorts a
      sub region of an image; rotations; random wraping
- Adversarial Feature Learning, Adversarially learned inference
    - both are published in ICLR 2017
    - idea is exactly the same with Large Scale Adversarial Representation Learning
    - idea
        - the idea is quite similar with BiGan. Two networks. One is to map the
          image to the latent variables; the other is to map the latent
          variable to the image. The latent variable is pre-defined and follows
          a pre-defined distribution. The discrimator is used to distinguish
          the pairs.
    - no experiment on the imagenet linear probe study.
- Large Scale Adversarial Representation Learning
    - NIPs 2019, deepmind
    - contribution
        - combine the idea if BiGan and BigGan, used for representation
          learning.
    - imagenet dataset
        - 55.4
- Colorful Image Colorization
    - ECCV 2016
    - the input is a gray image and the output is a colored image. The loss is
      based on cross entropy, where the color space is quantized into 300+
      bins. Class balancing is applied by re-weighting.
- Unsupervised representation learning by predicting image rotations
    - ICLR 2018
    - rotate the image and then predict it as the pretext task
    - Experiments
        - Imagenet + linear prob, AlexNet
            - Random: 14.1
            - proposed: 36.5
            - Upper bound: 50.5

# self-supervised learning/separable task/two view
- Bootstrap Your Own Latent A New Approach to Self-Supervised Learning
    - 6/13/2020 arxiv. deepmind
    - solution
        - similar with moco but without negative queue
        - both crops of the same images are passed to each network. That is,
          though the iteration is 100 epochs, the training cost should be 2000
          epochs as moco.
        - the online encoder (query encoder in moco) uses 2 sets of mlp. Each
          MLP's structure is identical. The hidden size is 4096. The input
          variable to MLP is 2048 (the output of average pooling). In SimCLR or
          moco, the hidden size is teh same as the input size. Meanwhile, these
          two baselines have only 1 set of MLP.
        - moving average weight decay parameter is first 0.996 and gradually
          increases to 1. That means, the parameters of the target network
          tracks fast to the online network and then slowly. This is contrary
          to our finding previously, that is, when the online network changes
          fast, we should track slowly.
        - 512 TPU v3 cores, Batch size 512, 1000 epochs, takes 8 hours.
    - exp
        - linear protocal acc is 74.3
        - color solarization is used in data augmentation.
        - the linear evaluation protocal is a little different from what we
          thought. It added a non-linear layer on top of the logits before
          applying cross entropy loss. Meanwhile, teh regularization is applied
          on the output of the non-linear layer. This change improves from 74.3
          to 74.8.

- Splitbrain autoencoders: Unsupervised learning by cross-channel prediction
    - cvpr 17
    - contribution
        - split the input through channels into two sub inputs. Each goes
          through the network and predict the other split. Specifically, the
          input is split into L and ab components. The original network is
          split into two sub networks by spliting the channels evenly. Then, L
          is passed to one sub network and predict ab; ab is passed to the
          other subnetwork and predict L. The loss is based on classificatoin
          loss by quantizaing the target into multiple discrete slots.
          Empiracally, regression is slightly worse.
    - on the standard imagenet problem, the accuracy is 32.8 with conv5
      feature; 35.2 with conv4 feature; 35.4 with conv3 feature. 
- Unsupervised visual representation learning by context prediction
    - ICCV 2015
    - the idea is to predict the location relationship between a central crop
      and the 8 neighbor crop. For each image, randomly select one small crop first,
      and then randomly select one of the 8 crops around it. Each crop is sent
      to the network and have a feature. The 2 features are concatenated and
      then processed by linear layer. The goal is to predict if it is relation,
      i.e. 8-class classification.
    - Trivial shortcuts
        - Continuity
            - Solution
                - Random gap between patches
                - [48 – 7, 48 + 7] (patch size = 96x96)
        - Chromatic aberration
            - Solution
                - shift green/magenta towards gray
                - randomly drop color channels
                - mean subtraction
                - Downsample and upsample
    - no experiments on imagenet
- AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data
    - CVPR 2019
    - idea
        - the network takes two inputs. one is the original image; while the
          other is the transformed image. The goal is to predict the
          transformation parameters. In the experiments, it uses the projection
          parameters and l2 distance. The transformation is randomly scaling by
          0.8 to 1.2, randomly rotated by 0/90/180/270, and randomly
          translating +-0.125 of its width and height.
    - experiments
        - alexnet, the upper bound is 50.5 and this paper's approach is 37.7.

# self-supervised learning/separable task/multi view
- Representation Learning by Learning to Count
    - ICCV 2017
    - Idea
        - main idea
            - Let's say the original image size is S.
            - Downsample it to S/2 and extract a feature.
            - Split original image into 2x2 tiles and extract all these 4 features
              on these tiles.
            - the loss is the squared l2 norm of the difference between the global
              feature and the sum of the fetures for these 4 tiles.
        - one trivial solution is to map everything into 0 features. To
          mitigate this issue, a regularization term is added, i.e. the
          difference between two image pairs should be large. The difference is
          measured based on the global feature of this other image and the sum
          of the current 4 features from tiles.
        - the idea is based on 5 views, but it also depends on another image.
          This is also more like a combination of multi-view with non-separable
          loss.
    - the motivation is that the feature should represent the number of visual
      primitive in some sense. Thus, the sum of the individual features should
      be the feature from all image content.
    - the accuracy is only 25.7 on conv5 of alexnet on imagenet (linear probe).
- Selfie: Self-supervised Pretraining for Image Embedding
    - arxiv 2019/7
    - apply the BERT to the image.
        - split the image into grids.
        - Each sub-region is fed to the network. The first 3 conv block of
          resnet50. Assume N regions or N output features
        - then use the transform layer to fuse those features. Before fusing
          it, add a dummy feature and the first output is the fused feature.
        - randomly mask out some of the grids in the N regions.
        - the masked regions are fed to the network.
        - the global representation with some regions masked out is added with
          (learnable) positional embedding and then do comparitions with the
          masked regions. Use contrastive loss to learn. If we have M masked
          regions, we can apply M times. Each softmax also has M entries.
    - there is no experiment on the imagenet linear probe results since the
      pretrained is based on teh first 3 conv blocks. But mostly based on the
      fine-tuning.
- Unsupervised learning of visual representations by solving jigsaw puzzles
    - ECCV 2016
    - key idea
        - randomly select 225x225 regions from the input image
        - divide it into 3x3 grids. Each is 75x75
        - randomly choose 64x64 within each 75x75 region.
        - each region is fed into the network and get a feature vector
        - all 9 feature vector is concatenated into one
        - use linear layer to classify it.
    - the algorithm pre-defines a permutation set, rather than to use all 9!
      permutations, which would be too hard for the network to learn. An
      empirical result is taht the more the better, the larger distance each
      permutation is from each other the better. We can use Hamming distance to
      measure the distance.

### self-supervised learning/non-separable task/#classes less than #samples
- Self-supervised Pretraining of Visual Features in the Wild
    - arxiv 3/5/2021
    - key
        - use 1 billion image and apply SwAV to pre-train the model
        - use gradient checkpointint to save memory
    - on detection, 2 points' improvement
- Online Deep Clustering for Unsupervised Representation Learning
    - cvpr 2020
    - contribution
        - make the solution of alterating cluster & parameter update online.
        - allocated a memory to store the features for all samples and the
          corresponding labels, which is the cluster asssignment. Teh cluster
          centroid is also kept in the memory.
            - thus, it can not be scaled to billion-level images
        - in each iteration, it has 4 steps
            - network forward to get the feature representation
            - based on the cluster assignment, calculate the loss and update
              the network parameters.
            - the feature in the memory is updated by teh current features with
              momentum update
                - so, it cannot scale up to billion-level datasets
            - the cluster assignment label is updated by finding the closest
              center
- Unsupervised Learning of Visual Features by Contrasting Cluster Assignments
    - arxiv 6/2020
    - method
        - each image has two or multiple crops. Each crop corresponds one
          feature.
        - for each iteration, get the cluster code based on the current batch
          and the stored features.
        - each feature can have a psuedo gt based on the feature assignment.
        - each feature can predict the assignment of the other crops.
        - the loss is the prediction error.
    - exp
        - imagenet linear prob: 75.3. 800 epochs; 64 V-100 (16GB), 50 hours
        - fine-tune on imagenet with pre-trained on instagram-1B
            - SimCLR: 60.4 with frozen features; 77.2 with fine-tuning all
            - proposed: 66.5 with frozen features; 77.8 with fine-tuning all
            - with larger model capacity, the gain of pre-trainign on large
              dataset is even larger.
- ClusterFit: Improving Generalization of Visual Representations
    - cvpr2020, facebook, good paper
    - contribution
        - after a normal pre-training task, do a clustering on the learned feature
          and use the assignment as the pseudo label. Finally, re-train the
          whole network with these psuedo labels
        - teh pre-training task can be unsupervised pretext task, or the normal
          image classificatoin task.
    - experiments
        - imagenet1k -> target dataset
            - setting
                - basline
                    - pre-train r50 on iamgenet1k with gt classification labels
                    - fine-tune the last layer on different datasets.
                - proposed
                    - pre-train r50 on imagenet1k with gt labels
                    - do clustering with different K on the features from the
                      pre-trained model
                    - use the assignment as psuedo labels and re-train the whole
                      network
                    - fine-tune the last layer on different datasets
            - result:
                - if there is no noise in gt and target on imagenet1k, baseline
                  achieves higher accuracy on proposed
                - if there is no noise in gt and target on imagenet-9k and
                  inaturalst datasets, proposed is better. On imagenet-9k, the gain
                  is from 32 to 34.
                - if there is noise (randomly manually injected) to gt, the
                  proposed is better across all target datasets at noise level of
                  75%. With fewer noise, the accuracy on imagenet-9k and inat is
                  better with the proposed
        - imagenet-1B -> target
            - target on imagenet1k
                - baseline: 78, proposed 76.5 --> worse
            - target on imagenet-9k
                - baseline: 32.9, proposed 37.5 -> better
            - target on Place365
                - baseline: 51.2; proposed 52.6
            - target on inat
                - baseline: 43.9; proposed: 49.7
        - self-supervised
            - 45.1 -> 55.2 with jigsaw pre-text task
            - 50.0 -> 56.1 with RotNet
- Unsupervised Deep Learning by Neighbourhood Discovery
    - icml 2019
    - idea, the points within the same neighorhood should be similar. The
      neigborhood is computed based on the similarity between the features.
      Every few iterations, the neighborhood relationship is updated by k-nn.
      Within each batch and the relationship from the neighborhood, we can find
      out the pair (i, j), which should be similar. The similarity between i
      and j is the contrastive softmax between feture i and feature j over the
      sum of feature i and other features. The neighborhood is small initially
      and gradually increased with the motivation that initially the
      neighborhood is not reliable and we should only trust a few point's
      relationship. The trust is based on the entropy where the softmax
      similarity between current point and all others are taken as a
      probability distribution.
    - experiments
        - imagenet, alexnet, 37.9 (upper bound 50.5)
- Self-labelling via simultaneous clustering and representation learning.
    - iclr 2020
    - [code](https://github.com/yukimasano/self-label)
    - vgg, oxford
    - key idea
        - based on Deep Clustering for Unsupervised Learning of Visual Features
        - the contribution is to explicitly balance the label assignment
        - another trick is to add multiple heads for different clustering
          results.
    - experiment
        - 39.6 on imagenet linear probe with AlexNet. (Upper bound is 50.5)
        - addeding RotNet loss and more data augmentation, the accuracy can be
          44.7.
- Deep Clustering for Unsupervised Learning of Visual Features
    - ECCV 2018
    - key idea
        - alternate clustering, which solves the labeling problem and the
          classification, which learns the network parameters based on the
          peudo labels.
        - clustering
            - avoid empty clusters is by 1) randomly selecting a non-empty
              cluster, 2) using the centroid with a small random perturbation
              as the new centroid. 
            - based on the central cropped region
            - features are PCA-reduced to 256, whitened, and l2-normalized
            - updating the cluster every epoch

### self-supervised learning/non-seperable task/#class equals #samples
- Efficient Self-supervised Vision Transformers for Representation Learning
    - arxiv 6/2021
    - key
        - use swin as the backbone
        - use DINO as the loss.
        - each augmentation generates T tokens. Then it has 2T tokens. each
          token in one view finds the matched token in another view. The
          matching here is measured by teh cosine similarity. In the end,
          minimize the cross entropy loss again, same as DINO
- Self-Supervised Learning with Swin Transformers
    - arxiv 5/2021
    - key
        - backbone as Swin Transformer
- Simple Distillation Baselines for Improving Small Self-supervised Models
    - arxiv 6/2021
    - key
        - two BYOL models. one is teacher and the otehr is student.
- An Empirical Study of Training Self-Supervised Vision Transformers
    - arxiv 4/2021
    - key
        - moco v3
            - momentum encoder. negative is from the batch size, not from
              queue
            - more mlp is on encoder
        - random init patch projection layer is better than learning the
          projection
- Propagate Yourself: Exploring Pixel-Level Consistency for Unsupervised Visual Representation Learning
    - nips 2020
    - contribution
        - impose loss on the feature map with spatial information. so that the
          fpn can be learned for object detection.
        - each spatical feature is passed by a self-attention module, and then
          teh positive feature pairs between two image crops should be similar.
          The positive pair is constructed by the raw feature and the smoothed
          feature from another view.
- Whitening for Self-Supervised Representation Learning
    - arxiv 6/13/2020
    - accuracy is 66 only, which is far worse than the state-of-the-art.
- Debiased Contrastive Learning
    - handles the problem of false negative sampling problem. The idea is more
      like a soft-version of excluding similar points with the current samples.
      The accuracy can be improved with more positive crops to have a better
      estimation of the current samples.
- Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere
    - add a parameters to dynamically tune the balance between alignment and
      uniformity, which is good.
- Wasserstein Dependency Measure for Representation Learning
    - nips 2019.
    - the theory is very complicated. but the idea is 1) the loss is the same
      as CPC, 2) impose some gradient constraints based on the paper of Improved Training of Wasserstein GANs
    - no experiments on the imagenet dataset, but mainly on the hand-made
      datasets.
- Rethinking Data Augmentation: Self-Supervision and Self-Distillation
    - iclr 2020 submission and withdrawn 
        - the main review comments are the novelty and the experiments.
    - there are lots of experiment results, but no results on imagenet, which
      might be one of the problems.
    - the key idea
        - let's say there are K transforms, and C classes. The task is to
          predict K * C outputs. After that, for inference it learns a single
          mapping by learning the teacher, which is aggregated by all teh
          transform.
- Unsupervised Feature Learning via Non-parametric Instance Discrimination
    - (official code)[https://github.com/zhirongw/lemniscate.pytorch]
    - cvpr18
    - idea: each image is a class and adopt contrastive loss between current image and all others, whose complexity is reduced by
      the trick of noise contrastive estimation from O(n) to O(1).
    - imagenet accuracy is 35.6 on alexinet and 54.0 on resnet50 on the linear
      classification evaluation task.
- Contrastive Multiview Coding
    - [code](https://github.com/HobbitLong/CMC)
    - contribution
        - extend the contrastive loss from the 2 views to N views. The loss is
          not improved, but applied multi times to incorporate all views.
    - on imagenet, two views come from L and ab color component of the images.
- Improved Baselines with Momentum Contrastive Learning
    - arxiv 3/9/2020
    - incorporate the tricks shown in A Simple Framework for Contrastive Learning of Visual Representations
      to the momentum constrastive learning, i.e. add extra MLP layer for
      pre-training, more data augmentation, more iterations.
        - the accuracy is improved from 60.6 to 71.1.
- Data-efficient image recognition with contrastive predictive coding
    - iclr 2020 submisssion but rejected. appear in arxiv 12/2019
        - the review comment of why it is rejected is lack of novelty.
        - the experiment results are interesting to learn.
        - the paper is not that well written unless you are familiar with the
          paper it is based on.
    - novelty
        - based on the paper of Representation Learning with Contrastive Predictive Coding,
          this paper introduces more tricks to boost the accuracy and apply it
          on few-shot learning or low-labeled tasks.
          - tricks
            - more powerful network, named resnet161. original implementation
              uses resnet101. --> 5 point gain
            - originally, each patch's size is 64x64. in this paper, it is
              increased. --> 2 point gain. the paper does not tell the
              increased resolution
            - originally, BN is not used, and the reason might be that the
              pretraining works on the patch, but the testing works on the full
              image. The BN captures the input statistics and is thus not
              appropriate in such settings where train and inference are
              different. in this work, it uses layer normalizatoin. -> 2 point
              gain. This paper does not tell if other normalization (not related with 
              batch size) has similar gains.
            - originally, the image patches in the upper side predict the
              regions in the below. This paper extends it to other directions:
              lower predicts upper; left predicts right; right predicts left.
              By using two directions, -> 2 point gain. By using 4 directions,
              2.5 point gains.
            - apply more augmentation on each patch, e.g. color dropping -> 3
              point gain. including other augmentations -> 4.5 point gains.
    - experiments (this may also be the novelty)
        - on the task of imagenet pretraining + imagenet classificatoin with
          one linear layer, it improves from 48.7 to 71.5
        - on the few shot classification task (using 1% of the imagenet data),
          the tricks which works on using 100% imagenet data is not necessary
          to work here.
          - by using 1%, the fully supervised gives 44.1, which is improved to
            77.1 by the pre-training+finetuning. The reason may be that the
            pre-training could see more data.
        - the results on voc detection are 76.6% given resnet161, while the
          supervised counterpart is 74.7.
- Learning Representations by Maximizing Mutual Information Across Views
    - NIPs 2019, MSR Montreal. 3 authors, the first two are also the authors of
      the paper which this paper is based on
    - (code)[https://github.com/Philip-Bachman/amdim-public]
    - the contribution is to extend the paper of Learning deep representations 
      by mutual information estimation and maximization by introducing
        - multi-view of the images to construct the loss
            - multi-view comes from multiple instantiation of the data augmentation
            - the baseline is to use one instantiation
        - multiple feature maps from different spatical sizes are used rather
          than 1.
        - extend the representation of a notion of mixed representation.
    - experiments
        - on cifar10, the baseline is 75.21; while this paper achieves 89.5.
        - on cifar100, the baseline is 49.74, while this paper achieves 68.1
- Learning deep representations by mutual information estimation and maximization
    - iclr 2019, MSR montreal, Yoshua
    - (official code)[https://github.com/rdevon/DIM]
    - the contribution is to incorporate local features in the representation
      learning. Before, each image correponds to one vector, e.g. R^1024. Now,
      it correponds to multiple vectors, e.g. R^{7x7x104} from the last feature
      map before global average pooling.
      - the objective is to maximize the esitmated mutural information between
        the input and the output, as claimed. But actually, it applies the
        maximization between the local features (the features in 7x7 feature map)
        and the global features (after average pooling).
      - there are 3 losses in total
        - the first one should be similar with simCLR, which uses the global
          feature for each image and compares it against all the rest in the
          current batch size.
        - the second is to use the local feature. That is, use the global
          feature to compare it with the local features of current image and
          teh other images.
        - the third one is an adversarial loss which tries to make the output
          follow certain distribution, e.g. uniform distribution.
    - no experiments on imagenet, but on cifar and shrinked version of
      imagenet. The accuracy is comparable with contrastive predictive coding.
- Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks
    - PAMI 2016
    - The idea is
        - randomly sample N patches from the dataset
            - the patch is sampled with a probability propertionally to the
              gradient. But no information about where the gradient is from and
              how the loss is when the gradient is computed
        - for each image, apply M random transformations
        - assign a unique label on each patch
        - train it like a supervised training tasks.
    - Thus, it is highly dependent on how many images can be sampled and how many transformations
      are used. In the experiments, N is 8k~16k. M~150.
    - No experiments on imagenet
- What Makes for Good Views for Contrastive Learning?
    - arxiv 5/2020, from Google.
    - the solution is built on top of moco v1, but with similar tricks mentioned in
      moco v2 or sim-clr. Comparably, it uses even stronger data augmenation,
      e.g. color droping. Another difference is it uses a jigsaw branch similar
      as pretext-invariant representation paper. That is, it also has the loss
      for jigsaw problem.
    - the accuracy with 200 epoch's pre-training achieves 70; the accuracy with
      800 epoch's pretraining achieves 73.
- Self-Supervised Learning of Pretext-Invariant Representations
    - cvpr2020, facebook
    - contribution
        - based on the instance discrimination
        - the difference is that this paper add a new head after conv5, and
          processes a transformed input. The output feature is also
          discriminated with similar loss function as instance discrimination.
        - the intuitivation is the transformed representation should also be
          similar with non-transformed representation.
        - the transform here is 1) split the input to 3x3 regions, 2) process
          each region and get a representation, 3) concate all these 9
          representations with a random order, 4) map the concatenated feature
          to a fixed feature, which will be compared against the features in
          the memory bank. Note, since the loss is similar with instance
          discrimination, the memory bank's size is the same as the dataset.
    - experiments
        - on imagenet linear prob task with r50, it is 63.6, while moco is 60.6
- Representation Learning with Contrastive Predictive Coding
    - deepmind, arxiv 1/2019
    - the paper of DATA-EFFICIENT IMAGE RECOGNITION WITH CONTRASTIVE PREDICTIVE
      CODING gives a better explanation about how the algorithm performs.
    - contribution
        - a framework of contrastive predictive coding for feature learning
            - assume the input signal is x_t
            - encode x_t as z_t=g_enc(x_t)
            - calculate context representatoin as c_t = g_ar(z_<=t). That is,
              calculate a representation based on the signal from the time
              earlier or equal to t.
            - use c_t to predict x_{t+k} by contrastive loss. That is, maximize
              f(x_{t+k}, c_t) = z_{t+k}^T W_k c_t compared with the sum of
              f(x_{j}, c_t), for all j.
        - apply the framework to speech, images, text, rl
            - in imagenet, the accuracy is 48.7.
                - each image is split into 7x7 overlapped regions
                - each region is encoded by g_enc to get z_{u, v}
                    - The encoder of g_enc is based on resnet v2 101.
                - the autoregressive model accepts the regions from top to 
                  predict the regions in the below.
                    - that is, c_{u, v} based on z_{x, v}, (x <= u)
                    - the g_ar is based on PixelCNN-style autoregressive model.
                - in sec 2.1 of DATA-EFFICIENT IMAGE RECOGNITION WITH CONTRASTIVE
                  PREDICTIVE CODING gives a clearer explanation of the details on vision
- A Simple Framework for Contrastive Learning of Visual Representations
    - arxiv 2/2020
    - Hinton
    - contribution
        - add non-linear layer after the representation before applying the
          contrastive loss
        - more iterations
        - larger batch size
    - others
        - image classification problem
            - pre-training + fine-tining last layer
        - imagenet2012
- Momentum Contrast for Unsupervised Visual Representation Learning
    - The task is to learn a backbone or a feature extractor from the unlabeled
      image data, without any annotations.
    - The basic idea (not contributed in this paper) is to learn a dictionary,
      each value of which is a key. For example, we can randomly select 8092 images
      as a dictionary. Each element is an image, and can be mapped to a feature,
      by an encoder (any learnable CNN). For each training image, a feature is
      extracted by the target extraction network (can be the same or different network from encoder).
      The training image's feature is compared with all the features in the dictionary by inner product.
      The loss is the cross entropy loss. The positive pairs normally come from
      teh same image but from differnet views, e.g. augmentation. The negative
      is randomly selected from the dictionary or with all the features in the
      dictionary.
    - The contribution is to solve the problem when the dictionary size is large,
      where in-memory operation might not be feasible. The key idea is 1) keep a buffer to store a dynamic dictionary, whose size is small, 2) run the loss and update the parameter by comparing the training image's feature and the features in the small dictionary; 3) enqueue current images to the dictionary; 4) dequeue the oldest images from the dictionary so that the dictionary size is the same for the next iteration. 
    - The results are verified by a linear classier with the features extracted by the extraction network, which is a common practice in the literature. The results show that this initialization performs as good as or is better than the imagenet-pretrained detector/instance segmentatin/key point detection. 
    - Some things we can learn from is that
	    - Lots of experiments when transferring different tasks! Parameter search! The learning rate of the classifer is 30; the weight decay is 0. These parameters are not consistent with what we used, and they must have done lots of experiments and most of them fails. The temporature parameter in the loss function is 0.07 (this parameter is from a related work, not from this paper), which is also not a usual one. 
	    - The BN in this literature is not working as the paper discusses based on some related work. But this paper alters the orders of the dictionary across different GPUs and make BN work. 
	    - The parameter update in dictionary encoder network is not based on SGD, but only based on the momentum update. That is, the updated parameter is 0.999 * the parameter in last iteration + (1 - 0.999) * the parameter in the extraction network. 
	    - a little bit counter-intuitive since it discards SGD. Not sure if a lower learningjkkjk rate also works. Anyway, this works. The parameter is 0.999, which is quite close to 1 and they must have done lots of parameter tunning


# self-supervised learning/video
- Self-Supervised Learning of Video-Induced Visual Invariances
    - cvpr 2020
    - idea
        - encode the frame, then pool to get shot-level feature. pool it again to
          get video-level feature
        - for each frame, apply the rotation self-supervised loss
        - for the shot embedding, it randomly permute the order and predict
          whether it is permuted or not.
        - another loss is based on predictive contrastive coding. That is, use
          the first k shots' embeding to predict the m-step's future embeding,
          which will be used as the positive pairs.

#### self-supervised learning/observations with more experiments
- Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases
    - arxiv 7/29/2020
    - useless
- How Useful is Self-Supervised Pretraining for Visual Tasks?
    - compare different factors for the exising approaches. No new algorithm is
      proposed, but only some empirical observations, including
      - fine-tuning last linear is not consistent with fine-tuning the whole
        network.
      - more labeled data in the downstream task will reduce the benefit of the
        pre-training.
- Scaling and Benchmarking Self-Supervised Visual Representation Learning
    - facebook. ICCV 2019
    - conducted lots of experiments and observed the accuracy is better with
      larger datasize, larger model, and difficult tasks
- Revisiting Self-Supervised Visual Representation Learning
    - cvpr 2019
    - conducted experiments on 4 existing approaches and with different
      variants of resnet50. The conclusion is that the best performed model
      architectures are different with different pretext tasks.

#### self-supervised learning/combine multiple tasks
- What Should Not Be Contrastive in Contrastive Learning
    - arxiv 8/2020
    - the idea is to add multiple heads in moco framework. The first head is
      teh same as usual. Let's say we have n type of data augmentation. Then,
      it will add extra n heads. Each head focuses on one type of data
      augmentation. For the i-th extra head, the possitive pairs should have
      the same parameters for the i-th data augmentation type. All other views
      even from the same image would be used as negative samples. This is
      similar like to ask each head to predict the parameter of one type of
      data augmentation.
    - For the experiment, there is no experiment on imagenet-1k linear probe.
      On one thing, it creates a task to predict teh ratation degree. On the
      otehr hands, it finds this algorithm works on some data set, e.g. iNat.
      It is not working on imagenet.
- Prototypical Contrastive Learning of Unsupervised Representations
    - arxiv 5/2020
    - key idea
        - based on moco
        - do clustering on the features from the momentum encoder
        - train the model with two losses
            - one is the same as moco
            - one is that the feature should be as close as to the nearest
              centroid, and as far as possible to other centroids
                - this is similar with the clustering-based but with a fixed
                  linear layer where no bias and the weight is the centroid.
                  The label is the index of the nearest centroid.
    - the accuracy on imagenet linear prob experiment is 65.9 with 200 epochs.
        - i believe the accuracy is similar with sim-clr, better than mocov1,
          but worse than moco v2.
- Self-Supervised Representation Learning by Rotation Feature Decoupling
    - CVPR 2019
    - idea: combine the loss of ratation loss and instance discrimination.
        - the network outputs two feature vectors with same dimentions
        - the first vector is for rotation loss.
            - a weight is added to each rotated instance. The weight comes from
              a pre-trained network which distinguishes rotated or non-rotated,
              i.e. a binary classifier. In this loss, if it is non-rotated, the
              weight is 0, otherwise teh weight is 1 - confidence^2. So, if the
              previous network cannot tell if the image is rotated, the weight
              here should also be small. The motivation is that some images are
              rotation sensitive, but some are rotation agnostic. For example,
              a plain in the sky might be rotation agnostic.
        - the second vector is designed to have nothing with the ratation, and
          there are two losses here.
          - the first is to expect those 4 features from the same images should
            be similar. The loss is the squared l2 loss of the feature and the
            average feature.
          - the second is the instance discrimination loss which expects the
            feature from different images are different.
          in the batch
    - In the experiments on imagenet linear probe test, the accuracy is 44.3
      with AlexNet, compared with 36.5 for RotNet; 35.6 for Instance
      Discrimination.

# self-supervised learning/application
### self-supervised learing/application/metric learning
- Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss
    - arxiv 8/2020
    - need to read it again. no idea of how it relates the sift-matching result
      with the loss

# self-supervised learning/application/domain generalization
- Domain Generalization by Solving Jigsaw Puzzles
    - iccv 2019
    - apply the jigswaw loss on the domain generalization
        - the network receives two kind of images. One is the source image from
          multiple source domains with gt labels. Each domain has its own
          domain classifier. The second is shuffled images. Shuffle means
          within 3x3 crops in each image. The loss is to predict the
          permutation. The permutation classifer is shared for all source
          domains.
# self-supervised learning/application/3d
- Self-supervised Feature Learning by Cross-modality and Cross-view Correspondences
    - arxiv 4/2020
    - apply to the 3d data points
# self-supervised learning/application/supervised classification
- Distilling Visual Priors from Self-Supervised Learning
    - eccv-workshop 2020
    - summary
        - the problem is classification problem where each class has 50 traiing
          images. The images come from imagenet.
        - the idea is to first pre-train a newtork with moco-v2. Then, the
          weight is used to initialize two networks. one is called teacher; the
          other is called student. The network strucutre should be the same.
          Thirdly, the teacher network is frozeon, and the student network is
          fine-tuned with 1) classification loss and 2) distillation loss based
          on the feature map at the same time.
        - other modification this paper does is to add a margin in teh
          constrastive learning for pre-training. However, this is kinds of
          similar with changing the weight decay. If tuning the weight decay,
          it might achieve similar resutls.
- Supervised Contrastive Learning
    - arxiv 4/2020
    - idea
        - in the supervised learning, adding the contrastive loss.
        - the positive pairs come from teh images with teh same labels
        - the negative pairs and the positive pairs are all in denominator of
          the loss
        - the learning here is alternative
            - learn based on the contrastive loss, pairs are sampled based on
              the annotations
            - learn the last linear layer for cross entropy.
    - on imagenet
        - 77.0 -> 78.8 for resnet50
        - 78.0 -> 80.8 for resnet200

# self-supervised learning/application/knowledge distillation
- Knowledge Distillation Meets Self-Supervision
    - arxiv 6/2020
    - solution
        - teacher network is first trained with feature extractor and
          classifier
        - add a projection head on the teacher network and use the contrastive
          loss to do self-supervised learning
        - the student network also has a classifier and the projection head to
          mimic the teacher's behavior.
        - 4 losses to train the student network
            - the cross entropy loss supervised by the ground-truth
            - the kl loss between the student and the teacher with temporature.
                - the above two is the same as what traditional approach is
            - the misalignment errors of a heavily-augmented image copy between
              the teacher and the student. This copy is used in self-supervised
              learning. As argued in the paper, the copy is not designed to
              have correct classification prediction since augmentation is
              strong here. But it asks the network outputs similar information
              as the teacher from the classification branch (not from the
              self-supervised learning branch)
            - the last one, which is only the one related with self-supervised
              learning is the misalignment error between teh similarity matrix
              from teacher and the matrix from the student. That is, each image
              has two copies, processed by the networks. Then, each network can
              compute the similarity between any two pairs.
                - one more thing the author does is to filter the entries in
                  the matrix. The rule is that only the samples which the
                  teacher network can successifully classify will be used as
                  the transfor information. The error can be measured by where
                  the self gt is. In experiments, it uses top-75% samples.

- Contrastive Representation Distillation
    - iclr 2020
    - the paper studies the problem of knwoledge distillation for the
      supervised learning. The intuition here is to apply the contrastive loss
      to make the features from the same images (from teacher and student) as
      similar as possible and make the features from different images (one 
      is from student network, the other should come from teacher network) different.
      The loss here drived is not like the one cross entropy loss, but more
      like a binary classification problem. That is, it is a sum of multiple
      losses. One loss is for the positive pairs (features from same image. One
      feature is from teacher, one is from student); the others are for the
      negative pairs. Each negative pair means that one feature is from current
      image and student network; the other feature is from memory bank and
      should be extracted from the teacher network.
    - experiment
        - See table 1. One examle is student's network without knwoledge
          distillation is 73.26; teacher is 75.61; the traditional knwoledge
          distill is 74.92; the paper's is 75.64.

# self-supervised learning/application/adversarial attack
- Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
    - NIPs 209
    - the idea is to train the classification loss with the self-supervised
      loss based on the rotation. The observation is that the model could be
      more robust. The classification loss is not based on the raw input image,
      but based on the image modified by the gradient, so that the network can
      learn better since the input makes the loss higher. During the testing,
      the input image is also modified to make teh loss higher to test the
      robustness. In this noisy input, the accuracy is dropped from 90+ to 40+.
      With the addition of the self-supervised loss, the gain could be around 5
      points.
# self-supervised learning/application/domain adaptation
- Self-Supervised Learning Across Domains
    - arxiv 7/24/2020
    - during training, add the loss of rotation loss or jigsaw loss
- Unsupervised Domain Adaptation through Self-Supervision
    - The problem is that source domain has labels, target domain has no labels
      but images only, the labels on these two domains are the same. The goal
      is to do classification on the target domain.
    - The idea here is to apply k different self-supervised losses on the
      source domain and the target domains, in addition to the cross entropy
      loss on the source domain. The self-supervised losses are 1) rotation,
      which predicts one of 0/90/180/270 rotation degree applied on the image,
      2) flip, which predicts if the image is vertically flipped, and 3) crop
      location, which predicts one of the four corners of the absolute location
      coordinates.
# self-supervised learning/application/semi-supervised learning
- Big Self-Supervised Models are Strong Semi-Supervised Learners
    - arxiv 6/17/2020
    - contribution
        - teacher network is pre-trained on unlabeled images
        - teacher netowrk is fine-tuned on labeled images
        - student network is trained and distilled with teacher network on
          unlabeled images or combined with labeled images.
        - sim-clr-v2 is presented. the difference is
            - larger backbone network
            - use 3 linear projection layer and use the first one as the input
              for linear classification
            - the last, which is the most important, is to add a momentum
              encoder and a memory queue, which has 1 point's improvement
- Boosting Few-Shot Visual Learning with Self-Supervision
    - ICCV 2019
    - during few-shot feature extractor training, add a loss from
      self-supervised training. This paper uses rotation-based and
      position-based losses.
    - experiments
        - with the rotation auxiliary loss, there is about 1 point's gain.
        - with patch location relationship loss, there is also about 1 point's
          gain.
        - applied to semi-supervised few-shot learning as well, and in certain
          cases, tehre are 3-4 points' gain.
- S4L: Self-supervised semi-supervised learning
    - iccv 2019
    - problem: target supervised classification dataset (1% of the imagenet
      dataset or 10%) with the unlabeled image datasets. The solution is to
      train the model on the joint of the two datasets with the loss from
      unsupervised learning. Each batch consists of the images from unlabeled
      or labeled sets evenly. For the image including unlabeled and labeled, it applies the rotation
      self-supervised loss. That is randomly rotate the input image
      with 0/90/180/270 degres and predict the degree
      with the cross entropy loss. The other is some exemplar loss, which is
      similar with the contrastive loss. The experiments show that with 1% of
      the labelled data,
        - the approach of training supervised model only on the labelled data
          gives 48.43% top-5 accuracy
        - the approach of psuedo labeling (train a model on labeled, propagate
          the labels on the unlabeled, re-train the model on the full) gives
          51.56.
        - self supervised + linear layer gives at most 25.98 accuracy
        - self supervised + fine-tune gives at most 45.11, which is even lower
          than the 48.43, which is from the supervised learning only. 
        - the joint training gives 53.37 with the rotation loss.

# large scale detection in terms of labels
- Scaling Object Detection by Transferring Classification Weights
    - iccv 2019, [code](https://github.com/xternalz/AE-WTN)
    - an enchanced weight transfer network to scale the classes
- Learning to Segment Every Thing 
    - CVPR 2018, [code](https://github.com/ronghanghu/seg_every_thing)
    - proposed a weight transfer network

# video
- Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification
    - 7/2018
    - key
        - seperable 3d conv to replace full 3d conv
        - apply 3d conv on top-level feature maps rather than on lower-level
          feature map
        - S3D
- Cross Pixel Optical Flow Similarity for Self-Supervised Learning
    - ACCV 2018
    - align the image representation to the optical flow
    - the accuracy on imagenet linear probe is 28% only.

# General
- Curriculum learning
    - 2019 ICML, Yoshua as the first author
    - the basic idea is to let the network to learn different tasks. Each task
      is like a curriculum. The scheduling is that the tasks will be gradually
      harder so that the network can learn gradually. Similar with what we
      larned from elementary school, where the courses are becoming harder and
      harder
      - theoretically, you can apply a weight on the example, which means the
        sampling rate, or how likely the sample should be used. In this case,
        the weight will change the input distribution, and thus the entropy if
        we take the input data as a probability distribution. The curriculumn
        should make the entropy higher gradually.
    - The experiments it did is based on some synthetic data, and the
      policy is that first to learn easy samples and then the rest, which
      demonstrates to be better than to learn all data equally.
# Face recognition
- FaceNet: A Unified Embedding for Face Recognition and Clustering
    - arxiv 12/2015, 4830 citations until 4/7/2020, cvpr2015
    - contribution
        - (as claimed in the paper) previously, the approach is to learn a classification network on the
          annotated identity and use intermediate representations as the face
          representation, which has high dimensionals and is hard to justify
          why the feature could work.
        - the idea here is to use triplets loss to explicitly learn the
          embedding, which is of 128 dimensions. The loss is hinge loss with a
          marge based on one anchor image, a positive image and a negative
          image.
    - training
        - 100M-200M face thumbnails, 8M identies
        - input size: 96~224

# Image Caption/Visual Question Answering
- Weakly Supervised Content Selection for Improved Image Captioning
    - arxiv 9/2020
    - first predict some tags and then caption
- Towards VQA Models That Can Read
    - dataset
- Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA
    - arxiv 11/2019
    - idea
        - the network inputs have 3 modalities
            - the question word's embedding
            - the visual object embedding
            - the text embedding
- TextCaps: a Dataset for Image Captioning with Reading Comprehension
    - introduced a dataset, which has lots of text (OCR).

# Detection by caption dataset
- Caption Dataset --> Detection
    - Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection
        - iccv 2019
        - Problem
            - use the caption dataset to infer the image-level labels, which
              are used as input to the weakly-supervised object detection
              algorithm for detection
        - algorithm
            - Novelty: a method of how to infer the image-level labels from the caption
              text
                - each word is embeded by GloVe algorithm, resulting 300D
                  feature
                - the embedding is project to a 400D space
                - max pooling over multiple words in the sentence
                - softmax for the target category domains (e.g. 80 for coco). A
                  linear layer should be applied before the softmax (not
                  mentioned in the paper).
            - use OICR as the weakly supervised learning algorithm after
              infering the image labels. The algorithm is slightly modified
                - in OICR or WSDDN, two softmax operations are applied across
                  proposals and classes, but here one of the operations is
                  replaced as sigmoid. The difference is not studied in the
                  experiment. The re-weighting in OICR is removed here, and the
                  accuracy is not changed substantionaly as claimed but without
                  experiment result support.
        - experiment
            - coco-caption dataset and flicker caption datasets
                - assume these two datasets are carefully annotated with
                  captions. The cost might be more than the image-level labels.
                  In experiments, no studies are shown with noisy caption
                  datasets, e.g. CC, or randomly crawled internet data.
            - for the method of extracting the image-level labels from the
              caption, the proposed method improves from 39.9 (exact term
              matching), 41.7 (learned GLoVe) to 43.1 on voc test by leveraging coco. If using
              Flicker, it is 31.0 (exact term matching) to 33.6 (the results of
              learned GLoVE is not disclosed).

# weakly supervised object detection
- Evaluating Weakly Supervised Object Localization Methods Right
    - cvpr 2020
    - present some new metric to focus on the localiazation metric, more like
      CorLoc. Meawnwhile, it shows classification activation map can work
      pretty well.
- Pairwise Similarity Knowledge Transfer for Weakly Supervised Object Localization
    - eccv 2020
    - introduce the pairwise similarity function to predict whether two
      proposals come from the same class

# Semi-supervised training
- semi-supervised classification with graph convolutional networks
    - arxiv, 2016, citation 4k+
- Billion-scale semi-supervised learning for image classification
    - arxiv only 5/2019
    - key idea
        - train a classifier on the supervised data
        - propagate the labels on the unlabeld data
        - pre-train the student network on the propagated dataset
        - fine-tune on the supervised data
    - Comopared with Data Distillation: Towards Omni-Supervised Learning
        - the ref paper studied the approach on detection and keypoint
          detection, while this paper studies the approach on image
          classification.
        - the ref paper re-train on the mixed two dataset, while this paper
          seperates them into two stages: pre-train on the propagated data
          and then fine-tune on the labeled data
- Data Distillation: Towards Omni-Supervised Learning
    - cvpr18
    - key idea
        - leverage the unlabeled data to help the supervised training
        - method
            - train a teacher network on the supervised data
            - propagate teh labels on the unlabeled data by multi transform
              inference
                - multi transform inference means to do inference on multi form
                  of the input, e.g. by multi scaling
                - the label is hard label and teh threshold is set so that the
                  average number of labels on unlabeled image is similar with that
                  in labeled dataset.
            - re-train the model on the combined data from labeled dataset and
              the auto-labeled dataset.
    - experiments are on keypoint detection and object detection.
        - The gain for keypoint detection is large
        - teh gain on detection is around 1 point.

# Data Augmentation
- CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features
    - iccv 2019
    - idea is that combine two inputs by randomly 1) sampling a sub region and
      2) fill this region with one image, 3) fill the regions outside the
      sub-region with the other image.
- Randaugment: Practical data augmentation with a reduced search space
    - arxiv 11/2019
    - key idea
        - the search space is composed of two parameters. one is the number of
          transformations, and the other is the strength.
        - grid search to determine the parameters
- adversarial autoaugment
    - iclr 2020
    - the idea is to have a policy network, which tries to maximize the loss of
      the target network. It is based on M augmented instances for each image,
      and make decisions to tune the policy network.
    - resnet50, imagenet, -> 79%.
- Fast AutoAugment
    - NIPs 2019
    - The key idea is to train a model without data augmentation first; and
      then evaluate the model on the pre-reserved data with different data
      augmentation. That is, the performance of different poicies is not
      evaluated by re-training, but by evaluation
    - on Imagenet, the accuary with R50 is 22.4. Baseline is 23.7; AutoAgument
      gives 22.4 also.
- Learning Data Augmentation Strategies for Object Detection
    - apply the auto augmentation idea from classification to detection.
    - the gain is 2.3 on R50+RetinaNet
    - rotation is perfererred in the augmentation searching
- Population based augmentation: Efficient learning of augmentation policy schedules
    - icml 2019

# Optimization
- ShakeDrop Regularization for Deep Residual Learning
    - ieee access 2019
    - a hybrid way to pertube the weight of the residual branch stochastically
      in the forward and backward during training.
- Practical Bayesian Optimization of Machine Learning Algorithms
    - not a good tutorial.

# Teacher-Student
## teacher-student/classification
- MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks
    - arxiv 2020
    - the teacher signal is the ensemble of multiple teachers. Then, it uses
      cross entropy for knowledge distillation. A discriminator is also used
      to judge whether the input is a teacher feature or a student feature.
    - the gt is not used in training; but there is no experiment in this paper
      to show +gt worsen the accuracy.
    - the discriminator is used here, but there is no experiment in this paper
      to show removing is worsen the accuracy.
- Knowledge Distillation Meets Self-Supervision
    - arxiv 6/2020
    - solution
        - teacher network is first trained with feature extractor and
          classifier
        - add a projection head on the teacher network and use the contrastive
          loss to do self-supervised learning
        - the student network also has a classifier and the projection head to
          mimic the teacher's behavior.
        - 4 losses to train the student network
            - the cross entropy loss supervised by the ground-truth
            - the kl loss between the student and the teacher with temporature.
                - the above two is the same as what traditional approach is
            - the misalignment errors of a heavily-augmented image copy between
              the teacher and the student. This copy is used in self-supervised
              learning. As argued in the paper, the copy is not designed to
              have correct classification prediction since augmentation is
              strong here. But it asks the network outputs similar information
              as the teacher from the classification branch (not from the
              self-supervised learning branch)
            - the last one, which is only the one related with self-supervised
              learning is the misalignment error between teh similarity matrix
              from teacher and the matrix from the student. That is, each image
              has two copies, processed by the networks. Then, each network can
              compute the similarity between any two pairs.
                - one more thing the author does is to filter the entries in
                  the matrix. The rule is that only the samples which the
                  teacher network can successifully classify will be used as
                  the transfor information. The error can be measured by where
                  the self gt is. In experiments, it uses top-75% samples.

- Contrastive Representation Distillation
    - iclr 2020
    - the paper studies the problem of knwoledge distillation for the
      supervised learning. The intuition here is to apply the contrastive loss
      to make the features from the same images (from teacher and student) as
      similar as possible and make the features from different images (one 
      is from student network, the other should come from teacher network) different.
      The loss here drived is not like the one cross entropy loss, but more
      like a binary classification problem. That is, it is a sum of multiple
      losses. One loss is for the positive pairs (features from same image. One
      feature is from teacher, one is from student); the others are for the
      negative pairs. Each negative pair means that one feature is from current
      image and student network; the other feature is from memory bank and
      should be extracted from the teacher network.
    - experiment
        - See table 1. One examle is student's network without knwoledge
          distillation is 73.26; teacher is 75.61; the traditional knwoledge
          distill is 74.92; the paper's is 75.64.

- The State Of Knowledge Distillation For Classification Tasks
    - 12/2019, arxiv
    - did some experiments on the knowledge distillation, cifar10

- Data Distillation: Towards Omni-Supervised Learning
    - cvpr 2018
    - apply the model on the unlabled dataset and then pre-train the model on
      that dataset. Finally, fine-tune it on the target dataset.

- Snapshot Distillation: Teacher-Student Optimization in One Generation
    - cvpr 19
    - highlight
        - iteration k's snapshot will be used as the teacher for training from
          k to k + k2. Then, the model at k + k2 will be used as teacher for
          the next several iterations
        - to make sure the teacher's model is different from the student model.
          The learning rate is restarted and then decreased by cosine learning
          rate policy.
        - the temporature is only applied to the logits from the teacher model,
          not from the student model.
        - equivalently, the backward pass of the teacher model is not required,
          which could save time
- Improving Fast Segmentation With Teacher-student Learning
    - BMVC 2018
    - scematic segmentation problem, not instance segmentation, but it seems
      like it can also be applied to instance segmentation.
    - the loss
        - traditional loss
        - alignment based on the probability map, similar like the traditional
          soft-label alignment loss
        - alignment based on consistency loss
            #- for student and teacher network, compute the first order
              information, i.e. for each spatial location, calculate the mean
              of the difference between its response and the neighbor's
              response. If it is smooth, the information should be 0. If it is
              near the edge, the value should be large.
            - then add an alignment loss on the first order information
            - equivalent to have a higher weight on edge area
        - add extra training data for training
    - experiment
        - no teacher -> 40.9
        - add soft-label alignment loss -> 42.3
            - this is where the gain comes from most.
        - add that consistency loss -> 42.8
        - add extra training image -> 43.8

- Fitnets: Hints for thin deep nets
    - ICLR 2015
    - Novelty
        - Before, the teacher-student is to transfer the knowledge
          through the soft label. The novelty here is to introduce the
          intermediate feature maps. The first step is a pre-training
          by aligning the feature map; and the second step is training
          like a usual teacher-student loss.
    - Experiment
        - on MNIST, the standard one is 1.9% (error rate); the
          knowledge distillation is 0.65%. The one with the proposed
          pre-training is 0.51%.
- Label Refinery: Improving ImageNet Classification through Label Progression
    - The teacher network passes the knowledge through soft label for
      each cropped image during the classification network training.
      The improvement here is significant, on imagenet
    - 2018 arxiv

# Instance segmentation
- solo: segmenting objects by locations
    - arxiv 7/2020
    - key
        - instance segmentation task
        - two branches
            - one branch is used to predict the category information on S by S
              grid.
            - another branch contains S^2 channels. Each corresponds to
              class-agnostic mask for that location

# Dection & Counting
- Rethinking Object Detection in Retail Stores
    - the used [this](http://www.colabeler.com/) tool for annotation
    - contribution
        - A new task: detect the location, give the category, count the number of
          instances in that box, for product on the shelf
        - A new dataset for that task. training: 34K images, testing: 16K
          iamges. Good quality from the examples. Not released yet (3/19/2020).
        - present a solution for this problem based on the faster-rcnn (or
          cascaded faster rcnn) with one extra head to predict the number of
          instances, which is modeled as a classification problem. That is, if
          the maximum number of instances is N, then we can have N outputs,
          each of which is for one number. Then, leverage the cross entropy
          loss. This is better than regression in some cases. For example, it
          is from 40.8 to 42.3 with 1 stage (the number of roiAlign in cascaded
          faster rcnn). For 2 stages, it is 42.6 to 43.1.

