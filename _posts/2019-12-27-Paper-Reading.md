---
layout: post
comments: true
title: Paper Reading
---

- To read
    - Dual Student: Breaking the Limits of the Teacher in Semi-supervised Learning
    - Transductive Learning for Zero-Shot Object Detection
    - Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering
    - What Should Not Be Contrastive in Contrastive Learning
    - Towards Unsupervised Crowd Counting via Regression-Detection Bi-knowledge Transfer
    - Learning to Cluster under Domain Shift
    - Unsupervised Deep Metric Learning with Transformed Attention Consistency and Contrastive Clustering Loss
    - Unsupervised Feature Learning by Cross-Level Discrimination between Instances and Groups
    - Spatiotemporal Contrastive Video Representation Learning
    - Exploring Relations in Untrimmed Videos for Self-Supervised Learning
    - Self-supervised Video Representation Learning Using Inter-intra Contrastive Framework
    - Functional Regularization for Representation Learning: A Unified Theoretical Perspective
    - Self-supervised Temporal Discriminative Learning for Video Representation Learning
    - Self-supervised learning using consistency regularization of spatio-temporal data augmentation for action recognition
    - Memory-augmented Dense Predictive Coding for Video Representation Learning
    - Predicting What You Already Know Helps: Provable Self-Supervised Learning
    - Self-supervised learning through the eyes of a child

    - Learning from the Past: Continual Meta-Learning with Bayesian Graph Neural Networks
    - High-order structure preserving graph neural network for few-shot learning
    - Meta-learning with memory-augmented neural networks

    - Towards VQA Models That Can Read

    - Mining Cross-Image Semantics for Weakly Supervised Semantic Segmentation
    - Predicting What You Already Know Helps: Provable Self-Supervised Learning
    - Contrastive Visual-Linguistic Pretraining
    - ReLaB: Reliable Label Bootstrapping for Semi-Supervised Learning
    - CrossTransformers: spatially-aware few-shot transfer
    - Video Representation Learning by Recognizing Temporal Transformations
    - Improving Object Detection with Selective Self-supervised Self-training

    - A comprehensive overhaul of feature distillation

    - Meta-Learning for Semi-Supervised Few-Shot Classification
    - Temporal Self-Ensembling Teacher for Semi-Supervised Object Detection
    - Prototypical Networks for Few-shot Learning
    - Improving Few-Shot Learning using Composite Rotation based Auxiliary Task
    - Transfer Learning or Self-supervised Learning? A Tale of Two Pretraining Paradigms
    - Enabling On-Device CNN Training by Self-Supervised Instance Filtering and Error Map Pruning
    - Self-supervised Neural Architecture Search
    - Knowledge Distillation by On-the-Fly Native Ensemble
    - Online Knowledge Distillation via Collaborative Learning
    - MOD: A Deep Mixture Model with Online Knowledge Distillation for Large Scale Video Temporal Concept Localization
    - Dynamic Refinement Network for Oriented and Densely Packed Object Detection
    - Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models
    - Learning Invariant Representation for Unsupervised Image Restoration
    - Transformation GAN for Unsupervised Image Synthesis and Representation Learning
    - Deep Representation Learning on Long-Tailed Data: A Learnable Embedding Augmentation Perspective
    - Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds
    - HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation
    - Probabilistic Structural Latent Representation for Unsupervised Embedding
    - Decoupled Representation Learning for Skeleton-Based Gesture Recognition
    - Towards Backward-Compatible Representation Learning
    - Sketch-BERT: Learning Sketch Bidirectional Encoder Representation From Transformers by Self-Supervised Learning of Sketch Gestalt
    - Towards Universal Representation Learning for Deep Face Recognition
    - S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement and Data Generation
    - Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning
    - Unsupervised Representation Learning for Gaze Estimation
    - Hierarchically Robust Representation Learning
    - Learning Representations by Predicting Bags of Visual Words

    - Large Scale Video Representation Learning via Relational Graph Clustering
    - Distribution-Induced Bidirectional Generative Adversarial Network for Graph Representation Learning
    - Are we done with ImageNet?
    - Evolving Losses for Unsupervised Video Representation Learning
    - Learning Representations by Predicting Bags of Visual Words
    - Knowledge Distillation Meets Self-Supervision
    - Training low bitwidth convolutional neural networks with low bitwidth gradients
    - fb-net
    - Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection
    - https://blog.evjang.com/2016/11/tutorial-categorical-variational.html
    - htps://openreview.net/pdf?id=rkE3y85ee 
    - https://towardsdatascience.com/what-is-gumbel-softmax-7f6d9cdcb90e?gi=26de57769dc4 
    - Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing
    - Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models
    - Attention Augmented Convolutional Networks
    - Stand-Alone Self-Attention in Vision Models
    - On the Relationship between Self-Attention and Convolutional Layers
    - Multi-similarity loss with general pair weighting for deep metric learning
    - Cut, paste and learn: Surprisingly easy synthesis for instance detection
    - Circle Loss: A Unified Perspective of Pair Similarity Optimization
    - MaxUp: A Simple Way to Improve Generalization of Neural Network Training
    - DetNAS: Backbone Search for Object Detection
    - https://lilianweng.github.io/lil-log/tag/generative-model
    - https://github.com/jason718/awesome-self-supervised-learning
    - Extracting and Composing Robust Features with Denoising Autoencoders
    - Context Encoders: Feature Learning by Inpainting
    - Multimedia Search with Pseudo-Relevance Feedback 
    - M2det: A single-shot object detector based on multi-level feature pyramid network
    - Dynamic anchor feature selection for single-shot object detection
    - Low-shot learning with large-scale diffusion
    - FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence
    - âˆ†-encoder: an effective sample synthesis method for few-shot object recognition
    - Conditional image generation with pixelcnn decoders
    - Population Based Training of Neural Networks
    - Algorithms for Hyper-Parameter Optimization
    - random search for hyper-parameter optimization
    - Snapshot Distillation: Teacher-Student Optimization in One Generation
    - Learning to compose domain-specifici transformations for data
      augmentation.
    - Object detection via a multiregion & semantic segmentation-aware cnn model
    - Web-Scale Responsive Visual Search at Bing
    - Wasserstein GAN
    - Image-to-image translation with conditional adversarial networks
    - https://venturebeat.com/2020/02/11/researchers-develop-technique-to-increase-sample-efficiency-in-reinforcement-learning/
    - Generative adversarial nets
    - Conditional generative adversarial nets
    - U-net: Convolutional networks for biomedical image segmentation
    - Progressive pose attention transfer for person image generation
    - Differentiable learningto-normalize via switchable normalization
    - Disentangled Person Image Generation
    - Web-Scale Responsive Visual Search at Bing
    - When Unsupervised Domain Adaptation Meets Tensor Representations
    - Learning Transferable Features with Deep Adaptation Networks
    - Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks
    - unsupervised domain adaptation by backpropagation
    - Domain-Adversarial Training of Neural Networks
    - Few-shot Object Detection via Feature Reweighting
    - Meta-RCNN: Meta Learning for Few-Shot Object Detection
    - https://towardsdatascience.com/few-shot-learning-in-cvpr19-6c6892fc8c5
    - Robust scene text recognition with automatic rectification
    - Convolutional Sequence to Sequence Learning
    - DARTS: Differentiable Architecture Search
        - https://github.com/quark0/darts
    - symmetry-constrained rectification network for scene text recognition
    - XNAS: Neural Architecture Search with Expert Advice
    - Progressive Neural Architecture Search
    - Non-Local Neural Network
    - CornerNet: Detecting Objects as Paired Keypoints
    - GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond
    - Bottom-up Object Detection by Grouping Extreme and Center Points
    - Side-Aware Boundary Localization for More Precise Object Detection
    - Feature Selective Anchor-Free Module for Single-Shot Object Detection
    - Efficient Object Detection in Large Images using Deep Reinforcement Learning
    - Dually Supervised Feature Pyramid for Object Detection and Segmentation
    - SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization
    - IoU-uniform R-CNN: Breaking Through the Limitations of RPN
    - AugFPN: Improving Multi-scale Feature Learning for Object Detection
    - RDSNet: A New Deep Architecture for Reciprocal Object Detection and Instance Segmentation
    - Learning from Noisy Anchors for One-stage Object Detection
    - RefineNet: multi-path refinement networks for dense prediction
    - FCOS: Fully Convolutional One-Stage Object Detection
        - github: https://github.com/tianzhi0549/FCOS
    - NAS-FCOS: Fast Neural Architecture Search for Object Detection
        - https://github.com/Lausannen/NAS-FCOS
    - Self-Supervised Bernoulli Autoencoders for Semi-Supervised Hashing

# Product Recognition
- Fine-Grained Product Class Recognition for Assisted Shopping
    - ICCV 2015 workshop
    - still based on a lot of feature designs. not fully by neural network
- Product recognition in store shelves as a sub-graph isomorphism problem
    - arxiv 2017, ICIAP 2017
    - still based on sift matching. solving the problem to match the observed
      production relationship with a pre-defined layout graph.
- Recognizing Products: A Per-exemplar Multi-label Image Classification Approach
    - ECCV14
    - recognize based on SIFt matching.

# Data
- Connecting Vision and Language with Localized Narratives
    - provide annotations on coco (all, 100k+) and part (504k) of open image dataset
    - an annotator is required to 1) describe the image content by voice 2)
      hover the mouse on the image region, synchronized with the voice, 3) write
      the transcription immediately after annotating one image.
      - to align the transcript to the voice, (then to the mouse trace/image
        region)
        - automatically generate a transcription of the spoken caption.
        - align the manually written transcription with the auto transcription
          which has the timestamp information
          - the alignment keeps the order.
          - the alignment is based on the edit distance between words.
    - 40.6 seconds for speaking; 110.2 for writing the transcript.
    - manually tried the annotation process, and it is not that natural. maybe
      combine with the click event or draw event (click-drag-clik)? Otherwise,
      there might be some redundant trace between objects, which might not be
      useful and not be easy to remove.

# Network Component
- Non-local Neural Networks
    - cvpr 2018
    - similar with self-attention module to increase the dependency of the
      output on the global information through the whole spatial input.
- AdderNet: Do We Really Need Multiplications in Deep Learning
    - 12/31/2019, arxiv
    - replace the conv layer with addition only operations. That is, the
      similarity between the input region and the kernel is replaced by the
      (negative) l1 distance, which has no multiplications.
    - BN is still used here.
    - The gradient of the filter is calculated based on squared l2 distance loss.
    - The gradient of the input from the output is also based on the full
      precision, but truncated within -1 to 1.
    - exp
        - resnet18, imagenet; CNN: 69.8; the proposed: 67.0
        - resnet50, imagenet; CNN: 76.2; the proposed: 74.9
- Weight Standardization
    -  3/25/2019 arxiv
    - idea
        - normalize the weight for the conv layers.
    - experiment
        - r50 on imagenet
        - bn: 24.3; ws + bn: 23.76
        - gn: 24.81: ws + gn: 23.72
- Evolving Normalization-Activation Layers
    - arxiv 4/2020
    - search the nromalization + activation layer and replace it with BN-Relu
    - +1 improvement on instance segmentation, +0.2 around in imagenet
- Dynamic ReLU
    - Key idea
        - Generalize Relu and LeackyRelu as a learnable relu, which is the
          maximum of multiple linear layer. The relu and the leackyrelu can be
          treated as a special case.
        - the parameters are learned from a small network composed of 2 linear
          layers after the global spatial pooling.
    - Experiments
        - the comparision is based on the relu or leakyrelu. The gain is even
          more than 4 points on mobilenetv2 x 0.35, with a sacrifice of more
          parameters (even doubled). Not sure if the gain comes from more
          parameters or the relu. In large network, the gain becomes small,
          where the ratio of extra parameters is smaller. (10%).
- Making Convolutional Networks Shift-Invariant Again
    - ICML 19
    - The motivation is that the downsampling layer (pool and conv with
      stride) could make output variant with input shift. The solution is
      to apply a smoothing filter before sampling.
        - smoothing filter or anti-aliasing filter is equivalent with a
          conv layer with the following parameters
            - the group size is the same as the input size. That is, each
              input feature map is processed independently
            - the output feature number is the same as the input feature
              number. That is, each input feature map generates one output
              feature.
            - the kernel is a pre-defined parameter, which behaves like a
              low-pass filter, e.g. outer product of [1, 2, 1] and [1, 2, 1]^T
                - note, since it is a pre-defined paraameter, one natural
                  extension is to make it learnable! But i did not see the
                  experiment with this.
            - normalization is used.
        - max pooling with stride 2
            - this layer is converted to
                - 1) max pooling with stride 1,
                - 2) apply a smoothing filter or anti-aliasing filter
                - 3) sub-sampling.
        - conv with stride 2 and relu
            - this layer is converted to
                - conv with stride 1
                - relu
                - apply a smoothing filter
                - sub-sampling
            - it combines with relu, and it is not like the following
                - conv with stride 1
                - apply a smoothing filter
                - sub-sampling
                - relu
            - the reason should be that conv is a linear operation;
              smoothing is also a linear operation. Thus it would be
              equivalent to have one learnable conv layer as long as the
              kernel size of smoothing filter is smaller than/equal to the
              conv kernel size.
            - the computational time cost of such converted conv operation
              will be doubled at least since conv with stride 2 becomes
              with conv with stride 1 first. It would be hard to optimize
              since the non-linear layer is applied afterwards.
        - average pooling with stride 2
            - this layer is converted to
                - smoothing filter
                - sub-sampling with stride 2
    - Experiment
        - in imagenet, it has 0.5-1 point gain.
        - the inference time should be increased, but there is no report in
          the paper. it only applies on the down-sampling layer, and thus
          maybe not too much, 10% maybe?
        - the smoothing kernel can all become learnable since it is one of
          special kinds of conv layer with group and fixed kernel. no
          report of such experiment in the paper. Learning time could be
          longer, but inference time should be the same.
        - not sure how it performs in detection and other tasks.

# Analysis
- Rethinking Pre-training and Self-training
    - 6/11/2020 arxiv
    - conclusion
        - stronger data augmentation for coco detection task does not need
          pre-training, which could even hurt the accuracy.
    - similar with data distillation

# Network architecture
- Are Labels Necessary for Neural Architecture Search?
    - arxiv 3/2020
    - highlights
        - the paper does not provide any new algorithm for NAS
        - the paper replaces the supervised loss based on labels by rotation,
          jigsaw, colorization losses, which are unsupervised.
        - one experiment is to rank 500 architectures based on supervised loss
          and self-supervised loss. Then, calculate the correlation. The
          conclusion is that the rank is highly correlated so that replacing
          supervised labels with self-supervised labels is promising for NAS
        - another experiment is to replace the loss in off-the-shelf DARTS (a
          NAS algorithm) to search the architecture and evaluate on imagenet
          classification and scemantic segmentation problems. The accuracy is
          competitive compared with supervised counterpart.
    - pros
        - raised a good question and found labels are not essential for NAS
    - cons
        - no new algorithm
- FBNetV3: Joint Architecture-Recipe Search using Neural Acquisition Function
    - 6/3/2020 arxiv, FB
    - architecture search, novelty
        - combine the architecture search and training algorithm
    - misc
        - RMSProp is better than SGD
        - EMA also improves
        - pre-train the predictor (input is archtecture embedding; output is
          the accuracy) by the task to predict the flops, which performs better
          than non-pretraining
    - experiment
        - fbnetv3-a vs efficient-net-0
            - 343M flops vs 390M flops
            - 78.0 top1 vs 77.3
        - fbnetv3-c vs efficient-net-1
            - 544M vs 734M
            - 79.6 vs 79.2
        - fbnetv3-e vs efficient-net-2
            - 752 vs 1G
            - 80.4 vs 80.3
        - fbnetv3-g vs efficient-net-3
            - 2G vs 1.8G
            - 82.3 vs 81.7

# Image Classification
- Adversarial Examples Improve Image Recognition
    - arxiv 11/2019
    - the idea is to use seperate BN for clean image and adversirial images
      during training, which increases the accuarcy on efficient-net by 0.3~0.7
      points.
- Designing Network Design Spaces
    - The key idea is to filter the network architecture search by applying
      some constraints, e.g. the network width should be non-decreasing. Then,
      randomly select, e.g. 10, networks to do training and use the best one.
      - some filters include network bandwidth are non-decreasing; the group
        number among different blocks should be the same; sharing the
        bottleneck ratio among different blocks.
      - the weight how to measure if the constraint changes the powerfullness
        of the network space is by 1) randomly sampling 500 networks in the
        space, 2) train and testing each network, 3) plot the cumulative error
        rate, 4) visually decide if the constraint or the smaller search space
        is as good as the original one.
- EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks
    - ICML 2019
    - contribution
        - scale the network width (channels), depth, and input image size at the same time
    - others
        - apply an existing work (same author) to search a network with fewer target flops and 
          use flops as the cost measurement rather than hardware cost
            - MnasNet: Platform-aware neural architecture search for mobile
- ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design
    - ECCV 18


# Optimization
- Fixing the train-test resolution discrepancy
    - 3/30/2020, arxiv
    - the contribution
        - during test, use a larger input
        - during training, fine-tune the network with the larger input size.
    - exp
        - 85.4 -> 86.4 for x101-32-48d
- Understanding the Role of Momentum in Stochastic Gradient Methods
    - NIPs 2019, Igor/Pengchuan
- some docs about multi-armed bandit optimization
    - https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html
    - https://arxiv.org/pdf/1904.07272.pdf
- Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization
    - arxiv 6/2018, journal of machine learning research
- Bayesian Optimization
    - https://github.com/fmfn/BayesianOptimization
        - github star is 3.8k
        - good to use; checkout the advanced tutorial, with
          suggest-evaluate-register
    - https://ax.dev/
        - github star is 1k
        - from facebook
        - good to use. checkout the service api example
    - https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf
## optimization/transport problem
- Sinkhorn Distances: Lightspeed Computation of Optimal Transport
    - nips 2013
    - a very nice solution
- https://michielstock.github.io/OptimalTransport/
    - nice blog about some basis

# network component
    - In-Place Activated BatchNorm for Memory-Optimized Training of DNNs
        - an efficient BN implementation.
        - worth giving it a try
        - 2018, CVPR

- network architecture search
    - MnasNet: Platform-aware neural architecture search for mobile
    - Neural Architecture Search: A Survey
        - high-level introduction of the approaches from 3 aspects: the search space,
          search strategy, and performance evaluation
        - pretty-good survey

# Document Understanding
- LayoutLM: Pre-training of Text and Layout for Document Image Understanding
    - arxiv 6/16/2020 from MSRA
    - 2d encoding based on two lookup tables for each region.

# network structure/Graph Network
- Graph Attention Networks
    - arxiv 2018/4, iclr 2018
    - a very nice paper which improves the representation of each node by the
      adjacent nodes with the attnetion weight


# meta learning
- Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
    - icml - 2017
    - the goal is to optimize the loss where the parameters should be adapted
      with a training sample. Each training sample here means a task which
      consists of n-way k-shot samples.
# Few-short learning
- Learning to compare: Relation network for few-shot learning
    - arxiv 3/2018
    - each image in teh support set and query set is encoded by a learnable
      network. The query's feature is concatenated with each support feature.
      Another network is to map the concatenated feature to a scalar to indicte
      the relationship begtween the query and the feagture in the support set.
      This is for one-shot learning case. For multi shots, all features within
      the same classes are avereged to have 1 feature vector.
- Matching networks for one shot learning
    - arxiv 2016/6
    - similar like a knn classifier
- Edge-Labeling Graph Neural Network for Few-shot Learning
    - arxv 2019/5, 51 citation
    - good writing, nice idea
    - update the edge vectors as well
- Laplacian Regularized Few-Shot Learning
    - icml 2020
    - very elegent optimizaation method to solve teh class prediction problem.
- DPGN: Distribution Propagation Graph Network for Few-shot Learning
    - cvpr 2020
    - based on graph cnn
    - use two graphs. one is the point graph; the other is called distribution
      graph.
- Generalizing from a Few Examples: A Survey on Few-Shot Learning
    - archiv, 5/13/19. worth reading next time

- Infinite Mixture Prototypes for Few-Shot Learning
    - For each class, it designs multiple prototypes to represent. Different
      classes might have different numbers of prototypes. The way how to
      generate is based on the sample's distance to the prototype. If it is
      larger than some pre-defined value, then create a new one.

# Metric Learning
- Cross-Batch Memory for Embedding Learning
    - arxiv 4/2020
    - idea
        - use a memory bank, implemented as queue to increase the negative
          samples.
        - the application is the supervised learning for few-shot learning
          case.
# Graph Neural Network
- A Comprehensive Survey on Graph Neural Networks
    - A good survey.

# Vision Language
- DeViSE: A Deep Visual-Semantic Embedding Model
    - nips 2013
    - the idea
        - each label is represented by a vector learned from laungage model
        - in the training, the image is targed on this soft target
    - experiment
        - if the labels are treated as a flat structure, the cross entropy loss
          is still the best. For example, top-1's acc is 55.6; while the
          proposed is 54.9.
        - if we take into account the hirachical structure of the labels, and
          have a newer metric to account for the strucutre, the accuracy of the
          proposed is better. For example, the acc at top10, the traditional
          cross entropy is 31.3; while the proposed is 33.1

# Text Detection
- ReLaText: Exploiting Visual Relationships for Arbitrary-Shaped Scene Text Detection with Graph Convolutional Networks
    - arxiv 3/16/2020
    - the approach is
        - detect the text primitive
            - partition the text (multiple words) to input N partitions
            - based on FPN
        - enhance the feature for each primitive
            - considering the spatial relationship between pairs
            - input is the roi-align feature
            - implemented by multi-linear perception
        - classify if there should be an edge between primitives
            - the feature of the edge is the concatenation of the primitive
              features and the location informatoin
            - binary classifier problem.

# Object Detection
- REPGN:OBJECT DETECTION WITH RELATIONAL PROPOSAL GRAPH NETWORK
    - 4/18/2019, arxiv only
    - apply the graph netowrk on the proposals. the gain is less than 1 point
      here on coco.
- DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution
- Bridging the gap between372anchor-based and anchor-free detection via adaptive training sample selection
    - cvpr 2020
    - contribution
        - an adaptive method for positive/negative assignment, which is based
          on teh center distance between teh anchor and the gt and the adaptive
          statistic information
        - overall, the imporovement is 1-2 points
- End-to-end object detection with Transformers
    - arxiv 5/2020, facebook
    - idea
        - similar with yolo, but the feature map is re-fused by transformer
          layers.
        - in the end, no aanchor is associated with the spatial locations. Each
          location predicts independent box coordinates and classification
          result
        - in the training, the assignment is based on the loss, which targets
          the matching (bipartite matching from the prediction to the ground
          truth) to have smallest loss.
    - roughly, it achieves 1 percent gain, compared with faster-rcnn
- RepPoints: Point Set Representation for Object Detection
    - idea
        - for the bounding box regression, predict 9 points, and use the
          min/max to get the bounding box. The loss is also based on this box.
- A deep learning pipeline for product recognition on store shelves
    - arxiv 12/2018, IPAS 2018 (IEEE International Conference on Image Processing, Applications and Systems)
    - approach
        - 1-class detector (yolov2)
        - feature extraction (triplet ranking loss)
            - only 1 image exists in the database; or 1-shot learning
            - the image is augmented to create the positive pair.
            - one network is to process both the reference image (clean) and
              teh raw image (not that high quality)
            - vgg16
            - l2 normalized
            - distance is based on the cosine similarity
        - k-NN
            - if the distance to the 1-NN and 2-NN is similar, the recoginition
              is discarded. The paper does not explain what it is by 'discard
              the recognition'.
        - re-ranking
            - by local feature matching
                - score is teh location similarity divided by the local feature distance.
            - by the macro category from other crops in the same shelf/image
                - check the macro category of other products in the same image
                  and use that to filter the ranking result.
    - experiment
        - without fine-tuning the feature extractor -> 21.49
        - fine-tuning -> 27.84;
        - fine-tuning + local feature reranking -> 32.34
        - fine-tuning + 1-NN/2-NN discarding -> 30.46
        - fine-tuning + filter by macro category -> 30.15
        - fine-tuning + local feature reranking + 1-NN/2NN discarding + filter
          by macro -> 36.02
- Dataset
    - Scale Match for Tiny Person Detection
        - release a dataset of TinyPerson
        - the new method is to scale extra dataset for pre-training so that
          teh size could be similar
- Teacher student
    - Mimicking Very Efficient Network for Object Detection
        - student network extracts the region proposal, which is used to
          extract the features from the student network and the teacher
          netowork. The idea is to align the extracted features.
            - The student network can also receive half-sized image as input. The paper discussed this optition, but there is no experiment about this, which is strange.
        - cvpr 17
- Domain adaptation
    - Few-shot Adaptive Faster R-CNN
        - CVPR 19
        - github was set by the author, but no code is shared (1/30/2020)
    - SCL: Towards Accurate Domain Adaptive Object Detection via Gradient Detach Based Stacked Complementary Losses
        - on arxiv 11/29/2019
        - cmu
        - setting
            - the source domain has full annotations (label and bounding
              boxes)
            - the target domain has no labels at all. No image-level labels
        - novelty
            - on the backbone, 3 branches are inserted with the domain
              classifier, so that the features could be in the same domain
              with reverse gradient policy
                - the loss can be cross entropy, weighted loss, focal loss.
                  The author studied the accuracy with different losses
            - context feature is extracted from the branches to combine
              with the region-level features. Thus, each region-level
              feature contains an image-level feature, which are finally
              used for classification and regression
- application
    - Model Adaption Object Detection System for Robot
    - EdgeNet: Balancing Accuracy and Performance for Edge-based Convolutional Neural Network Object Detectors
        - on arxiv 11/4/2019
        - looks like a hardware related paper. Not interested
    - RoIMix: Proposal-Fusion among Multiple Images for Underwater Object Detection
        - arxiv 11/8/2019
        - Peking University
        - Contribution
            - The region proposal is mixed up by another region proposal
                - the mix-up is performed on the features, not on coordinates.
                - the mix-up is performed by a linear combination
                - the label is not changed by the second region proposal. Thus
                  the weight for the current poposal is always larger than that
                  for the second proposal by performing max operation
                  (max(lambda, 1 - lambda))
                - the benefit is to mimic overlapping for underwater dataset
                  because it could be transparent in this case
- Small network
    - Localization-aware Channel Pruning for Object Detection
        - arxiv 11/21/2019
        - Huazhong Univerisity
        - reduce network parameters.
- Network architecture
    - Rethinking Classification and Localization for Object Detection
        - arxiv 12/2019
        - key idea
            - use two networks in box head to process the roi feature. One
              is based on fully connected layers; the other is based on
              conv layers. One approach is to add classification and
              localization loss on each head; the other is to add
              classification loss on the fully-connected head and the other
              is to add localization loss on the conv layer
        - experiment
            - one loss on each head
                - 37.3 -> 38.8
    - IoU-aware Single-stage Object Detector for Accurate Localization
        - arxiv 2019/12
        - claimed novelty
            - predict the IoU of the predicted box and the ground truth.
            - the final score is the IoU multiplied by the classification
              prediciton
            - this is the same as what Yolo V2 does.
        - best acc is 40.6 on coco -> X101-FPN-RetinaNet
    - Learning Rich Features at High-Speed for Single-Shot Object Detection
        - iccv/2019
        - novelty
            - downsample the raw image to multiple sub scales and fuse it
              with the feature maps from different levels
        - highest acc is 37.3 on coco with 32 ms for each image on Titan X
    - Guided Attention Network for Object Detection and Counting on Drones
        - 2019/9
        - novelty
            - 4 feature maps. 1/2, 1/4, 1/8, 1/16 for small scale
            - the loss is applied on the feature map of 1/2, not on all
              these 4 maps
            - a component of background attention module, which is used to
              fuse the feature map from higher level (smaller size) with
              the current feature map.
        - the best approach on CARPK is 90.2.
    - Attentional Network for Visual Object Detection
        - not that interesting.
        - 2017
        - no experiment on coco
        - Some recurrent network with reinforcement learning
    - Objects as Points
        - 2019/4 in arxiv.
        - Novelty
            - In Yolov2, the single feature map is with stride of 32. In
              this paper, it is 4.
            - The objectness is learned with focal loss. The objectness is
              called centerness here. 
            - the objectness is class-specific. That is, if we have 80
              classes, we have 80 feature maps. In YoloV2, it is
              class-agnositic
            - cneter offset is predicted as a class-agnositic way. In
              YoloV2, it is class specific
            - box size is predicted as class-agnositc way, which is the
              same as YoloV2.
        - experiment
            - the highest accuracy it can achieve is 42.1 with hourglass-104 as the backbone

    - Improving Object Detection with Inverted Attention
        - on arxiv 3/28/2019 
        - contribution
            - the feature map is re-weighted by its inversed gradient
                - the intuition is that the gradient is high on the most
                  discriminative regions. If we reverse it, we can focus on
                  less discriminative regions. Thus, the idea is try to
                  focus on the whole region part rather than the most
                  discriminative regions. However, it is not straigtforward
                  to conclude the accuracy would be better. Meanwhile, in
                  the experiment, only 20% features are re-weighted.
    - Enriched Feature Guided Refinement Network for Object Detection
        - ICCV 19
        - https://github.com/Ranchentx/EFGRNet
        - Tianjin University
        - Contribution
            - a framework to enchance the features used for prediction at each
              prediction layer
              - a feature enchancemenet module
                - the input image is first downsampled to 1/8. Then, it is fed
                  to a convolutional network as the enchanced feature.
                  - the netework here contains dilated=1, 2, 4, conv layers to
                    contain more contextual features.
                - acc is improved from 77.2 to 79.4 on voc
                - if dilation is 1, the acc is 78.7. Change one as dilation =
                  2, the acc is 79.0, with another as dilation=4, the acc is
                  79.4.
              - Feature guided refinement module
                  - from the enchanced feature, it predicts an objectness for
                    each anchor. Then, sum up all the objectness for all
                    different anchor shapes at the same spatial location, as
                    the attention. The final feature is the original enchanced
                    feature with original feature multiplied by the attention.
                  - Before doing the final prediction, it uses a deconv layer
                    to filter the features. The deconv offset comes from the
                    offset prediction (bounding box regression).
                  - acc is improved from 77.2 to 81.0
               - with the two modules, the acc is improved from 77.2 to 81.4
        - Experiment
            - on voc, the baseline is 77.2, and the approach improves by 4.1
              point.
              - But the improved solution has lots of more parameters and
                computations. The comparision might come from more parameters
                and computations. It is unclear if the accuracy is higher with
                similar computations
            - on coco, the baseline is 20ms with acc = 25.1. The improved one
              is 21ms with accc = 33.2, which looks pretty promising.
              - However, it is not clear if the training logic is the same,
                e.g. the number of iterations.
    - EfficientDet: Scalable and Efficient Object Detection
    - Learning Spatial Fusion for Single-Shot Object Detection
    - Beihang University
    - Strong baseline
        - improve yolov3 with existing approaches
            - bag of tricks (33.0 to 37.2)
                - mixup algorithm
                - cosine learning rate scheduling
                - sync bn
            - add one anchor-free branch together with anchor-based
              branched
            - add anchor guiding mechanism (37.2 to 38.2)
                - Region proposal by guided anchoring
                    - CVPR19
            - add IoU loss (37.2 to 37.6)
            - final 37.2 -> 38.8
    - contribution
        - adaptive fusion, i.e. fuse the three feature maps with
          adaptive weights. Each spatial position has a different
          weight. The weights on the same location but from different
          feature maps are summed as 1.
    - other details
        - data augmentation: 320 to 608
        - NMS: 0.6
        - 300 epochs
        - cosine learning rate from 0.001 to 1e-5
        - weight decay 5e-4
        - turn off mixup augmentation for the last 30 epochs
        - ignore the adjacent negative samples at the same location
          with the positives. epsilon: ignore region ratio
            - epsilon = 0.2 -> 38.8 -> 39.1
            - epsilon = 0.5 -> 38.8 -> 37.5
        - 38.8 -> 40.6 by the proposed adaptive fusion method
        - other fusion method
            - if we use sum as fusion method: 38.8 -> 39.3
            - if we use concat as fusion method: 38.8 -> 39.5. The number
              of parameters are not disclosed
        - improve retinaNet with the fusion method from 35.9 to 37.4
          with R50 and from 39.1 to 40.1 with R101. Note, the
          comparision with sum and concatenation is not disclosed
    - official code release: https://github.com/ruinmessi/ASFF

## Object detection/knowledge distillation
- Learning Efficient Object Detection Models with Knowledge Distillation
    - nips 2017
    - contribution:
        - add the class-aware weights on the combined classification loss. 
          The weight for all categories are the same, but different from the background
          - note, this weighting only applies to the soft loss. The
            original loss is not altered
          - compared with the non-weighted soft loss, the gain is from
            57.4 to 57.7 in VOC; from 50.8 to 51.3 in KITTI.
        - present a teacher bounded regression loss, that is, if the
          teacher's regression loss is larger, ignore the teacher's
          output and student's loss. If it is smaller, let the student
          move towards teacher's performance. This is regarded as teh
          soft regression loss, and the paper always added the normal
          regression loss.
          - compared with the non-bounded counterpart, the accuracy is
            improved from 54.6 to 55.9 in voc; from 48.5 to 50.1 in
            KITTI.
    - non-contribution, but adopt existing approaches
        - for the classification loss, it comebines the soft loss (from the
          teacher network) and the hard loss (from the ground-truth). Note,
          this is not the contribution.
        - use the feature map to align from the teacher to the student
            - one observation is that even the feature dimension is the
              same, adding a 1x1 conv layer is helpful.
                - with the adaptive layer, the accuarcy goes from 56.9
                  to 58 in voc; and 50.3 to 52.1 in KITTI.
        - with hint, the accuracy on training can be higher, which is
          kinds of counter-intuitive since it imposes more constraint
          on the features, which are not related with the ground-truth
          alignment. This might be the problem of optimization
          capability issue, where hint can help the optimization
    - experiments
        - small network is student; large network is the teacher
            - with different datasets, the accuary improvement can be
              3-4 points in terms of mAP@0.5.
            - For coco metric, the improvement is always around 1 point.
        - same network, but the network with small input image is
          student; the network with large input image is the teacher
            - the distilled student network can achieve comparable with
              the high resolution netework and is much better than the
              low resolution network.
- Mimicking Very Efficient Network for Object Detection
    - cvpr 2017
    - the knowledge is passed through the roi cropped feature for
      faster-rcnn network.
- Quantization Mimic: Towards Very Tiny CNN for Object Detection
    - ECCV 18
    - the paper is similar with mimiking features for object detection,
      where the roi feature is aligned for student's from teachers. The
      difference is that the feature is quantized before aligning, but
      there is no clue on how the network is learned, since the
      quantization operation will give 0 gradient.
- Distilling Object Detectors with Fine-grained Feature Imitation
    - CVPR 19
    - [github](https://github.com/twangnh/Distilling-Object-Detectors)
    - almost the same with Mimicking Very Efficient Network for
      Object Detection(cvpr17) except that, in Mimicking, it uses
      the proposal to crop the region; while 
      in this paper, it expands the region more, i.e. align the
      features within the neighbor of the target position.
      Specifically, for each ground-truth box, it first finds the
      position with highest objectness; then the objectness is
      multiplied by 0.5 as the threshold to filter all non-target region; finally the features
      within the target regions are aligned.
    - Experiment
        - it also compares the case of aligning the full feature, whose
          accuracy loses 8.9 point; while aligning the features within a
          sub region increases the accuracy by 5.2 point. The baseline
          accuracy is 62.63.
- GAN-Knowledge Distillation for one-stage Object Detection
    - arxiv only, 7/2019
    - the paper is not ready at all. some experiments are missing, some
      table are not fully filled.
    - the idea is interesting. It has a discriminator to predict
      whether the feature is from teacher or from student, which guides
      teh student network to mimic teacher's behavior.
- Mask Guided Knowledge Distillation for Single Shot Detector
    - icme 2019
    - previous methods focus on the region features to be aligned
      between the teacher's and the student's. This paper combines the
      global feature alignment and this local feature assignments.
    - Based on SSD
    - with global feature alignment, it is 54.6%. The baseline is not
      reported by not aligning the feature. with the global + local
      feature alignment, it is 56.88%.
- Learning Efficient Detector with Semi-supervised Adaptive Distillation
    - arxiv 1/2019. not find if it is in peer-reviewed conf/journal


# Visual and Lauguage
- End-to-End Learning of Visual Representations From Uncurated Instructional Videos 
    - cvpr 2020
    - video and narative, pre-training. based on contrastive learning. the
      positive pairs come from the same video;
- Large-Scale Adversarial Training for Vision-and-Language Representation Learning
    - arxiv 6/2020
    - apply the adversarial signal to the image/text to increase the robustness
      of the model

# Domain adaptation
- Unsupervised Domain Adaptation by Backpropagation
    - icml 2015
    - gradient reversal layer
# Supervised Learning with Extra Image + Image-level labels
- Revisiting Unreasonable Effectiveness of Data in Deep Learning Era
    - arxiv 2017
    - contribution is to use large-scale dataset as the pre-trained dataset and
      then fine-tune the model on the target domain.
        - target imagenet classification problem, random initialization gives 77.5
          top-1 accuracy; initialization from JFT-300M gives 79.2
        - target coco object detection, imagenet-pretrained initialization gives
          34.3 mAP; 300M-pretrained gives 36.7; imagenet-pretrined +
          300M-pretrained gives 37.4.
        - taerget on voc, imagenet-pretrained gives 76.3; 300M gives 81.4; imagenet
          + 300M gives 81.3

# Self-supervised Learning

## Self-supervised learning/Theory

- A CRITICAL ANALYSIS OF SELF-SUPERVISION, OR WHAT WE CAN LEARN FROM A SINGLE IMAGE
    - iclr 2020
    - contribution is a conclusion: fewer images are enough to learn teh
      lower-level feature parameters. The experiment is based on N images,
      which can be 1 or 1.2 million. Then, apply data augmentation to extend it
      to a fixed number of training images. Finally, apply pre-training and
      then fine-tuning on different layers to see how the performance is. When
      N is small, the accuracy based on the lower-level features are similar
      with larger N, but it is much worse on the higher-level features.
- UNDERSTANDING THE LIMITATIONS OF VARIATIONAL MUTUAL INFORMATION ESTIMATORS
    - iclr 2020
    - some analysis
- ON MUTUAL INFORMATION MAXIMIZATION FOR REPRESENTATION LEARNING
    - iclr 2020
    - some insignt and history of the maxInfo
    - insight: the accuracy does not depend on MI (mutual information) only,
      but also empiracal estimators
- On variational bounds of mutual information
    - ICML 19
- Noise-contrastive estimation: A new estimation principle for unnormalized statistical models
    - AISTAT 2010
    - the problem is to estimate the probability based on some samples, which
      follows some unknown probability.
    - the idea is to have some noisy data, which is based on another
      distribution. The objective is to distinguish the data from the noise
      data.
- Formal limitations on the measurement of mutual information
    - some theory on how to measure the mutual information.

#### self-supervised pretext task/data
- Distilling Localization for Self-Supervised Representation Learning
    - arxiv 4/2020
    - idea
        - the idea is to add a data augmentation, which extracts the salient
          region by unsupervised method and paste it to other images (random
          grayscale image is good enough, compared with texture images or
          images which has no saliency response in imagenet). This augmentation
          is performed with a probability of 50%.
    - experiment
        - 60.6 on imagenet linear probe, which is similar with moco v1.
- Automatic Shortcut Removal for Self-Supervised Representation Learning
    - arxiv 2/2020
    - contribution
        - add a u-net before feeding the image to the network.
        - the u-net reconstructs the original images, which might remove some
          shutcut information.
        - the u-net is called lens here.
        - then the pretext is performed on the reconstructed image
    - experiments
        - rotation as teh pretext task, 46.6 -> 48.6
        - Exemplar, 43.7 -> 46.1
        - Jigsaw, 37.2 -> 40.9

## self-supervised learning/separable task

### self-supervised learning/separable task/single view
- Steering Self-Supervised Feature Learning Beyond Local Pixel Statistics
    - cvpr 2020. 4/5/2020 arxiv
    - use 5 transformation to process the image; the task is to predict which
      transform it is. The 5 transformations are inpainting, which distorts a
      sub region of an image; rotations; random wraping
- Adversarial Feature Learning, Adversarially learned inference
    - both are published in ICLR 2017
    - idea is exactly the same with Large Scale Adversarial Representation Learning
    - idea
        - the idea is quite similar with BiGan. Two networks. One is to map the
          image to the latent variables; the other is to map the latent
          variable to the image. The latent variable is pre-defined and follows
          a pre-defined distribution. The discrimator is used to distinguish
          the pairs.
    - no experiment on the imagenet linear probe study.
- Large Scale Adversarial Representation Learning
    - NIPs 2019, deepmind
    - contribution
        - combine the idea if BiGan and BigGan, used for representation
          learning.
    - imagenet dataset
        - 55.4
- Colorful Image Colorization
    - ECCV 2016
    - the input is a gray image and the output is a colored image. The loss is
      based on cross entropy, where the color space is quantized into 300+
      bins. Class balancing is applied by re-weighting.
- Unsupervised representation learning by predicting image rotations
    - ICLR 2018
    - rotate the image and then predict it as the pretext task
    - Experiments
        - Imagenet + linear prob, AlexNet
            - Random: 14.1
            - proposed: 36.5
            - Upper bound: 50.5

# self-supervised learning/separable task/two view
- Bootstrap Your Own Latent A New Approach to Self-Supervised Learning
    - 6/13/2020 arxiv. deepmind
    - solution
        - similar with moco but without negative queue
        - both crops of the same images are passed to each network. That is,
          though the iteration is 100 epochs, the training cost should be 2000
          epochs as moco.
        - the online encoder (query encoder in moco) uses 2 sets of mlp. Each
          MLP's structure is identical. The hidden size is 4096. The input
          variable to MLP is 2048 (the output of average pooling). In SimCLR or
          moco, the hidden size is teh same as the input size. Meanwhile, these
          two baselines have only 1 set of MLP.
        - moving average weight decay parameter is first 0.996 and gradually
          increases to 1. That means, the parameters of the target network
          tracks fast to the online network and then slowly. This is contrary
          to our finding previously, that is, when the online network changes
          fast, we should track slowly.
        - 512 TPU v3 cores, Batch size 512, 1000 epochs, takes 8 hours.
    - exp
        - linear protocal acc is 74.3
        - color solarization is used in data augmentation.
        - the linear evaluation protocal is a little different from what we
          thought. It added a non-linear layer on top of the logits before
          applying cross entropy loss. Meanwhile, teh regularization is applied
          on the output of the non-linear layer. This change improves from 74.3
          to 74.8.

- Splitbrain autoencoders: Unsupervised learning by cross-channel prediction
    - cvpr 17
    - contribution
        - split the input through channels into two sub inputs. Each goes
          through the network and predict the other split. Specifically, the
          input is split into L and ab components. The original network is
          split into two sub networks by spliting the channels evenly. Then, L
          is passed to one sub network and predict ab; ab is passed to the
          other subnetwork and predict L. The loss is based on classificatoin
          loss by quantizaing the target into multiple discrete slots.
          Empiracally, regression is slightly worse.
    - on the standard imagenet problem, the accuracy is 32.8 with conv5
      feature; 35.2 with conv4 feature; 35.4 with conv3 feature. 
- Unsupervised visual representation learning by context prediction
    - ICCV 2015
    - the idea is to predict the location relationship between a central crop
      and the 8 neighbor crop. For each image, randomly select one small crop first,
      and then randomly select one of the 8 crops around it. Each crop is sent
      to the network and have a feature. The 2 features are concatenated and
      then processed by linear layer. The goal is to predict if it is relation,
      i.e. 8-class classification.
    - Trivial shortcuts
        - Continuity
            - Solution
                - Random gap between patches
                - [48 â€“ 7, 48 + 7] (patch size = 96x96)
        - Chromatic aberration
            - Solution
                - shift green/magenta towards gray
                - randomly drop color channels
                - mean subtraction
                - Downsample and upsample
    - no experiments on imagenet
- AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data
    - CVPR 2019
    - idea
        - the network takes two inputs. one is the original image; while the
          other is the transformed image. The goal is to predict the
          transformation parameters. In the experiments, it uses the projection
          parameters and l2 distance. The transformation is randomly scaling by
          0.8 to 1.2, randomly rotated by 0/90/180/270, and randomly
          translating +-0.125 of its width and height.
    - experiments
        - alexnet, the upper bound is 50.5 and this paper's approach is 37.7.

# self-supervised learning/separable task/multi view
- Representation Learning by Learning to Count
    - ICCV 2017
    - Idea
        - main idea
            - Let's say the original image size is S.
            - Downsample it to S/2 and extract a feature.
            - Split original image into 2x2 tiles and extract all these 4 features
              on these tiles.
            - the loss is the squared l2 norm of the difference between the global
              feature and the sum of the fetures for these 4 tiles.
        - one trivial solution is to map everything into 0 features. To
          mitigate this issue, a regularization term is added, i.e. the
          difference between two image pairs should be large. The difference is
          measured based on the global feature of this other image and the sum
          of the current 4 features from tiles.
        - the idea is based on 5 views, but it also depends on another image.
          This is also more like a combination of multi-view with non-separable
          loss.
    - the motivation is that the feature should represent the number of visual
      primitive in some sense. Thus, the sum of the individual features should
      be the feature from all image content.
    - the accuracy is only 25.7 on conv5 of alexnet on imagenet (linear probe).
- Selfie: Self-supervised Pretraining for Image Embedding
    - arxiv 2019/7
    - apply the BERT to the image.
        - split the image into grids.
        - Each sub-region is fed to the network. The first 3 conv block of
          resnet50. Assume N regions or N output features
        - then use the transform layer to fuse those features. Before fusing
          it, add a dummy feature and the first output is the fused feature.
        - randomly mask out some of the grids in the N regions.
        - the masked regions are fed to the network.
        - the global representation with some regions masked out is added with
          (learnable) positional embedding and then do comparitions with the
          masked regions. Use contrastive loss to learn. If we have M masked
          regions, we can apply M times. Each softmax also has M entries.
    - there is no experiment on the imagenet linear probe results since the
      pretrained is based on teh first 3 conv blocks. But mostly based on the
      fine-tuning.
- Unsupervised learning of visual representations by solving jigsaw puzzles
    - ECCV 2016
    - key idea
        - randomly select 225x225 regions from the input image
        - divide it into 3x3 grids. Each is 75x75
        - randomly choose 64x64 within each 75x75 region.
        - each region is fed into the network and get a feature vector
        - all 9 feature vector is concatenated into one
        - use linear layer to classify it.
    - the algorithm pre-defines a permutation set, rather than to use all 9!
      permutations, which would be too hard for the network to learn. An
      empirical result is taht the more the better, the larger distance each
      permutation is from each other the better. We can use Hamming distance to
      measure the distance.

### self-supervised learning/non-separable task/#classes less than #samples
- Online Deep Clustering for Unsupervised Representation Learning
    - cvpr 2020
    - contribution
        - make the solution of alterating cluster & parameter update online.
        - allocated a memory to store the features for all samples and the
          corresponding labels, which is the cluster asssignment. Teh cluster
          centroid is also kept in the memory.
            - thus, it can not be scaled to billion-level images
        - in each iteration, it has 4 steps
            - network forward to get the feature representation
            - based on the cluster assignment, calculate the loss and update
              the network parameters.
            - the feature in the memory is updated by teh current features with
              momentum update
                - so, it cannot scale up to billion-level datasets
            - the cluster assignment label is updated by finding the closest
              center
- Unsupervised Learning of Visual Features by Contrasting Cluster Assignments
    - arxiv 6/2020
    - method
        - each image has two or multiple crops. Each crop corresponds one
          feature.
        - for each iteration, get the cluster code based on the current batch
          and the stored features.
        - each feature can have a psuedo gt based on the feature assignment.
        - each feature can predict the assignment of the other crops.
        - the loss is the prediction error.
    - exp
        - imagenet linear prob: 75.3. 800 epochs; 64 V-100 (16GB), 50 hours
        - fine-tune on imagenet with pre-trained on instagram-1B
            - SimCLR: 60.4 with frozen features; 77.2 with fine-tuning all
            - proposed: 66.5 with frozen features; 77.8 with fine-tuning all
            - with larger model capacity, the gain of pre-trainign on large
              dataset is even larger.
- ClusterFit: Improving Generalization of Visual Representations
    - cvpr2020, facebook, good paper
    - contribution
        - after a normal pre-training task, do a clustering on the learned feature
          and use the assignment as the pseudo label. Finally, re-train the
          whole network with these psuedo labels
        - teh pre-training task can be unsupervised pretext task, or the normal
          image classificatoin task.
    - experiments
        - imagenet1k -> target dataset
            - setting
                - basline
                    - pre-train r50 on iamgenet1k with gt classification labels
                    - fine-tune the last layer on different datasets.
                - proposed
                    - pre-train r50 on imagenet1k with gt labels
                    - do clustering with different K on the features from the
                      pre-trained model
                    - use the assignment as psuedo labels and re-train the whole
                      network
                    - fine-tune the last layer on different datasets
            - result:
                - if there is no noise in gt and target on imagenet1k, baseline
                  achieves higher accuracy on proposed
                - if there is no noise in gt and target on imagenet-9k and
                  inaturalst datasets, proposed is better. On imagenet-9k, the gain
                  is from 32 to 34.
                - if there is noise (randomly manually injected) to gt, the
                  proposed is better across all target datasets at noise level of
                  75%. With fewer noise, the accuracy on imagenet-9k and inat is
                  better with the proposed
        - imagenet-1B -> target
            - target on imagenet1k
                - baseline: 78, proposed 76.5 --> worse
            - target on imagenet-9k
                - baseline: 32.9, proposed 37.5 -> better
            - target on Place365
                - baseline: 51.2; proposed 52.6
            - target on inat
                - baseline: 43.9; proposed: 49.7
        - self-supervised
            - 45.1 -> 55.2 with jigsaw pre-text task
            - 50.0 -> 56.1 with RotNet
- Unsupervised Deep Learning by Neighbourhood Discovery
    - icml 2019
    - idea, the points within the same neighorhood should be similar. The
      neigborhood is computed based on the similarity between the features.
      Every few iterations, the neighborhood relationship is updated by k-nn.
      Within each batch and the relationship from the neighborhood, we can find
      out the pair (i, j), which should be similar. The similarity between i
      and j is the contrastive softmax between feture i and feature j over the
      sum of feature i and other features. The neighborhood is small initially
      and gradually increased with the motivation that initially the
      neighborhood is not reliable and we should only trust a few point's
      relationship. The trust is based on the entropy where the softmax
      similarity between current point and all others are taken as a
      probability distribution.
    - experiments
        - imagenet, alexnet, 37.9 (upper bound 50.5)
- Self-labelling via simultaneous clustering and representation learning.
    - iclr 2020
    - [code](https://github.com/yukimasano/self-label)
    - vgg, oxford
    - key idea
        - based on Deep Clustering for Unsupervised Learning of Visual Features
        - the contribution is to explicitly balance the label assignment
        - another trick is to add multiple heads for different clustering
          results.
    - experiment
        - 39.6 on imagenet linear probe with AlexNet. (Upper bound is 50.5)
        - addeding RotNet loss and more data augmentation, the accuracy can be
          44.7.
- Deep Clustering for Unsupervised Learning of Visual Features
    - ECCV 2018
    - key idea
        - alternate clustering, which solves the labeling problem and the
          classification, which learns the network parameters based on the
          peudo labels.
        - clustering
            - avoid empty clusters is by 1) randomly selecting a non-empty
              cluster, 2) using the centroid with a small random perturbation
              as the new centroid. 
            - based on the central cropped region
            - features are PCA-reduced to 256, whitened, and l2-normalized
            - updating the cluster every epoch

### self-supervised learning/non-seperable task/#class equals #samples
- Whitening for Self-Supervised Representation Learning
    - arxiv 6/13/2020
    - accuracy is 66 only, which is far worse than the state-of-the-art.
- Debiased Contrastive Learning
    - handles the problem of false negative sampling problem. The idea is more
      like a soft-version of excluding similar points with the current samples.
      The accuracy can be improved with more positive crops to have a better
      estimation of the current samples.
- Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere
    - add a parameters to dynamically tune the balance between alignment and
      uniformity, which is good.
- Wasserstein Dependency Measure for Representation Learning
    - nips 2019.
    - the theory is very complicated. but the idea is 1) the loss is the same
      as CPC, 2) impose some gradient constraints based on the paper of Improved Training of Wasserstein GANs
    - no experiments on the imagenet dataset, but mainly on the hand-made
      datasets.
- Rethinking Data Augmentation: Self-Supervision and Self-Distillation
    - iclr 2020 submission and withdrawn 
        - the main review comments are the novelty and the experiments.
    - there are lots of experiment results, but no results on imagenet, which
      might be one of the problems.
    - the key idea
        - let's say there are K transforms, and C classes. The task is to
          predict K * C outputs. After that, for inference it learns a single
          mapping by learning the teacher, which is aggregated by all teh
          transform.
- Unsupervised Feature Learning via Non-parametric Instance Discrimination
    - (official code)[https://github.com/zhirongw/lemniscate.pytorch]
    - cvpr18
    - idea: each image is a class and adopt contrastive loss between current image and all others, whose complexity is reduced by
      the trick of noise contrastive estimation from O(n) to O(1).
    - imagenet accuracy is 35.6 on alexinet and 54.0 on resnet50 on the linear
      classification evaluation task.
- Contrastive Multiview Coding
    - [code](https://github.com/HobbitLong/CMC)
    - contribution
        - extend the contrastive loss from the 2 views to N views. The loss is
          not improved, but applied multi times to incorporate all views.
    - on imagenet, two views come from L and ab color component of the images.
- Improved Baselines with Momentum Contrastive Learning
    - arxiv 3/9/2020
    - incorporate the tricks shown in A Simple Framework for Contrastive Learning of Visual Representations
      to the momentum constrastive learning, i.e. add extra MLP layer for
      pre-training, more data augmentation, more iterations.
        - the accuracy is improved from 60.6 to 71.1.
- Data-efficient image recognition with contrastive predictive coding
    - iclr 2020 submisssion but rejected. appear in arxiv 12/2019
        - the review comment of why it is rejected is lack of novelty.
        - the experiment results are interesting to learn.
        - the paper is not that well written unless you are familiar with the
          paper it is based on.
    - novelty
        - based on the paper of Representation Learning with Contrastive Predictive Coding,
          this paper introduces more tricks to boost the accuracy and apply it
          on few-shot learning or low-labeled tasks.
          - tricks
            - more powerful network, named resnet161. original implementation
              uses resnet101. --> 5 point gain
            - originally, each patch's size is 64x64. in this paper, it is
              increased. --> 2 point gain. the paper does not tell the
              increased resolution
            - originally, BN is not used, and the reason might be that the
              pretraining works on the patch, but the testing works on the full
              image. The BN captures the input statistics and is thus not
              appropriate in such settings where train and inference are
              different. in this work, it uses layer normalizatoin. -> 2 point
              gain. This paper does not tell if other normalization (not related with 
              batch size) has similar gains.
            - originally, the image patches in the upper side predict the
              regions in the below. This paper extends it to other directions:
              lower predicts upper; left predicts right; right predicts left.
              By using two directions, -> 2 point gain. By using 4 directions,
              2.5 point gains.
            - apply more augmentation on each patch, e.g. color dropping -> 3
              point gain. including other augmentations -> 4.5 point gains.
    - experiments (this may also be the novelty)
        - on the task of imagenet pretraining + imagenet classificatoin with
          one linear layer, it improves from 48.7 to 71.5
        - on the few shot classification task (using 1% of the imagenet data),
          the tricks which works on using 100% imagenet data is not necessary
          to work here.
          - by using 1%, the fully supervised gives 44.1, which is improved to
            77.1 by the pre-training+finetuning. The reason may be that the
            pre-training could see more data.
        - the results on voc detection are 76.6% given resnet161, while the
          supervised counterpart is 74.7.
- Learning Representations by Maximizing Mutual Information Across Views
    - NIPs 2019, MSR Montreal. 3 authors, the first two are also the authors of
      the paper which this paper is based on
    - (code)[https://github.com/Philip-Bachman/amdim-public]
    - the contribution is to extend the paper of Learning deep representations 
      by mutual information estimation and maximization by introducing
        - multi-view of the images to construct the loss
            - multi-view comes from multiple instantiation of the data augmentation
            - the baseline is to use one instantiation
        - multiple feature maps from different spatical sizes are used rather
          than 1.
        - extend the representation of a notion of mixed representation.
    - experiments
        - on cifar10, the baseline is 75.21; while this paper achieves 89.5.
        - on cifar100, the baseline is 49.74, while this paper achieves 68.1
- Learning deep representations by mutual information estimation and maximization
    - iclr 2019, MSR montreal, Yoshua
    - (official code)[https://github.com/rdevon/DIM]
    - the contribution is to incorporate local features in the representation
      learning. Before, each image correponds to one vector, e.g. R^1024. Now,
      it correponds to multiple vectors, e.g. R^{7x7x104} from the last feature
      map before global average pooling.
      - the objective is to maximize the esitmated mutural information between
        the input and the output, as claimed. But actually, it applies the
        maximization between the local features (the features in 7x7 feature map)
        and the global features (after average pooling).
      - there are 3 losses in total
        - the first one should be similar with simCLR, which uses the global
          feature for each image and compares it against all the rest in the
          current batch size.
        - the second is to use the local feature. That is, use the global
          feature to compare it with the local features of current image and
          teh other images.
        - the third one is an adversarial loss which tries to make the output
          follow certain distribution, e.g. uniform distribution.
    - no experiments on imagenet, but on cifar and shrinked version of
      imagenet. The accuracy is comparable with contrastive predictive coding.
- Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks
    - PAMI 2016
    - The idea is
        - randomly sample N patches from the dataset
            - the patch is sampled with a probability propertionally to the
              gradient. But no information about where the gradient is from and
              how the loss is when the gradient is computed
        - for each image, apply M random transformations
        - assign a unique label on each patch
        - train it like a supervised training tasks.
    - Thus, it is highly dependent on how many images can be sampled and how many transformations
      are used. In the experiments, N is 8k~16k. M~150.
    - No experiments on imagenet
- What Makes for Good Views for Contrastive Learning?
    - arxiv 5/2020, from Google.
    - the solution is built on top of moco v1, but with similar tricks mentioned in
      moco v2 or sim-clr. Comparably, it uses even stronger data augmenation,
      e.g. color droping. Another difference is it uses a jigsaw branch similar
      as pretext-invariant representation paper. That is, it also has the loss
      for jigsaw problem.
    - the accuracy with 200 epoch's pre-training achieves 70; the accuracy with
      800 epoch's pretraining achieves 73.
- Self-Supervised Learning of Pretext-Invariant Representations
    - cvpr2020, facebook
    - contribution
        - based on the instance discrimination
        - the difference is that this paper add a new head after conv5, and
          processes a transformed input. The output feature is also
          discriminated with similar loss function as instance discrimination.
        - the intuitivation is the transformed representation should also be
          similar with non-transformed representation.
        - the transform here is 1) split the input to 3x3 regions, 2) process
          each region and get a representation, 3) concate all these 9
          representations with a random order, 4) map the concatenated feature
          to a fixed feature, which will be compared against the features in
          the memory bank. Note, since the loss is similar with instance
          discrimination, the memory bank's size is the same as the dataset.
    - experiments
        - on imagenet linear prob task with r50, it is 63.6, while moco is 60.6
- Representation Learning with Contrastive Predictive Coding
    - deepmind, arxiv 1/2019
    - the paper of DATA-EFFICIENT IMAGE RECOGNITION WITH CONTRASTIVE PREDICTIVE
      CODING gives a better explanation about how the algorithm performs.
    - contribution
        - a framework of contrastive predictive coding for feature learning
            - assume the input signal is x_t
            - encode x_t as z_t=g_enc(x_t)
            - calculate context representatoin as c_t = g_ar(z_<=t). That is,
              calculate a representation based on the signal from the time
              earlier or equal to t.
            - use c_t to predict x_{t+k} by contrastive loss. That is, maximize
              f(x_{t+k}, c_t) = z_{t+k}^T W_k c_t compared with the sum of
              f(x_{j}, c_t), for all j.
        - apply the framework to speech, images, text, rl
            - in imagenet, the accuracy is 48.7.
                - each image is split into 7x7 overlapped regions
                - each region is encoded by g_enc to get z_{u, v}
                    - The encoder of g_enc is based on resnet v2 101.
                - the autoregressive model accepts the regions from top to 
                  predict the regions in the below.
                    - that is, c_{u, v} based on z_{x, v}, (x <= u)
                    - the g_ar is based on PixelCNN-style autoregressive model.
                - in sec 2.1 of DATA-EFFICIENT IMAGE RECOGNITION WITH CONTRASTIVE
                  PREDICTIVE CODING gives a clearer explanation of the details on vision
- A Simple Framework for Contrastive Learning of Visual Representations
    - arxiv 2/2020
    - Hinton
    - contribution
        - add non-linear layer after the representation before applying the
          contrastive loss
        - more iterations
        - larger batch size
    - others
        - image classification problem
            - pre-training + fine-tining last layer
        - imagenet2012
- Momentum Contrast for Unsupervised Visual Representation Learning
    - The task is to learn a backbone or a feature extractor from the unlabeled
      image data, without any annotations.
    - The basic idea (not contributed in this paper) is to learn a dictionary,
      each value of which is a key. For example, we can randomly select 8092 images
      as a dictionary. Each element is an image, and can be mapped to a feature,
      by an encoder (any learnable CNN). For each training image, a feature is
      extracted by the target extraction network (can be the same or different network from encoder).
      The training image's feature is compared with all the features in the dictionary by inner product.
      The loss is the cross entropy loss. The positive pairs normally come from
      teh same image but from differnet views, e.g. augmentation. The negative
      is randomly selected from the dictionary or with all the features in the
      dictionary.
    - The contribution is to solve the problem when the dictionary size is large,
      where in-memory operation might not be feasible. The key idea is 1) keep a buffer to store a dynamic dictionary, whose size is small, 2) run the loss and update the parameter by comparing the training image's feature and the features in the small dictionary; 3) enqueue current images to the dictionary; 4) dequeue the oldest images from the dictionary so that the dictionary size is the same for the next iteration. 
    - The results are verified by a linear classier with the features extracted by the extraction network, which is a common practice in the literature. The results show that this initialization performs as good as or is better than the imagenet-pretrained detector/instance segmentatin/key point detection. 
    - Some things we can learn from is that
	    - Lots of experiments when transferring different tasks! Parameter search! The learning rate of the classifer is 30; the weight decay is 0. These parameters are not consistent with what we used, and they must have done lots of experiments and most of them fails. The temporature parameter in the loss function is 0.07 (this parameter is from a related work, not from this paper), which is also not a usual one. 
	    - The BN in this literature is not working as the paper discusses based on some related work. But this paper alters the orders of the dictionary across different GPUs and make BN work. 
	    - The parameter update in dictionary encoder network is not based on SGD, but only based on the momentum update. That is, the updated parameter is 0.999 * the parameter in last iteration + (1 - 0.999) * the parameter in the extraction network. 
	    - a little bit counter-intuitive since it discards SGD. Not sure if a lower learningjkkjk rate also works. Anyway, this works. The parameter is 0.999, which is quite close to 1 and they must have done lots of parameter tunning


# self-supervised learning/video
- Self-Supervised Learning of Video-Induced Visual Invariances
    - cvpr 2020
    - idea
        - encode the frame, then pool to get shot-level feature. pool it again to
          get video-level feature
        - for each frame, apply the rotation self-supervised loss
        - for the shot embedding, it randomly permute the order and predict
          whether it is permuted or not.
        - another loss is based on predictive contrastive coding. That is, use
          the first k shots' embeding to predict the m-step's future embeding,
          which will be used as the positive pairs.

#### self-supervised learning/observations with more experiments
- Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases
    - arxiv 7/29/2020
    - useless
- How Useful is Self-Supervised Pretraining for Visual Tasks?
    - compare different factors for the exising approaches. No new algorithm is
      proposed, but only some empirical observations, including
      - fine-tuning last linear is not consistent with fine-tuning the whole
        network.
      - more labeled data in the downstream task will reduce the benefit of the
        pre-training.
- Scaling and Benchmarking Self-Supervised Visual Representation Learning
    - facebook. ICCV 2019
    - conducted lots of experiments and observed the accuracy is better with
      larger datasize, larger model, and difficult tasks
- Revisiting Self-Supervised Visual Representation Learning
    - cvpr 2019
    - conducted experiments on 4 existing approaches and with different
      variants of resnet50. The conclusion is that the best performed model
      architectures are different with different pretext tasks.

#### self-supervised learning/combine multiple tasks
- Prototypical Contrastive Learning of Unsupervised Representations
    - arxiv 5/2020
    - key idea
        - based on moco
        - do clustering on the features from the momentum encoder
        - train the model with two losses
            - one is the same as moco
            - one is that the feature should be as close as to the nearest
              centroid, and as far as possible to other centroids
                - this is similar with the clustering-based but with a fixed
                  linear layer where no bias and the weight is the centroid.
                  The label is the index of the nearest centroid.
    - the accuracy on imagenet linear prob experiment is 65.9 with 200 epochs.
        - i believe the accuracy is similar with sim-clr, better than mocov1,
          but worse than moco v2.
- Self-Supervised Representation Learning by Rotation Feature Decoupling
    - CVPR 2019
    - idea: combine the loss of ratation loss and instance discrimination.
        - the network outputs two feature vectors with same dimentions
        - the first vector is for rotation loss.
            - a weight is added to each rotated instance. The weight comes from
              a pre-trained network which distinguishes rotated or non-rotated,
              i.e. a binary classifier. In this loss, if it is non-rotated, the
              weight is 0, otherwise teh weight is 1 - confidence^2. So, if the
              previous network cannot tell if the image is rotated, the weight
              here should also be small. The motivation is that some images are
              rotation sensitive, but some are rotation agnostic. For example,
              a plain in the sky might be rotation agnostic.
        - the second vector is designed to have nothing with the ratation, and
          there are two losses here.
          - the first is to expect those 4 features from the same images should
            be similar. The loss is the squared l2 loss of the feature and the
            average feature.
          - the second is the instance discrimination loss which expects the
            feature from different images are different.
          in the batch
    - In the experiments on imagenet linear probe test, the accuracy is 44.3
      with AlexNet, compared with 36.5 for RotNet; 35.6 for Instance
      Discrimination.

# self-supervised learning/application
# self-supervised learning/application/domain generalization
- Domain Generalization by Solving Jigsaw Puzzles
    - iccv 2019
    - apply the jigswaw loss on the domain generalization
        - the network receives two kind of images. One is the source image from
          multiple source domains with gt labels. Each domain has its own
          domain classifier. The second is shuffled images. Shuffle means
          within 3x3 crops in each image. The loss is to predict the
          permutation. The permutation classifer is shared for all source
          domains.
# self-supervised learning/application/3d
- Self-supervised Feature Learning by Cross-modality and Cross-view Correspondences
    - arxiv 4/2020
    - apply to the 3d data points
# self-supervised learning/application/supervised classification
- Distilling Visual Priors from Self-Supervised Learning
    - eccv-workshop 2020
    - summary
        - the problem is classification problem where each class has 50 traiing
          images. The images come from imagenet.
        - the idea is to first pre-train a newtork with moco-v2. Then, the
          weight is used to initialize two networks. one is called teacher; the
          other is called student. The network strucutre should be the same.
          Thirdly, the teacher network is frozeon, and the student network is
          fine-tuned with 1) classification loss and 2) distillation loss based
          on the feature map at the same time.
        - other modification this paper does is to add a margin in teh
          constrastive learning for pre-training. However, this is kinds of
          similar with changing the weight decay. If tuning the weight decay,
          it might achieve similar resutls.
- Supervised Contrastive Learning
    - arxiv 4/2020
    - idea
        - in the supervised learning, adding the contrastive loss.
        - the positive pairs come from teh images with teh same labels
        - the negative pairs and the positive pairs are all in denominator of
          the loss
        - the learning here is alternative
            - learn based on the contrastive loss, pairs are sampled based on
              the annotations
            - learn the last linear layer for cross entropy.
    - on imagenet
        - 77.0 -> 78.8 for resnet50
        - 78.0 -> 80.8 for resnet200

# self-supervised learning/application/knowledge distillation
- Knowledge Distillation Meets Self-Supervision
    - arxiv 6/2020
    - solution
        - teacher network is first trained with feature extractor and
          classifier
        - add a projection head on the teacher network and use the contrastive
          loss to do self-supervised learning
        - the student network also has a classifier and the projection head to
          mimic the teacher's behavior.
        - 4 losses to train the student network
            - the cross entropy loss supervised by the ground-truth
            - the kl loss between the student and the teacher with temporature.
                - the above two is the same as what traditional approach is
            - the misalignment errors of a heavily-augmented image copy between
              the teacher and the student. This copy is used in self-supervised
              learning. As argued in the paper, the copy is not designed to
              have correct classification prediction since augmentation is
              strong here. But it asks the network outputs similar information
              as the teacher from the classification branch (not from the
              self-supervised learning branch)
            - the last one, which is only the one related with self-supervised
              learning is the misalignment error between teh similarity matrix
              from teacher and the matrix from the student. That is, each image
              has two copies, processed by the networks. Then, each network can
              compute the similarity between any two pairs.
                - one more thing the author does is to filter the entries in
                  the matrix. The rule is that only the samples which the
                  teacher network can successifully classify will be used as
                  the transfor information. The error can be measured by where
                  the self gt is. In experiments, it uses top-75% samples.

- Contrastive Representation Distillation
    - iclr 2020
    - the paper studies the problem of knwoledge distillation for the
      supervised learning. The intuition here is to apply the contrastive loss
      to make the features from the same images (from teacher and student) as
      similar as possible and make the features from different images (one 
      is from student network, the other should come from teacher network) different.
      The loss here drived is not like the one cross entropy loss, but more
      like a binary classification problem. That is, it is a sum of multiple
      losses. One loss is for the positive pairs (features from same image. One
      feature is from teacher, one is from student); the others are for the
      negative pairs. Each negative pair means that one feature is from current
      image and student network; the other feature is from memory bank and
      should be extracted from the teacher network.
    - experiment
        - See table 1. One examle is student's network without knwoledge
          distillation is 73.26; teacher is 75.61; the traditional knwoledge
          distill is 74.92; the paper's is 75.64.

# self-supervised learning/application/adversarial attack
- Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty
    - NIPs 209
    - the idea is to train the classification loss with the self-supervised
      loss based on the rotation. The observation is that the model could be
      more robust. The classification loss is not based on the raw input image,
      but based on the image modified by the gradient, so that the network can
      learn better since the input makes the loss higher. During the testing,
      the input image is also modified to make teh loss higher to test the
      robustness. In this noisy input, the accuracy is dropped from 90+ to 40+.
      With the addition of the self-supervised loss, the gain could be around 5
      points.
# self-supervised learning/application/domain adaptation
- Self-Supervised Learning Across Domains
    - arxiv 7/24/2020
    - during training, add the loss of rotation loss or jigsaw loss
- Unsupervised Domain Adaptation through Self-Supervision
    - The problem is that source domain has labels, target domain has no labels
      but images only, the labels on these two domains are the same. The goal
      is to do classification on the target domain.
    - The idea here is to apply k different self-supervised losses on the
      source domain and the target domains, in addition to the cross entropy
      loss on the source domain. The self-supervised losses are 1) rotation,
      which predicts one of 0/90/180/270 rotation degree applied on the image,
      2) flip, which predicts if the image is vertically flipped, and 3) crop
      location, which predicts one of the four corners of the absolute location
      coordinates.
# self-supervised learning/application/semi-supervised learning
- Big Self-Supervised Models are Strong Semi-Supervised Learners
    - arxiv 6/17/2020
    - contribution
        - teacher network is pre-trained on unlabeled images
        - teacher netowrk is fine-tuned on labeled images
        - student network is trained and distilled with teacher network on
          unlabeled images or combined with labeled images.
        - sim-clr-v2 is presented. the difference is
            - larger backbone network
            - use 3 linear projection layer and use the first one as the input
              for linear classification
            - the last, which is the most important, is to add a momentum
              encoder and a memory queue, which has 1 point's improvement
- Boosting Few-Shot Visual Learning with Self-Supervision
    - ICCV 2019
    - during few-shot feature extractor training, add a loss from
      self-supervised training. This paper uses rotation-based and
      position-based losses.
    - experiments
        - with the rotation auxiliary loss, there is about 1 point's gain.
        - with patch location relationship loss, there is also about 1 point's
          gain.
        - applied to semi-supervised few-shot learning as well, and in certain
          cases, tehre are 3-4 points' gain.
- S4L: Self-supervised semi-supervised learning
    - iccv 2019
    - problem: target supervised classification dataset (1% of the imagenet
      dataset or 10%) with the unlabeled image datasets. The solution is to
      train the model on the joint of the two datasets with the loss from
      unsupervised learning. Each batch consists of the images from unlabeled
      or labeled sets evenly. For the image including unlabeled and labeled, it applies the rotation
      self-supervised loss. That is randomly rotate the input image
      with 0/90/180/270 degres and predict the degree
      with the cross entropy loss. The other is some exemplar loss, which is
      similar with the contrastive loss. The experiments show that with 1% of
      the labelled data,
        - the approach of training supervised model only on the labelled data
          gives 48.43% top-5 accuracy
        - the approach of psuedo labeling (train a model on labeled, propagate
          the labels on the unlabeled, re-train the model on the full) gives
          51.56.
        - self supervised + linear layer gives at most 25.98 accuracy
        - self supervised + fine-tune gives at most 45.11, which is even lower
          than the 48.43, which is from the supervised learning only. 
        - the joint training gives 53.37 with the rotation loss.

# large scale detection in terms of labels
- Scaling Object Detection by Transferring Classification Weights
    - iccv 2019, [code](https://github.com/xternalz/AE-WTN)
    - an enchanced weight transfer network to scale the classes
- Learning to Segment Every Thing 
    - CVPR 2018, [code](https://github.com/ronghanghu/seg_every_thing)
    - proposed a weight transfer network

### Video
- Cross Pixel Optical Flow Similarity for Self-Supervised Learning
    - ACCV 2018
    - align the image representation to the optical flow
    - the accuracy on imagenet linear probe is 28% only.

# General
- Curriculum learning
    - 2019 ICML, Yoshua as the first author
    - the basic idea is to let the network to learn different tasks. Each task
      is like a curriculum. The scheduling is that the tasks will be gradually
      harder so that the network can learn gradually. Similar with what we
      larned from elementary school, where the courses are becoming harder and
      harder
      - theoretically, you can apply a weight on the example, which means the
        sampling rate, or how likely the sample should be used. In this case,
        the weight will change the input distribution, and thus the entropy if
        we take the input data as a probability distribution. The curriculumn
        should make the entropy higher gradually.
    - The experiments it did is based on some synthetic data, and the
      policy is that first to learn easy samples and then the rest, which
      demonstrates to be better than to learn all data equally.
# Face recognition
- FaceNet: A Unified Embedding for Face Recognition and Clustering
    - arxiv 12/2015, 4830 citations until 4/7/2020, cvpr2015
    - contribution
        - (as claimed in the paper) previously, the approach is to learn a classification network on the
          annotated identity and use intermediate representations as the face
          representation, which has high dimensionals and is hard to justify
          why the feature could work.
        - the idea here is to use triplets loss to explicitly learn the
          embedding, which is of 128 dimensions. The loss is hinge loss with a
          marge based on one anchor image, a positive image and a negative
          image.
    - training
        - 100M-200M face thumbnails, 8M identies
        - input size: 96~224

# Image Caption/Visual Question Answering
- Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers
    - cvpr 2020
    - only use spatial feature rather than region-level feature.
- Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA
    - arxiv 11/2019
    - idea
        - the network inputs have 3 modalities
            - the question word's embedding
            - the visual object embedding
            - the text embedding
- TextCaps: a Dataset for Image Captioning with Reading Comprehension
    - introduced a dataset, which has lots of text (OCR).

# Detection by caption dataset
- Caption Dataset --> Detection
    - Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection
        - iccv 2019
        - Problem
            - use the caption dataset to infer the image-level labels, which
              are used as input to the weakly-supervised object detection
              algorithm for detection
        - algorithm
            - Novelty: a method of how to infer the image-level labels from the caption
              text
                - each word is embeded by GloVe algorithm, resulting 300D
                  feature
                - the embedding is project to a 400D space
                - max pooling over multiple words in the sentence
                - softmax for the target category domains (e.g. 80 for coco). A
                  linear layer should be applied before the softmax (not
                  mentioned in the paper).
            - use OICR as the weakly supervised learning algorithm after
              infering the image labels. The algorithm is slightly modified
                - in OICR or WSDDN, two softmax operations are applied across
                  proposals and classes, but here one of the operations is
                  replaced as sigmoid. The difference is not studied in the
                  experiment. The re-weighting in OICR is removed here, and the
                  accuracy is not changed substantionaly as claimed but without
                  experiment result support.
        - experiment
            - coco-caption dataset and flicker caption datasets
                - assume these two datasets are carefully annotated with
                  captions. The cost might be more than the image-level labels.
                  In experiments, no studies are shown with noisy caption
                  datasets, e.g. CC, or randomly crawled internet data.
            - for the method of extracting the image-level labels from the
              caption, the proposed method improves from 39.9 (exact term
              matching), 41.7 (learned GLoVe) to 43.1 on voc test by leveraging coco. If using
              Flicker, it is 31.0 (exact term matching) to 33.6 (the results of
              learned GLoVE is not disclosed).

# Semi-supervised training
- semi-supervised classification with graph convolutional networks
    - arxiv, 2016, citation 4k+
- Billion-scale semi-supervised learning for image classification
    - arxiv only 5/2019
    - key idea
        - train a classifier on the supervised data
        - propagate the labels on the unlabeld data
        - pre-train the student network on the propagated dataset
        - fine-tune on the supervised data
    - Comopared with Data Distillation: Towards Omni-Supervised Learning
        - the ref paper studied the approach on detection and keypoint
          detection, while this paper studies the approach on image
          classification.
        - the ref paper re-train on the mixed two dataset, while this paper
          seperates them into two stages: pre-train on the propagated data
          and then fine-tune on the labeled data
- Data Distillation: Towards Omni-Supervised Learning
    - cvpr18
    - key idea
        - leverage the unlabeled data to help the supervised training
        - method
            - train a teacher network on the supervised data
            - propagate teh labels on the unlabeled data by multi transform
              inference
                - multi transform inference means to do inference on multi form
                  of the input, e.g. by multi scaling
                - the label is hard label and teh threshold is set so that the
                  average number of labels on unlabeled image is similar with that
                  in labeled dataset.
            - re-train the model on the combined data from labeled dataset and
              the auto-labeled dataset.
    - experiments are on keypoint detection and object detection.
        - The gain for keypoint detection is large
        - teh gain on detection is around 1 point.

# Data Augmentation
- CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features
    - iccv 2019
    - idea is that combine two inputs by randomly 1) sampling a sub region and
      2) fill this region with one image, 3) fill the regions outside the
      sub-region with the other image.
- Randaugment: Practical data augmentation with a reduced search space
    - arxiv 11/2019
    - key idea
        - the search space is composed of two parameters. one is the number of
          transformations, and the other is the strength.
        - grid search to determine the parameters
- adversarial autoaugment
    - iclr 2020
    - the idea is to have a policy network, which tries to maximize the loss of
      the target network. It is based on M augmented instances for each image,
      and make decisions to tune the policy network.
    - resnet50, imagenet, -> 79%.
- Fast AutoAugment
    - NIPs 2019
    - The key idea is to train a model without data augmentation first; and
      then evaluate the model on the pre-reserved data with different data
      augmentation. That is, the performance of different poicies is not
      evaluated by re-training, but by evaluation
    - on Imagenet, the accuary with R50 is 22.4. Baseline is 23.7; AutoAgument
      gives 22.4 also.
- Learning Data Augmentation Strategies for Object Detection
    - apply the auto augmentation idea from classification to detection.
    - the gain is 2.3 on R50+RetinaNet
    - rotation is perfererred in the augmentation searching
- Population based augmentation: Efficient learning of augmentation policy schedules
    - icml 2019

# Optimization
- ShakeDrop Regularization for Deep Residual Learning
    - ieee access 2019
    - a hybrid way to pertube the weight of the residual branch stochastically
      in the forward and backward during training.
- Practical Bayesian Optimization of Machine Learning Algorithms
    - not a good tutorial.

# Teacher-Student
## teacher-student/classification
- Knowledge Distillation Meets Self-Supervision
    - arxiv 6/2020
    - solution
        - teacher network is first trained with feature extractor and
          classifier
        - add a projection head on the teacher network and use the contrastive
          loss to do self-supervised learning
        - the student network also has a classifier and the projection head to
          mimic the teacher's behavior.
        - 4 losses to train the student network
            - the cross entropy loss supervised by the ground-truth
            - the kl loss between the student and the teacher with temporature.
                - the above two is the same as what traditional approach is
            - the misalignment errors of a heavily-augmented image copy between
              the teacher and the student. This copy is used in self-supervised
              learning. As argued in the paper, the copy is not designed to
              have correct classification prediction since augmentation is
              strong here. But it asks the network outputs similar information
              as the teacher from the classification branch (not from the
              self-supervised learning branch)
            - the last one, which is only the one related with self-supervised
              learning is the misalignment error between teh similarity matrix
              from teacher and the matrix from the student. That is, each image
              has two copies, processed by the networks. Then, each network can
              compute the similarity between any two pairs.
                - one more thing the author does is to filter the entries in
                  the matrix. The rule is that only the samples which the
                  teacher network can successifully classify will be used as
                  the transfor information. The error can be measured by where
                  the self gt is. In experiments, it uses top-75% samples.

- Contrastive Representation Distillation
    - iclr 2020
    - the paper studies the problem of knwoledge distillation for the
      supervised learning. The intuition here is to apply the contrastive loss
      to make the features from the same images (from teacher and student) as
      similar as possible and make the features from different images (one 
      is from student network, the other should come from teacher network) different.
      The loss here drived is not like the one cross entropy loss, but more
      like a binary classification problem. That is, it is a sum of multiple
      losses. One loss is for the positive pairs (features from same image. One
      feature is from teacher, one is from student); the others are for the
      negative pairs. Each negative pair means that one feature is from current
      image and student network; the other feature is from memory bank and
      should be extracted from the teacher network.
    - experiment
        - See table 1. One examle is student's network without knwoledge
          distillation is 73.26; teacher is 75.61; the traditional knwoledge
          distill is 74.92; the paper's is 75.64.

- The State Of Knowledge Distillation For Classification Tasks
    - 12/2019, arxiv
    - did some experiments on the knowledge distillation, cifar10

- Data Distillation: Towards Omni-Supervised Learning
    - cvpr 2018
    - apply the model on the unlabled dataset and then pre-train the model on
      that dataset. Finally, fine-tune it on the target dataset.

- Snapshot Distillation: Teacher-Student Optimization in One Generation
    - cvpr 19
    - highlight
        - iteration k's snapshot will be used as the teacher for training from
          k to k + k2. Then, the model at k + k2 will be used as teacher for
          the next several iterations
        - to make sure the teacher's model is different from the student model.
          The learning rate is restarted and then decreased by cosine learning
          rate policy.
        - the temporature is only applied to the logits from the teacher model,
          not from the student model.
        - equivalently, the backward pass of the teacher model is not required,
          which could save time
- Improving Fast Segmentation With Teacher-student Learning
    - BMVC 2018
    - scematic segmentation problem, not instance segmentation, but it seems
      like it can also be applied to instance segmentation.
    - the loss
        - traditional loss
        - alignment based on the probability map, similar like the traditional
          soft-label alignment loss
        - alignment based on consistency loss
            #- for student and teacher network, compute the first order
              information, i.e. for each spatial location, calculate the mean
              of the difference between its response and the neighbor's
              response. If it is smooth, the information should be 0. If it is
              near the edge, the value should be large.
            - then add an alignment loss on the first order information
            - equivalent to have a higher weight on edge area
        - add extra training data for training
    - experiment
        - no teacher -> 40.9
        - add soft-label alignment loss -> 42.3
            - this is where the gain comes from most.
        - add that consistency loss -> 42.8
        - add extra training image -> 43.8

- Fitnets: Hints for thin deep nets
    - ICLR 2015
    - Novelty
        - Before, the teacher-student is to transfer the knowledge
          through the soft label. The novelty here is to introduce the
          intermediate feature maps. The first step is a pre-training
          by aligning the feature map; and the second step is training
          like a usual teacher-student loss.
    - Experiment
        - on MNIST, the standard one is 1.9% (error rate); the
          knowledge distillation is 0.65%. The one with the proposed
          pre-training is 0.51%.
- Label Refinery: Improving ImageNet Classification through Label Progression
    - The teacher network passes the knowledge through soft label for
      each cropped image during the classification network training.
      The improvement here is significant, on imagenet
    - 2018 arxiv

# Dection & Counting
- Rethinking Object Detection in Retail Stores
    - the used [this](http://www.colabeler.com/) tool for annotation
    - contribution
        - A new task: detect the location, give the category, count the number of
          instances in that box, for product on the shelf
        - A new dataset for that task. training: 34K images, testing: 16K
          iamges. Good quality from the examples. Not released yet (3/19/2020).
        - present a solution for this problem based on the faster-rcnn (or
          cascaded faster rcnn) with one extra head to predict the number of
          instances, which is modeled as a classification problem. That is, if
          the maximum number of instances is N, then we can have N outputs,
          each of which is for one number. Then, leverage the cross entropy
          loss. This is better than regression in some cases. For example, it
          is from 40.8 to 42.3 with 1 stage (the number of roiAlign in cascaded
          faster rcnn). For 2 stages, it is 42.6 to 43.1.

